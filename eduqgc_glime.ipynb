{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2bb3713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# # ===============================================================\n",
    "# # One-cell notebook: EDUQGC scaling study + GraphLIME explanations\n",
    "# # ===============================================================\n",
    "\n",
    "# import os, sys, time, pickle, random, math, itertools, json\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch import nn\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "# import pennylane as qml\n",
    "\n",
    "# # Torch Geometric\n",
    "# try:\n",
    "#     from torch_geometric.datasets import Planetoid\n",
    "#     import torch_geometric.utils as pyg_utils\n",
    "# except Exception as e:\n",
    "#     raise RuntimeError(\"Need torch-geometric installed.\")\n",
    "\n",
    "# # =====================================================\n",
    "# # Utils for determinism\n",
    "# # =====================================================\n",
    "# def set_seed(seed=42):\n",
    "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "#     random.seed(seed); np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(seed)\n",
    "#     torch.use_deterministic_algorithms(False)\n",
    "\n",
    "# # =====================================================\n",
    "# # EDUQGC Model\n",
    "# # =====================================================\n",
    "# class EDUQGC(nn.Module):\n",
    "#     def __init__(self, n_nodes, in_feats, T=2, seed=0, use_gpu_qnode=True, use_feat_skip=True, num_classes=3):\n",
    "#         super().__init__()\n",
    "#         self.n_nodes = n_nodes\n",
    "#         self.T = T\n",
    "#         self.use_feat_skip = use_feat_skip\n",
    "\n",
    "#         self.enc_W = nn.Parameter(torch.randn(T, 2, in_feats) * 0.08)\n",
    "#         self.enc_b = nn.Parameter(torch.randn(T, 2) * 0.02)\n",
    "\n",
    "#         self.edge_phase  = nn.Parameter(torch.randn(T) * 0.08)\n",
    "#         self.pre_theta   = nn.Parameter(torch.randn(T) * 0.08)\n",
    "#         self.pre_psi     = nn.Parameter(torch.randn(T) * 0.08)\n",
    "#         self.post_theta  = nn.Parameter(torch.randn(T) * 0.08)\n",
    "#         self.post_psi    = nn.Parameter(torch.randn(T) * 0.08)\n",
    "\n",
    "#         readin_dim = 1 + in_feats if use_feat_skip else 1\n",
    "#         self.readout = nn.Linear(readin_dim, num_classes)\n",
    "\n",
    "#         use_cuda = torch.cuda.is_available()\n",
    "#         qdev_name = \"lightning.gpu\" if (use_gpu_qnode and use_cuda) else \"default.qubit\"\n",
    "#         self.dev = qml.device(qdev_name, wires=n_nodes, shots=None)\n",
    "\n",
    "#         @qml.qnode(self.dev, interface=\"torch\", diff_method=\"best\")\n",
    "#         def circuit(edge_index, X, enc_W, enc_b,\n",
    "#                     edge_phase, pre_theta, pre_psi, post_theta, post_psi):\n",
    "#             for t in range(self.T):\n",
    "#                 enc_out = X @ enc_W[t].T + enc_b[t]\n",
    "#                 alphas = enc_out[:, 0]; betas = enc_out[:, 1]\n",
    "#                 for i in range(self.n_nodes):\n",
    "#                     qml.RX(alphas[i], wires=i)\n",
    "#                     qml.RY(betas[i], wires=i)\n",
    "\n",
    "#                 for i in range(self.n_nodes):\n",
    "#                     qml.RZ(pre_psi[t], wires=i)\n",
    "#                     qml.RX(pre_theta[t], wires=i)\n",
    "\n",
    "#                 E = edge_index.shape[1]\n",
    "#                 for e in range(E):\n",
    "#                     u = int(edge_index[0, e].item()); v = int(edge_index[1, e].item())\n",
    "#                     if u != v:\n",
    "#                         qml.ControlledPhaseShift(edge_phase[t], wires=[u, v])\n",
    "\n",
    "#                 for i in range(self.n_nodes):\n",
    "#                     qml.RZ(post_psi[t], wires=i)\n",
    "#                     qml.RX(post_theta[t], wires=i)\n",
    "\n",
    "#             return [qml.expval(qml.Z(i)) for i in range(self.n_nodes)]\n",
    "\n",
    "#         self._circuit = circuit\n",
    "\n",
    "#     def forward(self, edge_index_torch, x_torch):\n",
    "#         device = next(self.parameters()).device\n",
    "#         edge_index = edge_index_torch.to(device)\n",
    "#         X = x_torch.to(device).float()\n",
    "\n",
    "#         out = self._circuit(edge_index, X,\n",
    "#                             self.enc_W, self.enc_b,\n",
    "#                             self.edge_phase,\n",
    "#                             self.pre_theta, self.pre_psi,\n",
    "#                             self.post_theta, self.post_psi)\n",
    "\n",
    "#         expvals = torch.stack(out, dim=0).float().to(device)\n",
    "#         if expvals.dim() == 1:\n",
    "#             expvals = expvals.unsqueeze(1)\n",
    "#         elif expvals.dim() == 2 and expvals.shape[1] == 1:\n",
    "#             pass\n",
    "#         else:\n",
    "#             expvals = expvals.squeeze(-1).unsqueeze(1)\n",
    "\n",
    "#         readin = torch.cat([expvals, X], dim=1) if self.use_feat_skip else expvals\n",
    "#         logits = self.readout(readin)\n",
    "#         return logits\n",
    "\n",
    "# # =====================================================\n",
    "# # Dataset builder\n",
    "# # =====================================================\n",
    "# def build_cora_ego_dataset(num_graphs=20, target_nodes=20, pca_dim=10,\n",
    "#                            chosen_labels=(0,1,2), seed=42, save_path=None):\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     dataset = Planetoid(root=\"./cora_multi_exp/raw\", name=\"Cora\")\n",
    "#     data = dataset[0]\n",
    "#     x_all = data.x.numpy().astype(np.float32)\n",
    "#     y_all = data.y.numpy().astype(np.int64)\n",
    "#     edge_index_all = data.edge_index.numpy()\n",
    "\n",
    "#     chosen_labels_set = set(chosen_labels)\n",
    "#     eligible = np.array([i for i,y in enumerate(y_all) if y in chosen_labels_set])\n",
    "#     pca = PCA(n_components=pca_dim, random_state=seed)\n",
    "#     pca.fit(x_all[eligible])\n",
    "\n",
    "#     def pick_subgraph(center):\n",
    "#         nodes = pyg_utils.k_hop_subgraph(\n",
    "#             torch.tensor([center], dtype=torch.long),\n",
    "#             2,\n",
    "#             torch.as_tensor(edge_index_all, dtype=torch.long),\n",
    "#             relabel_nodes=False\n",
    "#         )[0].numpy()\n",
    "\n",
    "#         nodes = np.array([n for n in nodes if y_all[n] in chosen_labels_set])\n",
    "#         unique = set(nodes.tolist())\n",
    "#         if len(unique) < target_nodes:\n",
    "#             extra = [n for n in eligible if n not in unique]\n",
    "#             rng.shuffle(extra)\n",
    "#             nodes = list(unique) + extra[:target_nodes-len(unique)]\n",
    "#         nodes = np.array(nodes[:target_nodes])\n",
    "\n",
    "#         relabel = {old:i for i,old in enumerate(nodes)}\n",
    "#         mask = [(u in relabel and v in relabel)\n",
    "#                 for u,v in zip(edge_index_all[0], edge_index_all[1])]\n",
    "#         sub_edges = edge_index_all[:,mask]\n",
    "#         if sub_edges.size > 0:\n",
    "#             remapped = np.array([[relabel[int(u)], relabel[int(v)]]\n",
    "#                                  for u,v in sub_edges.T]).T\n",
    "#         else:\n",
    "#             remapped = np.zeros((2,0),dtype=int)\n",
    "#         edges = set()\n",
    "#         for u,v in remapped.T:\n",
    "#             edges.add((u,v)); edges.add((v,u))\n",
    "#         if len(edges)==0:\n",
    "#             edge_index = np.vstack([np.arange(target_nodes), np.arange(target_nodes)])\n",
    "#         else:\n",
    "#             u_list,v_list = zip(*edges)\n",
    "#             edge_index = np.vstack([u_list,v_list])\n",
    "#         X = pca.transform(x_all[nodes])\n",
    "#         y = np.array([ {old:i for i,old in enumerate(chosen_labels)}[int(lbl)]\n",
    "#                        for lbl in y_all[nodes]])\n",
    "#         return dict(edge_index=edge_index, X=X.astype(np.float32), y=y)\n",
    "\n",
    "#     rng.shuffle(eligible)\n",
    "#     centers = eligible[:num_graphs]\n",
    "#     graphs = [pick_subgraph(c) for c in centers]\n",
    "\n",
    "#     if save_path:\n",
    "#         Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "#         with open(save_path,\"wb\") as f:\n",
    "#             pickle.dump(dict(graphs=graphs,meta=dict(N=target_nodes,F=pca_dim,seed=seed)),f)\n",
    "#     return graphs\n",
    "\n",
    "# # =====================================================\n",
    "# # Train/Eval helpers\n",
    "# # =====================================================\n",
    "# def train_epoch(model, graphs, opt, device=\"cpu\"):\n",
    "#     model.train(); total_loss=0; nodes=0\n",
    "#     for g in graphs:\n",
    "#         e = torch.from_numpy(g[\"edge_index\"]).long().to(device)\n",
    "#         X = torch.from_numpy(g[\"X\"]).float().to(device)\n",
    "#         y = torch.from_numpy(g[\"y\"]).long().to(device)\n",
    "#         logits = model(e,X)\n",
    "#         loss = F.cross_entropy(logits,y)\n",
    "#         opt.zero_grad(); loss.backward(); opt.step()\n",
    "#         total_loss += loss.item()*X.shape[0]; nodes+=X.shape[0]\n",
    "#     return total_loss/max(1,nodes)\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(model, graphs, device=\"cpu\"):\n",
    "#     model.eval(); total_loss=0; nodes=0; correct=0\n",
    "#     for g in graphs:\n",
    "#         e = torch.from_numpy(g[\"edge_index\"]).long().to(device)\n",
    "#         X = torch.from_numpy(g[\"X\"]).float().to(device)\n",
    "#         y = torch.from_numpy(g[\"y\"]).long().to(device)\n",
    "#         logits = model(e,X)\n",
    "#         loss = F.cross_entropy(logits,y)\n",
    "#         pred = logits.argmax(1)\n",
    "#         correct += (pred==y).sum().item()\n",
    "#         total_loss += loss.item()*X.shape[0]; nodes+=X.shape[0]\n",
    "#     return total_loss/max(1,nodes), correct/max(1,nodes)\n",
    "\n",
    "# # =====================================================\n",
    "# # GraphLIME (classical explanation model)\n",
    "# # =====================================================\n",
    "# class GraphLIME:\n",
    "#     def __init__(self, num_samples=512, sigma=0.1, hop_k=0, kernel_width=0.75,\n",
    "#                  alpha_ridge=1e-2, random_state=42):\n",
    "#         self.num_samples=num_samples; self.sigma=sigma; self.hop_k=hop_k\n",
    "#         self.kernel_width=kernel_width; self.alpha_ridge=alpha_ridge\n",
    "#         self.random_state=random_state\n",
    "\n",
    "#     def _softmax(self, logits): return torch.softmax(logits, dim=-1)\n",
    "\n",
    "#     def _neighbors_khop(self, edge_index_np, node, k, N):\n",
    "#         if k<=0: return np.array([node],dtype=int)\n",
    "#         adj=[[] for _ in range(N)]\n",
    "#         for u,v in edge_index_np.T: adj[int(u)].append(int(v))\n",
    "#         hopset={node}; frontier={node}\n",
    "#         for _ in range(k):\n",
    "#             nxt=set()\n",
    "#             for u in frontier: nxt|=set(adj[u])\n",
    "#             hopset|=nxt; frontier=nxt\n",
    "#         return np.array(sorted(hopset),dtype=int)\n",
    "\n",
    "#     def explain(self, model, gdict, node, device):\n",
    "#         rng=np.random.default_rng(self.random_state)\n",
    "#         model.eval()\n",
    "#         edge_index=torch.from_numpy(gdict[\"edge_index\"]).long().to(device)\n",
    "#         X_np=gdict[\"X\"].astype(np.float32); y_np=gdict[\"y\"]\n",
    "#         N,F=X_np.shape\n",
    "#         X_base=torch.from_numpy(X_np).float().to(device)\n",
    "#         with torch.no_grad():\n",
    "#             base_logits=model(edge_index,X_base)\n",
    "#             base_probs=self._softmax(base_logits)[node].cpu().numpy()\n",
    "#             base_pred=int(np.argmax(base_probs))\n",
    "#         target_class=base_pred\n",
    "\n",
    "#         P_nodes=self._neighbors_khop(gdict[\"edge_index\"],node,self.hop_k,N)\n",
    "#         P_mask=np.zeros(N,dtype=bool); P_mask[P_nodes]=True\n",
    "#         feat_std=X_np.std(axis=0)+1e-8\n",
    "\n",
    "#         Z=np.zeros((self.num_samples,F),dtype=np.float32)\n",
    "#         y=np.zeros((self.num_samples,),dtype=np.float32)\n",
    "#         w=np.zeros((self.num_samples,),dtype=np.float32)\n",
    "\n",
    "#         for s in range(self.num_samples):\n",
    "#             X_pert=X_np.copy()\n",
    "#             noise_all=rng.normal(0.0,self.sigma,size=(N,F)).astype(np.float32)\n",
    "#             X_pert[P_mask]=X_np[P_mask]+noise_all[P_mask]*feat_std\n",
    "#             X_pert_t=torch.from_numpy(X_pert).float().to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 logits=model(edge_index,X_pert_t)\n",
    "#                 prob=self._softmax(logits)[node,target_class].item()\n",
    "#             Z[s]=X_pert[node]; y[s]=prob\n",
    "#             d=np.linalg.norm((X_pert-X_np)[P_mask].ravel())\n",
    "#             w[s]=np.exp(-(d**2)/(2*(self.kernel_width**2)+1e-12))\n",
    "\n",
    "#         reg=Ridge(alpha=self.alpha_ridge,fit_intercept=True,random_state=self.random_state)\n",
    "#         reg.fit(Z,y,sample_weight=w); coefs=reg.coef_\n",
    "#         imp=coefs/(np.linalg.norm(coefs,ord=2)+1e-12)\n",
    "\n",
    "#         return dict(node=node,target_class=target_class,base_pred=base_pred,\n",
    "#                     base_probs=base_probs.tolist(),importances=imp.tolist())\n",
    "\n",
    "#     def save_plot(self, expl, out_png, feature_names=None, topk=20):\n",
    "#         imps=np.array(expl[\"importances\"]); idx=np.argsort(np.abs(imps))[::-1]\n",
    "#         if topk: idx=idx[:topk]\n",
    "#         labels=[f\"PC{j}\" for j in range(len(imps))] if feature_names is None else feature_names\n",
    "#         labels_sel=[labels[j] for j in idx]; vals=imps[idx]\n",
    "#         plt.figure(figsize=(8,max(3,0.35*len(idx))))\n",
    "#         plt.barh(range(len(idx)),vals); plt.yticks(range(len(idx)),labels_sel)\n",
    "#         plt.gca().invert_yaxis(); plt.title(f\"GraphLIME Node {expl['node']}\")\n",
    "#         plt.xlabel(\"importance\"); plt.tight_layout()\n",
    "#         Path(out_png).parent.mkdir(parents=True,exist_ok=True)\n",
    "#         plt.savefig(out_png,dpi=200); plt.close()\n",
    "\n",
    "# # =====================================================\n",
    "# # Example: Run scaling exp + GraphLIME explain\n",
    "# # =====================================================\n",
    "# set_seed(42)\n",
    "# device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Build dataset & train one EDUQGC\n",
    "# graphs=build_cora_ego_dataset(num_graphs=5,target_nodes=20,pca_dim=10,seed=42)\n",
    "# train, val, test=graphs[:3],graphs[3:4],graphs[4:]\n",
    "# model=EDUQGC(n_nodes=20,in_feats=10,num_classes=3).to(device)\n",
    "# opt=torch.optim.Adam(model.parameters(),lr=0.03)\n",
    "\n",
    "# for ep in range(5):\n",
    "#     tr_loss=train_epoch(model,train,opt,device)\n",
    "#     val_loss,val_acc=evaluate(model,val,device)\n",
    "#     print(f\"Epoch {ep+1} | tr_loss={tr_loss:.3f} | val_acc={val_acc:.3f}\")\n",
    "\n",
    "# # Run GraphLIME explanation\n",
    "# lime=GraphLIME(num_samples=128,sigma=0.1,hop_k=0)\n",
    "# expl=lime.explain(model,graphs[0],node=0,device=device)\n",
    "# print(\"Explanation importances:\",expl[\"importances\"][:5])\n",
    "# lime.save_plot(expl,\"cora_multi_exp/explanations/example_node0.png\")\n",
    "# print(\"Saved GraphLIME plot at cora_multi_exp/explanations/example_node0.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3a4a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[repro] Best-effort reproducibility (non-strict).\n",
      "\n",
      "[RUN] N20_F10_seed42  device=cuda\n",
      "Loaded dataset from cora_multi_exp/datasets/cora_N20_F10_seed42.pkl (graphs=20)\n",
      "Epoch 01 | tr_loss=0.953 | val_loss=0.804 | val_acc=0.688\n",
      "Epoch 02 | tr_loss=0.661 | val_loss=0.634 | val_acc=0.738\n",
      "Epoch 03 | tr_loss=0.508 | val_loss=0.540 | val_acc=0.812\n",
      "Epoch 04 | tr_loss=0.423 | val_loss=0.486 | val_acc=0.812\n",
      "Epoch 05 | tr_loss=0.376 | val_loss=0.449 | val_acc=0.838\n",
      "Epoch 06 | tr_loss=0.346 | val_loss=0.419 | val_acc=0.850\n",
      "Epoch 07 | tr_loss=0.327 | val_loss=0.394 | val_acc=0.863\n",
      "Epoch 08 | tr_loss=0.312 | val_loss=0.375 | val_acc=0.863\n",
      "Epoch 09 | tr_loss=0.300 | val_loss=0.349 | val_acc=0.875\n",
      "Epoch 10 | tr_loss=0.288 | val_loss=0.327 | val_acc=0.887\n",
      "\n",
      "[RUN] N20_F15_seed42  device=cuda\n",
      "Loaded dataset from cora_multi_exp/datasets/cora_N20_F15_seed42.pkl (graphs=20)\n",
      "Epoch 01 | tr_loss=0.927 | val_loss=0.785 | val_acc=0.725\n",
      "Epoch 02 | tr_loss=0.620 | val_loss=0.639 | val_acc=0.762\n",
      "Epoch 03 | tr_loss=0.479 | val_loss=0.528 | val_acc=0.787\n",
      "Epoch 04 | tr_loss=0.398 | val_loss=0.475 | val_acc=0.812\n",
      "Epoch 05 | tr_loss=0.351 | val_loss=0.443 | val_acc=0.838\n",
      "Epoch 06 | tr_loss=0.323 | val_loss=0.415 | val_acc=0.825\n",
      "Epoch 07 | tr_loss=0.302 | val_loss=0.395 | val_acc=0.825\n",
      "Epoch 08 | tr_loss=0.286 | val_loss=0.375 | val_acc=0.825\n",
      "Epoch 09 | tr_loss=0.271 | val_loss=0.356 | val_acc=0.850\n",
      "Epoch 10 | tr_loss=0.256 | val_loss=0.341 | val_acc=0.863\n",
      "\n",
      "[RUN] N25_F10_seed42  device=cuda\n",
      "Loaded dataset from cora_multi_exp/datasets/cora_N25_F10_seed42.pkl (graphs=20)\n",
      "Epoch 01 | tr_loss=1.084 | val_loss=0.895 | val_acc=0.630\n",
      "Epoch 02 | tr_loss=0.748 | val_loss=0.706 | val_acc=0.740\n",
      "Epoch 03 | tr_loss=0.568 | val_loss=0.607 | val_acc=0.770\n",
      "Epoch 04 | tr_loss=0.467 | val_loss=0.548 | val_acc=0.780\n",
      "Epoch 05 | tr_loss=0.411 | val_loss=0.511 | val_acc=0.790\n",
      "Epoch 06 | tr_loss=0.378 | val_loss=0.487 | val_acc=0.800\n",
      "Epoch 07 | tr_loss=0.355 | val_loss=0.468 | val_acc=0.800\n",
      "Epoch 08 | tr_loss=0.338 | val_loss=0.454 | val_acc=0.790\n",
      "Epoch 09 | tr_loss=0.324 | val_loss=0.444 | val_acc=0.830\n",
      "Epoch 10 | tr_loss=0.313 | val_loss=0.436 | val_acc=0.830\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 522\u001b[39m\n\u001b[32m    519\u001b[39m SEED = \u001b[32m42\u001b[39m\n\u001b[32m    520\u001b[39m BASE_DIR = \u001b[33m\"\u001b[39m\u001b[33m./cora_multi_exp\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m results = \u001b[43mrun_full_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mN_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mF_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_graphs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_GRAPHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gpu_qnode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraphlime_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgraphlime_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\n\u001b[32m    533\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone. Keys in results:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(results.keys()))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 430\u001b[39m, in \u001b[36mrun_full_experiment\u001b[39m\u001b[34m(N_list, F_list, num_graphs, epochs, lr, seed, base_dir, use_gpu_qnode, graphlime_nodes, graphlime_samples)\u001b[39m\n\u001b[32m    428\u001b[39m nodes_to_explain = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(graphlime_nodes, gtest[\u001b[33m'\u001b[39m\u001b[33mX\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m0\u001b[39m])))\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node_idx \u001b[38;5;129;01min\u001b[39;00m nodes_to_explain:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     out = \u001b[43mexplain_node_graphlime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraphlime_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m+\u001b[49m\u001b[43mnode_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     coeffs = out[\u001b[33m'\u001b[39m\u001b[33mcoeffs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    434\u001b[39m     explanations_summary[run_tag].append(\u001b[38;5;28mdict\u001b[39m(node=node_idx, coeffs=coeffs.tolist(), intercept=out[\u001b[33m'\u001b[39m\u001b[33mintercept\u001b[39m\u001b[33m'\u001b[39m]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 280\u001b[39m, in \u001b[36mexplain_node_graphlime\u001b[39m\u001b[34m(model, graph, node_idx, device, num_samples, sigma, alpha, random_state)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# convert to tensor\u001b[39;00m\n\u001b[32m    279\u001b[39m Xt = torch.from_numpy(Xp).float().to(device)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N, C]\u001b[39;00m\n\u001b[32m    281\u001b[39m probs = torch.softmax(logits[node_idx], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# choose target class as model prediction on original x0\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mEDUQGCNodeClassifier.forward\u001b[39m\u001b[34m(self, edge_index_torch, x_torch)\u001b[39m\n\u001b[32m    144\u001b[39m edge_index = edge_index_torch.to(device)\n\u001b[32m    145\u001b[39m X = x_torch.to(device).float()\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medge_phase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_psi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_psi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m expvals = torch.stack(out, dim=\u001b[32m0\u001b[39m).float().to(device)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m expvals.dim() == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/qnode.py:922\u001b[39m, in \u001b[36mQNode.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_capture_qnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_qnode  \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/qnode.py:895\u001b[39m, in \u001b[36mQNode._impl_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._transform_program.set_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m res = \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    904\u001b[39m res = res[\u001b[32m0\u001b[39m]\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/execution.py:233\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, device, diff_method, interface, grad_on_execution, cache, cachesize, max_diff, device_vjp, postselect_mode, mcm_method, gradient_kwargs, transform_program, executor_backend)\u001b[39m\n\u001b[32m    229\u001b[39m tapes, outer_post_processing = outer_transform(tapes)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m outer_transform.is_informative, \u001b[33m\"\u001b[39m\u001b[33mshould only contain device preprocessing\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m user_post_processing(outer_post_processing(results))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/run.py:338\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(tapes, device, config, inner_transform_program)\u001b[39m\n\u001b[32m    335\u001b[39m         params = tape.get_parameters(trainable_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    336\u001b[39m         tape.trainable_params = qml.math.get_trainable_indices(params)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m results = \u001b[43mml_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/interfaces/torch.py:240\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, execute_fn, jpc, device)\u001b[39m\n\u001b[32m    232\u001b[39m     parameters.extend(tape.get_parameters())\n\u001b[32m    234\u001b[39m kwargs = {\n\u001b[32m    235\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtapes\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(tapes),\n\u001b[32m    236\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexecute_fn\u001b[39m\u001b[33m\"\u001b[39m: execute_fn,\n\u001b[32m    237\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mjpc\u001b[39m\u001b[33m\"\u001b[39m: jpc,\n\u001b[32m    238\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExecuteTapes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/interfaces/torch.py:89\u001b[39m, in \u001b[36mpytreeify.<locals>.new_apply\u001b[39m\u001b[34m(*inp)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_apply\u001b[39m(*inp):\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Inputs already flat\u001b[39;00m\n\u001b[32m     88\u001b[39m     out_struct_holder = []\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     flat_out = \u001b[43morig_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_struct_holder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pytree.tree_unflatten(flat_out, out_struct_holder[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/torch/autograd/function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/interfaces/torch.py:93\u001b[39m, in \u001b[36mpytreeify.<locals>.new_forward\u001b[39m\u001b[34m(ctx, out_struct_holder, *inp)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(ctx, out_struct_holder, *inp):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     out = \u001b[43morig_fw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     flat_out, out_struct = pytree.tree_flatten(out)\n\u001b[32m     95\u001b[39m     ctx._out_struct = out_struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/interfaces/torch.py:162\u001b[39m, in \u001b[36mExecuteTapes.forward\u001b[39m\u001b[34m(ctx, kwargs, *parameters)\u001b[39m\n\u001b[32m    159\u001b[39m ctx.tapes = kwargs[\u001b[33m\"\u001b[39m\u001b[33mtapes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    160\u001b[39m ctx.jpc = kwargs[\u001b[33m\"\u001b[39m\u001b[33mjpc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m res = \u001b[38;5;28mtuple\u001b[39m(\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_fn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# if any input tensor uses the GPU, the output should as well\u001b[39;00m\n\u001b[32m    165\u001b[39m ctx.torch_device = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/jacobian_products.py:487\u001b[39m, in \u001b[36mDeviceDerivatives.execute_and_cache_jacobian\u001b[39m\u001b[34m(self, tapes)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logger.isEnabledFor(logging.DEBUG):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    486\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mForward pass called with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, tapes)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m results, jac = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dev_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._results_cache[tapes] = results\n\u001b[32m    489\u001b[39m \u001b[38;5;28mself\u001b[39m._jacs_cache[tapes] = jac\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/workflow/jacobian_products.py:452\u001b[39m, in \u001b[36mDeviceDerivatives._dev_execute_and_compute_derivatives\u001b[39m\u001b[34m(self, tapes)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[33;03mConverts tapes to numpy before computing the the results and derivatives on the device.\u001b[39;00m\n\u001b[32m    448\u001b[39m \n\u001b[32m    449\u001b[39m \u001b[33;03mDispatches between the two different device interfaces.\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    451\u001b[39m numpy_tapes, _ = qml.transforms.convert_to_numpy_parameters(tapes)\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_device\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_tapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execution_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/devices/modifiers/simulator_tracking.py:95\u001b[39m, in \u001b[36m_track_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracker.update(\n\u001b[32m     90\u001b[39m         execute_and_derivative_batches=\u001b[32m1\u001b[39m,\n\u001b[32m     91\u001b[39m         executions=\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[32m     92\u001b[39m         derivatives=\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[32m     93\u001b[39m     )\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracker.record()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muntracked_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/devices/modifiers/single_tape_support.py:60\u001b[39m, in \u001b[36m_make_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     58\u001b[39m     is_single_circuit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     59\u001b[39m     circuits = (circuits,)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m results, jacs = \u001b[43mbatch_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (results[\u001b[32m0\u001b[39m], jacs[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m (results, jacs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/devices/modifiers/simulator_tracking.py:95\u001b[39m, in \u001b[36m_track_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracker.update(\n\u001b[32m     90\u001b[39m         execute_and_derivative_batches=\u001b[32m1\u001b[39m,\n\u001b[32m     91\u001b[39m         executions=\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[32m     92\u001b[39m         derivatives=\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[32m     93\u001b[39m     )\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracker.record()\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muntracked_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane/devices/modifiers/single_tape_support.py:60\u001b[39m, in \u001b[36m_make_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m     58\u001b[39m     is_single_circuit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     59\u001b[39m     circuits = (circuits,)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m results, jacs = \u001b[43mbatch_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (results[\u001b[32m0\u001b[39m], jacs[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m (results, jacs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane_lightning/lightning_base/lightning_base.py:451\u001b[39m, in \u001b[36mLightningBase.execute_and_compute_derivatives\u001b[39m\u001b[34m(self, circuits, execution_config)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the results and jacobians of circuits at the same time.\u001b[39;00m\n\u001b[32m    442\u001b[39m \n\u001b[32m    443\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m \u001b[33;03m    Tuple: A numeric result of the computation and the gradient.\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    450\u001b[39m batch_obs = execution_config.device_options.get(\u001b[33m\"\u001b[39m\u001b[33mbatch_obs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._batch_obs)\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m results = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimulate_and_jacobian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdynamic_wires_from_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_statevector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwire_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wire_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*results))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane_lightning/lightning_base/lightning_base.py:452\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the results and jacobians of circuits at the same time.\u001b[39;00m\n\u001b[32m    442\u001b[39m \n\u001b[32m    443\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m \u001b[33;03m    Tuple: A numeric result of the computation and the gradient.\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    450\u001b[39m batch_obs = execution_config.device_options.get(\u001b[33m\"\u001b[39m\u001b[33mbatch_obs\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._batch_obs)\n\u001b[32m    451\u001b[39m results = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimulate_and_jacobian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdynamic_wires_from_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_statevector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwire_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wire_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m circuit \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[32m    459\u001b[39m )\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*results))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane_lightning/lightning_base/lightning_base.py:342\u001b[39m, in \u001b[36mLightningBase.simulate_and_jacobian\u001b[39m\u001b[34m(self, circuit, state, batch_obs, wire_map)\u001b[39m\n\u001b[32m    340\u001b[39m res = \u001b[38;5;28mself\u001b[39m.simulate(circuit, state)\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m jac = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mLightningAdjointJacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcalculate_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res, jac\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eqgc-experiments/.venv/lib/python3.12/site-packages/pennylane_lightning/lightning_gpu/_adjoint_jacobian.py:161\u001b[39m, in \u001b[36mLightningGPUAdjointJacobian.calculate_jacobian\u001b[39m\u001b[34m(self, tape)\u001b[39m\n\u001b[32m    154\u001b[39m     jac = \u001b[38;5;28mself\u001b[39m._jacobian_lightning.batched(\n\u001b[32m    155\u001b[39m         processed_data[\u001b[33m\"\u001b[39m\u001b[33mstate_vector\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    156\u001b[39m         processed_data[\u001b[33m\"\u001b[39m\u001b[33mobs_serialized\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    157\u001b[39m         processed_data[\u001b[33m\"\u001b[39m\u001b[33mops_serialized\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    158\u001b[39m         trainable_params,\n\u001b[32m    159\u001b[39m     )\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     jac = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jacobian_lightning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstate_vector\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobs_serialized\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mops_serialized\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m jac = np.array(jac)\n\u001b[32m    169\u001b[39m has_shape0 = \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mlen\u001b[39m(jac))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Single-cell notebook: EDUQGC + scaling + GraphLIME + reproducibility + saving + visualization\n",
    "# Run this entire cell in one notebook cell.\n",
    "\n",
    "# ---------------------------\n",
    "# ENV (must be set before torch import for deterministic cuBLAS)\n",
    "# ---------------------------\n",
    "import os\n",
    "# Recommended config for reproducible cuBLAS behavior when using deterministic algorithms on CUDA:\n",
    "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "\n",
    "# ---------------------------\n",
    "# Imports (after env above)\n",
    "# ---------------------------\n",
    "import sys, time, json, math, random, pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch & related (import after CUBLAS var set)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# PennyLane (optional GPU lightning backend)\n",
    "import pennylane as qml\n",
    "\n",
    "# PyG + sklearn (must be installed)\n",
    "try:\n",
    "    from torch_geometric.datasets import Planetoid\n",
    "    import torch_geometric.utils as pyg_utils\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"This notebook requires torch-geometric. Install per https://pytorch-geometric.readthedocs.io/\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility utility (per PyTorch docs)\n",
    "# ---------------------------\n",
    "def set_reproducibility(seed:int = 42, strict: bool = True):\n",
    "    \"\"\"\n",
    "    Set seeds and determinism flags. If strict=True, attempt to enable\n",
    "    torch.use_deterministic_algorithms(True) and require CUBLAS env var.\n",
    "    If strict is False, do a best-effort deterministic setup (faster).\n",
    "    Note: If you change CUBLAS_WORKSPACE_CONFIG you must restart the kernel.\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if strict:\n",
    "        try:\n",
    "            # This will raise an error if an op is non-deterministic on the platform\n",
    "            torch.use_deterministic_algorithms(True)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            print(\"[repro] Strict deterministic mode enabled.\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(\n",
    "                \"Could not enable strict deterministic algorithms. \"\n",
    "                \"If using CUDA ensure CUBLAS_WORKSPACE_CONFIG was set before Python start. \"\n",
    "                f\"Exception: {e}\"\n",
    "            )\n",
    "            # fallback to best-effort\n",
    "            torch.use_deterministic_algorithms(False)\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"[repro] Best-effort reproducibility (non-strict).\")\n",
    "\n",
    "# ---------------------------\n",
    "# EDUQGC model (unchanged semantics; parameterizable input dims & classes)\n",
    "# ---------------------------\n",
    "class EDUQGCNodeClassifier(nn.Module):\n",
    "    def __init__(self, n_nodes, in_feats, T=2, seed=0, use_gpu_qnode=True, use_feat_skip=True, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.T = T\n",
    "        self.use_feat_skip = use_feat_skip\n",
    "\n",
    "        # Packed parameters for encoders\n",
    "        self.enc_W = nn.Parameter(torch.randn(T, 2, in_feats) * 0.08)\n",
    "        self.enc_b = nn.Parameter(torch.randn(T, 2) * 0.02)\n",
    "\n",
    "        self.edge_phase  = nn.Parameter(torch.randn(T) * 0.08)\n",
    "        self.pre_theta   = nn.Parameter(torch.randn(T) * 0.08)\n",
    "        self.pre_psi     = nn.Parameter(torch.randn(T) * 0.08)\n",
    "        self.post_theta  = nn.Parameter(torch.randn(T) * 0.08)\n",
    "        self.post_psi    = nn.Parameter(torch.randn(T) * 0.08)\n",
    "\n",
    "        readin_dim = 1 + in_feats if use_feat_skip else 1\n",
    "        self.readout = nn.Linear(readin_dim, num_classes)\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        qdev_name = \"lightning.gpu\" if (use_gpu_qnode and use_cuda) else \"default.qubit\"\n",
    "        # safe fallback: if lightning.gpu not available, will raise - we catch in runner\n",
    "        try:\n",
    "            self.dev = qml.device(qdev_name, wires=n_nodes, shots=None)\n",
    "        except Exception as e:\n",
    "            # fallback to default.qubit\n",
    "            warnings.warn(f\"Could not create device '{qdev_name}': {e}. Falling back to default.qubit (CPU).\")\n",
    "            self.dev = qml.device(\"default.qubit\", wires=n_nodes, shots=None)\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"best\")\n",
    "        def circuit(edge_index, X, enc_W, enc_b,\n",
    "                    edge_phase, pre_theta, pre_psi, post_theta, post_psi):\n",
    "            # edge_index: [2, E], X: [N, F]\n",
    "            for t in range(self.T):\n",
    "                enc_out = X @ enc_W[t].T + enc_b[t]  # [N, 2]\n",
    "                alphas = enc_out[:, 0]; betas = enc_out[:, 1]\n",
    "                # data-encoding rotations\n",
    "                for i in range(self.n_nodes):\n",
    "                    qml.RX(alphas[i], wires=i)\n",
    "                    qml.RY(betas[i], wires=i)\n",
    "                # pre rotations\n",
    "                for i in range(self.n_nodes):\n",
    "                    qml.RZ(pre_psi[t], wires=i)\n",
    "                    qml.RX(pre_theta[t], wires=i)\n",
    "                # entanglers\n",
    "                E = edge_index.shape[1] if edge_index.ndim == 2 else 0\n",
    "                for e in range(E):\n",
    "                    u = int(edge_index[0, e].item()); v = int(edge_index[1, e].item())\n",
    "                    if u != v:\n",
    "                        qml.ControlledPhaseShift(edge_phase[t], wires=[u, v])\n",
    "                # post rotations\n",
    "                for i in range(self.n_nodes):\n",
    "                    qml.RZ(post_psi[t], wires=i)\n",
    "                    qml.RX(post_theta[t], wires=i)\n",
    "            return [qml.expval(qml.Z(i)) for i in range(self.n_nodes)]\n",
    "        self._circuit = circuit\n",
    "\n",
    "    def forward(self, edge_index_torch, x_torch):\n",
    "        device = next(self.parameters()).device\n",
    "        edge_index = edge_index_torch.to(device)\n",
    "        X = x_torch.to(device).float()\n",
    "\n",
    "        out = self._circuit(edge_index, X,\n",
    "                            self.enc_W, self.enc_b,\n",
    "                            self.edge_phase,\n",
    "                            self.pre_theta, self.pre_psi,\n",
    "                            self.post_theta, self.post_psi)\n",
    "        expvals = torch.stack(out, dim=0).float().to(device)\n",
    "        if expvals.dim() == 1:\n",
    "            expvals = expvals.unsqueeze(1)\n",
    "        elif expvals.dim() == 2 and expvals.shape[1] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            expvals = expvals.squeeze(-1).unsqueeze(1)\n",
    "\n",
    "        readin = torch.cat([expvals, X], dim=1) if self.use_feat_skip else expvals\n",
    "        logits = self.readout(readin)\n",
    "        return logits\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset builder: Cora -> k-hop ego graphs -> PCA -> choose 3 labels\n",
    "# Returns list of dicts {edge_index: [2,E], X: [N,F], y: [N]}\n",
    "# ---------------------------\n",
    "def build_cora_ego_dataset(num_graphs=20, chosen_labels=(0,1,2), n_hops=2,\n",
    "                           target_nodes=20, pca_dim=10, seed=42, save_path=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dataset = Planetoid(root=\"./cora_data\", name=\"Cora\")\n",
    "    data = dataset[0]\n",
    "    x_all = data.x.numpy().astype(np.float32)\n",
    "    y_all = data.y.numpy().astype(np.int64)\n",
    "    edge_index_all = data.edge_index.numpy()\n",
    "\n",
    "    chosen_set = set(chosen_labels)\n",
    "    eligible = np.array([i for i in range(len(y_all)) if y_all[i] in chosen_set], dtype=np.int64)\n",
    "    if len(eligible) == 0:\n",
    "        raise RuntimeError(\"No eligible nodes for chosen_labels.\")\n",
    "\n",
    "    # Fit PCA on eligible nodes features for stable transform\n",
    "    pca = PCA(n_components=pca_dim, random_state=seed)\n",
    "    pca.fit(x_all[eligible])\n",
    "\n",
    "    def pick_subgraph(center_id):\n",
    "        # get k-hop subgraph nodes (use torch_geometric utility)\n",
    "        nodes = pyg_utils.k_hop_subgraph(torch.tensor([center_id], dtype=torch.long), n_hops,\n",
    "                                        torch.as_tensor(edge_index_all, dtype=torch.long), relabel_nodes=False)[0].numpy()\n",
    "        # keep only nodes with chosen labels\n",
    "        nodes = np.array([n for n in nodes if y_all[n] in chosen_set], dtype=np.int64)\n",
    "        unique = list(dict.fromkeys(nodes.tolist()))  # preserve order, unique\n",
    "        # pad if necessary\n",
    "        if len(unique) < target_nodes:\n",
    "            extras = [n for n in eligible if n not in unique]\n",
    "            rng.shuffle(extras)\n",
    "            take = extras[: max(0, target_nodes - len(unique))]\n",
    "            unique = unique + list(take)\n",
    "        # trim to exact size\n",
    "        if len(unique) > target_nodes:\n",
    "            rng.shuffle(unique)\n",
    "            unique = unique[:target_nodes]\n",
    "        nodes = np.array(unique, dtype=np.int64)\n",
    "        assert len(nodes) == target_nodes\n",
    "\n",
    "        # induced edges\n",
    "        node_set = set(nodes.tolist())\n",
    "        mask = np.array([(u in node_set and v in node_set) for u, v in zip(edge_index_all[0], edge_index_all[1])])\n",
    "        sub_edges = edge_index_all[:, mask]\n",
    "        relabel = {old: new for new, old in enumerate(nodes)}\n",
    "        if sub_edges.shape[1] > 0:\n",
    "            remapped = np.array([[relabel[int(u)], relabel[int(v)]] for u, v in sub_edges.T], dtype=np.int64).T\n",
    "            # make undirected pairs and deduplicate\n",
    "            edges = set()\n",
    "            for u, v in remapped.T:\n",
    "                edges.add((u, v)); edges.add((v, u))\n",
    "            u_list, v_list = zip(*sorted(edges))\n",
    "            edge_index = np.vstack([u_list, v_list]).astype(np.int64)\n",
    "        else:\n",
    "            # No edges => self-loops to keep circuit valid\n",
    "            edge_index = np.vstack([np.arange(target_nodes), np.arange(target_nodes)]).astype(np.int64)\n",
    "\n",
    "        # PCA compress\n",
    "        X_sub = pca.transform(x_all[nodes]).astype(np.float32)\n",
    "        # relabel y to 0..len(chosen_labels)-1\n",
    "        map_lbl = {old: new for new, old in enumerate(chosen_labels)}\n",
    "        y_sub = np.array([map_lbl[int(lbl)] for lbl in y_all[nodes]], dtype=np.int64)\n",
    "        return dict(edge_index=edge_index, X=X_sub, y=y_sub)\n",
    "\n",
    "    # Choose deterministic centers from eligible\n",
    "    rng.shuffle(eligible)\n",
    "    centers = eligible[:num_graphs] if len(eligible) >= num_graphs else np.resize(eligible, num_graphs)\n",
    "    graphs = [pick_subgraph(int(c)) for c in centers]\n",
    "\n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(dict(graphs=graphs, meta=dict(dataset=\"Cora\", chosen_labels=tuple(chosen_labels),\n",
    "                                                       n_hops=n_hops, target_nodes=target_nodes,\n",
    "                                                       pca_dim=pca_dim, seed=seed)), f)\n",
    "        print(f\"[dataset saved] {Path(save_path).resolve()}  (graphs={len(graphs)})\")\n",
    "\n",
    "    return graphs\n",
    "\n",
    "# ---------------------------\n",
    "# GraphLIME: local explanations by perturbing node features\n",
    "# - For a target node, perturb its features, keep other nodes same.\n",
    "# - Query model probabilities for the predicted class, weight by exponential kernel on L2,\n",
    "# - Fit a weighted Ridge linear model: perturbed_features -> model_score\n",
    "# - Return coefficients as importances (signed).\n",
    "# ---------------------------\n",
    "def explain_node_graphlime(model, graph, node_idx, device='cpu', num_samples=500, sigma=0.1, alpha=1.0, random_state=0):\n",
    "    \"\"\"\n",
    "    graph: dict(edge_index, X, y)\n",
    "    node_idx: int (0..N-1)\n",
    "    returns: dict(coeffs, intercept, scores_sampled, weights)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    edge_index = torch.from_numpy(graph['edge_index']).long().to(device)\n",
    "    X_all = graph['X'].astype(np.float32).copy()\n",
    "    N, F = X_all.shape\n",
    "\n",
    "    # baseline feature vector for node\n",
    "    x0 = X_all[node_idx].astype(np.float32)\n",
    "\n",
    "    # sample perturbations around x0 (normal noise)\n",
    "    # we sample in PCA space (already compressed)\n",
    "    noise = rng.normal(scale=sigma, size=(num_samples, F)).astype(np.float32)\n",
    "    samples = x0[None, :] + noise  # [S, F]\n",
    "\n",
    "    # build batch inputs where only node_idx features change\n",
    "    # We'll call model S times; for speed we can batch but QNode may not accept batching.\n",
    "    # So we do sequential calls; for small S it's OK. If slow, reduce num_samples.\n",
    "    scores = np.zeros(num_samples, dtype=np.float32)\n",
    "    for i in range(num_samples):\n",
    "        Xp = X_all.copy()\n",
    "        Xp[node_idx] = samples[i]\n",
    "        # convert to tensor\n",
    "        Xt = torch.from_numpy(Xp).float().to(device)\n",
    "        logits = model(edge_index, Xt)  # [N, C]\n",
    "        probs = torch.softmax(logits[node_idx], dim=0)\n",
    "        # choose target class as model prediction on original x0\n",
    "        scores[i] = float(probs.max().detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    # original score baseline for distances/weights\n",
    "    # compute distances between samples and x0 in L2\n",
    "    dists = np.linalg.norm(samples - x0[None, :], axis=1)\n",
    "    # kernel weights\n",
    "    weights = np.exp(- (dists ** 2) / (2 * (sigma ** 2) ) )\n",
    "\n",
    "    # fit weighted linear model: samples -> scores\n",
    "    clf = Ridge(alpha=alpha)\n",
    "    clf.fit(samples, scores, sample_weight=weights)\n",
    "    coeffs = clf.coef_.astype(float)   # shape (F,)\n",
    "    intercept = float(clf.intercept_)\n",
    "\n",
    "    return dict(coeffs=coeffs, intercept=intercept, samples=samples, scores=scores, weights=weights, x0=x0)\n",
    "\n",
    "# ---------------------------\n",
    "# Train / evaluate helpers\n",
    "# ---------------------------\n",
    "def train_epoch(model, graphs, optimizer, device=\"cpu\"):\n",
    "    model.train()\n",
    "    total_loss = 0.0; total_nodes = 0\n",
    "    for g in graphs:\n",
    "        e = torch.from_numpy(g['edge_index']).long().to(device)\n",
    "        X = torch.from_numpy(g['X']).float().to(device)\n",
    "        y = torch.from_numpy(g['y']).long().to(device)\n",
    "        logits = model(e, X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "        total_nodes += X.shape[0]\n",
    "    return total_loss / max(1, total_nodes)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, graphs, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_nodes = 0; total_loss = 0.0; correct = 0\n",
    "    for g in graphs:\n",
    "        e = torch.from_numpy(g['edge_index']).long().to(device)\n",
    "        X = torch.from_numpy(g['X']).float().to(device)\n",
    "        y = torch.from_numpy(g['y']).long().to(device)\n",
    "        logits = model(e, X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "        total_nodes += X.shape[0]\n",
    "    return total_loss / max(1, total_nodes), correct / max(1, total_nodes)\n",
    "\n",
    "# ---------------------------\n",
    "# Scaling experiment runner + GraphLIME explanation aggregation\n",
    "# ---------------------------\n",
    "# ---------------------------\n",
    "# Scaling experiment runner + GraphLIME explanation aggregation\n",
    "# ---------------------------\n",
    "def run_full_experiment(\n",
    "    N_list = [20,25,30,35],\n",
    "    F_list = [10,15,20,25],\n",
    "    num_graphs = 20,\n",
    "    epochs = 10,\n",
    "    lr = 0.03,\n",
    "    seed = 42,\n",
    "    base_dir = \"./cora_multi_exp\",\n",
    "    use_gpu_qnode = True,\n",
    "    graphlime_nodes = 10,   # number of nodes to explain per dataset (for aggregation)\n",
    "    graphlime_samples = 400\n",
    "):\n",
    "    set_reproducibility(seed, strict=False)  # strict can be toggled\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Path(base_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    combos = list(itertools.product(N_list, F_list))\n",
    "    summary = []\n",
    "    timings = []\n",
    "    explanations_summary = {}\n",
    "\n",
    "    for (N, F) in combos:\n",
    "        run_tag = f\"N{N}_F{F}_seed{seed}\"\n",
    "        print(f\"\\n[RUN] {run_tag}  device={device}\")\n",
    "\n",
    "        ds_path = Path(base_dir)/\"datasets\"/f\"cora_N{N}_F{F}_seed{seed}.pkl\"\n",
    "        ds_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if ds_path.exists():\n",
    "            with open(ds_path, \"rb\") as f:\n",
    "                payload = pickle.load(f)\n",
    "            graphs = payload['graphs']\n",
    "            print(f\"Loaded dataset from {ds_path} (graphs={len(graphs)})\")\n",
    "        else:\n",
    "            graphs = build_cora_ego_dataset(num_graphs=num_graphs, chosen_labels=(0,1,2),\n",
    "                                           n_hops=2, target_nodes=N, pca_dim=F, seed=seed, save_path=str(ds_path))\n",
    "        # Split\n",
    "        n = len(graphs)\n",
    "        ntr = int(0.6 * n); nval = int(0.2 * n)\n",
    "        train_graphs = graphs[:ntr]; val_graphs = graphs[ntr:ntr+nval]; test_graphs = graphs[ntr+nval:]\n",
    "\n",
    "        # model\n",
    "        model = EDUQGCNodeClassifier(n_nodes=N, in_feats=F, T=2, seed=seed, use_gpu_qnode=use_gpu_qnode, num_classes=3).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        start_time = time.time()\n",
    "        history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "        best_val_acc = -1.0; best_state = None\n",
    "        for ep in range(1, epochs+1):\n",
    "            tr_loss = train_epoch(model, train_graphs, opt, device=device)\n",
    "            val_loss, val_acc = evaluate(model, val_graphs, device=device)\n",
    "            history['train_loss'].append(tr_loss); history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "            print(f\"Epoch {ep:02d} | tr_loss={tr_loss:.3f} | val_loss={val_loss:.3f} | val_acc={val_acc:.3f}\")\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "        run_time = time.time() - start_time\n",
    "\n",
    "        # load best\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        # eval final\n",
    "        tr_loss, tr_acc = evaluate(model, train_graphs, device=device)\n",
    "        va_loss, va_acc = evaluate(model, val_graphs, device=device)\n",
    "        te_loss, te_acc = evaluate(model, test_graphs, device=device)\n",
    "\n",
    "        # save model\n",
    "        model_dir = Path(base_dir)/\"models\"\n",
    "        model_dir.mkdir(exist_ok=True, parents=True)\n",
    "        model_path = model_dir / f\"eduqgc_{run_tag}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # save run info\n",
    "        run_info = dict(tag=run_tag, N=N, F=F, seed=seed,\n",
    "                        tr_loss=tr_loss, tr_acc=tr_acc,\n",
    "                        val_loss=va_loss, val_acc=va_acc,\n",
    "                        test_loss=te_loss, test_acc=te_acc,\n",
    "                        runtime_sec=run_time, model_path=str(model_path), dataset_path=str(ds_path))\n",
    "        summary.append(run_info)\n",
    "        timings.append(run_info)\n",
    "\n",
    "        # GraphLIME explanations\n",
    "        expl_dir = Path(base_dir)/\"explanations\"/run_tag\n",
    "        expl_dir.mkdir(parents=True, exist_ok=True)\n",
    "        explanations_summary[run_tag] = []\n",
    "        if len(test_graphs) > 0:\n",
    "            gtest = test_graphs[0]\n",
    "            nodes_to_explain = list(range(min(graphlime_nodes, gtest['X'].shape[0])))\n",
    "            for node_idx in nodes_to_explain:\n",
    "                out = explain_node_graphlime(model, gtest, node_idx, device=str(device),\n",
    "                                             num_samples=graphlime_samples, sigma=0.2,\n",
    "                                             alpha=1.0, random_state=seed+node_idx)\n",
    "                coeffs = out['coeffs']\n",
    "                explanations_summary[run_tag].append(dict(node=node_idx, coeffs=coeffs.tolist(), intercept=out['intercept']))\n",
    "                k = min(10, len(coeffs))\n",
    "                idxs = np.argsort(np.abs(coeffs))[::-1][:k]\n",
    "                labels = [f\"PC{i}\" for i in idxs]\n",
    "                vals = coeffs[idxs]\n",
    "                plt.figure(figsize=(8,4))\n",
    "                y_pos = np.arange(len(labels))\n",
    "                plt.barh(y_pos, vals[::-1])\n",
    "                plt.yticks(y_pos, labels[::-1])\n",
    "                plt.xlabel(\"importance\")\n",
    "                plt.title(f\"GraphLIME {run_tag} node {node_idx}\")\n",
    "                plt.tight_layout()\n",
    "                png_path = expl_dir / f\"glime_node{node_idx}.png\"\n",
    "                plt.savefig(png_path, dpi=200)\n",
    "                plt.close()\n",
    "\n",
    "        # --- ensure logs dir exists ---\n",
    "        log_dir = Path(base_dir)/\"logs\"\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # flush to disk per run\n",
    "        with open(log_dir/f\"{run_tag}.json\", \"w\") as f:\n",
    "            json.dump(run_info, f, indent=2)\n",
    "        with open(Path(base_dir)/\"explanations\"/f\"{run_tag}_explanations.json\", \"w\") as f:\n",
    "            json.dump(explanations_summary.get(run_tag, []), f, indent=2)\n",
    "\n",
    "    # Save summary table\n",
    "    out_summary = Path(base_dir)/\"logs\"/f\"scaling_summary_{datetime.now().strftime('%Y%m%d-%H%M%S')}.json\"\n",
    "    out_summary.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_summary, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"\\nAll runs finished. Summary saved at {out_summary}\")\n",
    "\n",
    "    # Plot runtime heatmap and accuracy heatmap aggregated across combos\n",
    "    combos_sorted = sorted(summary, key=lambda r: (r['N'], r['F']))\n",
    "    Ns = sorted(set(r['N'] for r in combos_sorted))\n",
    "    Fs = sorted(set(r['F'] for r in combos_sorted))\n",
    "    runtime_mat = np.zeros((len(Ns), len(Fs)))\n",
    "    testacc_mat = np.zeros((len(Ns), len(Fs)))\n",
    "    for r in combos_sorted:\n",
    "        i = Ns.index(r['N']); j = Fs.index(r['F'])\n",
    "        runtime_mat[i,j] = r['runtime_sec']\n",
    "        testacc_mat[i,j] = r['test_acc']\n",
    "\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "    im = axes[0].imshow(runtime_mat, aspect='auto')\n",
    "    axes[0].set_xticks(range(len(Fs))); axes[0].set_yticks(range(len(Ns)))\n",
    "    axes[0].set_xticklabels(Fs); axes[0].set_yticklabels(Ns)\n",
    "    axes[0].set_xlabel(\"F (PCA dim)\"); axes[0].set_ylabel(\"N (nodes)\")\n",
    "    axes[0].set_title(\"Runtime (sec) per N,F\")\n",
    "    fig.colorbar(im, ax=axes[0])\n",
    "\n",
    "    im2 = axes[1].imshow(testacc_mat, aspect='auto', vmin=0, vmax=1)\n",
    "    axes[1].set_xticks(range(len(Fs))); axes[1].set_yticks(range(len(Ns)))\n",
    "    axes[1].set_xticklabels(Fs); axes[1].set_yticklabels(Ns)\n",
    "    axes[1].set_xlabel(\"F (PCA dim)\"); axes[1].set_ylabel(\"N (nodes)\")\n",
    "    axes[1].set_title(\"Test accuracy per N,F\")\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "    plt.tight_layout()\n",
    "    fig_path = Path(base_dir)/\"logs\"/f\"scaling_summary_plot_{datetime.now().strftime('%Y%m%d-%H%M%S')}.png\"\n",
    "    plt.savefig(fig_path, dpi=200)\n",
    "    plt.show()\n",
    "    print(f\"Saved plot: {fig_path}\")\n",
    "\n",
    "    # Save explanations summary\n",
    "    expl_sum_path = Path(base_dir)/\"explanations\"/\"explanations_summary.json\"\n",
    "    expl_sum_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(expl_sum_path, \"w\") as f:\n",
    "        json.dump(explanations_summary, f, indent=2)\n",
    "\n",
    "    return dict(summary=summary, timings=timings,\n",
    "                explanations=explanations_summary,\n",
    "                plot_path=str(fig_path),\n",
    "                summary_path=str(out_summary))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example: run everything with small defaults\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # standard parameters\n",
    "    N_list = [20,25]          # reduce for quick demo; change to [20,25,30,35] for full\n",
    "    F_list = [10,15]          # reduce for quick demo; change to [10,15,20,25] for full\n",
    "    NUM_GRAPHS = 20\n",
    "    EPOCHS = 10\n",
    "    LR = 0.03\n",
    "    SEED = 42\n",
    "    BASE_DIR = \"./cora_multi_exp\"\n",
    "\n",
    "    results = run_full_experiment(\n",
    "        N_list=N_list,\n",
    "        F_list=F_list,\n",
    "        num_graphs=NUM_GRAPHS,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LR,\n",
    "        seed=SEED,\n",
    "        base_dir=BASE_DIR,\n",
    "        use_gpu_qnode=True,\n",
    "        graphlime_nodes=5,\n",
    "        graphlime_samples=300\n",
    "    )\n",
    "    print(\"Done. Keys in results:\", list(results.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d045880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
