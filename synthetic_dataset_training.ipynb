{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "er0xRbYNoF6E"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets.molecule_net import MoleculeNet\n",
    "import random \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWYbSE9Fa8YD"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QHAKAS8ssJB_"
   },
   "outputs": [],
   "source": [
    "def gen_cycle_pairs(sizes):\n",
    "  pairs = []\n",
    "  for n in sizes:\n",
    "    for k in range(3, n-2):\n",
    "      single = nx.cycle_graph(n)\n",
    "      disjoint = nx.disjoint_union(nx.cycle_graph(k),\n",
    "                                   nx.cycle_graph(n-k))\n",
    "      pairs.append((single, disjoint))\n",
    "  return pairs\n",
    "\n",
    "\n",
    "def to_pyg(g, label):\n",
    "  data = torch_geometric.utils.from_networkx(g)\n",
    "  data.x = torch.zeros((g.number_of_nodes(), 50))\n",
    "  data.y = torch.tensor([label])\n",
    "  return data\n",
    "\n",
    "\n",
    "def cycles_dataset(sizes):\n",
    "  graph_pairs = gen_cycle_pairs(sizes)\n",
    "  data = sum(([to_pyg(g1, 1), to_pyg(g2, 0)] for (g1,g2) in graph_pairs), [])\n",
    "  return data\n",
    "\n",
    "\n",
    "def cycles_dict(n1, n2):\n",
    "  return {i: to_pyg(nx.cycle_graph(i), 1) for i in range(n1, n2+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UurrtCpMsPVP",
    "outputId": "5157b55c-8299-4a00-d1f2-7b46aa003400"
   },
   "outputs": [],
   "source": [
    "cycles_train_loader = torch_geometric.data.DataLoader(cycles_dataset([6, 7, 9, 10]), batch_size=1, shuffle=True)\n",
    "cycles_test_loader = torch_geometric.data.DataLoader(cycles_dataset([8]), batch_size=1, shuffle=True)\n",
    "cycles = cycles_dict(3,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Jxs2eBudkls0"
   },
   "outputs": [],
   "source": [
    "def prepare_tinymolhiv():\n",
    "  tiny_molhiv = MoleculeNet(root = './molhiv', name='HIV', \n",
    "                            pre_filter = lambda g: len(g.x) <= 10)  \n",
    "  pos_samples = [g for g in tiny_molhiv if g.y == 1]\n",
    "  neg_samples = [g for g in tiny_molhiv if g.y == 0]\n",
    "  random.seed(0)\n",
    "  random.shuffle(pos_samples)\n",
    "  random.shuffle(neg_samples)\n",
    "  neg_samples = neg_samples[:3*len(pos_samples)]\n",
    "  \n",
    "  for g in pos_samples:\n",
    "    g.y = g.y.squeeze(dim=0)\n",
    "  for g in neg_samples:\n",
    "    g.y = g.y.squeeze(dim=0)\n",
    "  \n",
    "  pos_splits = [pos_samples[i::5] for i in range(5)]\n",
    "  neg_splits = [neg_samples[i::5] for i in range(5)]\n",
    "\n",
    "  splits = [3*pos_splits[i] + neg_splits[i] for i in range(5)]\n",
    "  for i in range(5):\n",
    "    random.shuffle(splits[i])\n",
    "    for g in splits[i]:\n",
    "      g.x = g.x.float()\n",
    "  return splits\n",
    "\n",
    "def prepare_loaders(splits, eval_split=0):\n",
    "  train_data = sum((splits[i] for i in range(len(splits)) if not i == eval_split), [])\n",
    "  eval_data = splits[eval_split]\n",
    "  train_loader = torch_geometric.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "  eval_loader = torch_geometric.data.DataLoader(eval_data, batch_size=1, shuffle=True)\n",
    "  return train_loader, eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MVyhptOEmuzH"
   },
   "outputs": [],
   "source": [
    "hiv_splits = prepare_tinymolhiv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lS_aFO8_bAgY"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHtZ4SNybCa7"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uSV3fO30FH9B"
   },
   "outputs": [],
   "source": [
    "def tensorpow_squaremat(t, n):\n",
    "  res = t\n",
    "  start_dim = t.shape[0]\n",
    "  dim = start_dim\n",
    "  for i in range(n-1):\n",
    "    res = torch.tensordot(res, t, dims=0)\n",
    "    res = res.permute((0, 2, 1, 3))\n",
    "    dim *= start_dim    \n",
    "    res = res.reshape((dim, dim))\n",
    "  return res\n",
    "\n",
    "def tensormul_vecs(terms):\n",
    "  res = terms[0]\n",
    "  start_dim = res.shape[0]\n",
    "  dim = start_dim\n",
    "  for t in terms[1:]:\n",
    "    res = torch.tensordot(res, t, dims=0)\n",
    "    dim *= start_dim    \n",
    "    res = res.reshape(dim)\n",
    "  return res\n",
    "\n",
    "def hermitian(t):\n",
    "  return t + t.conj().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETkLNFY5bFuK"
   },
   "source": [
    "### EQGC classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ejdrBz81Rswc"
   },
   "outputs": [],
   "source": [
    "class EDU_QGC(torch.nn.Module):\n",
    "  def __init__(self, qb_per_node=1, n_layers = 1, init_u3=False):\n",
    "    super(EDU_QGC, self).__init__()\n",
    "    self.qb_per_node = qb_per_node\n",
    "    self.node_state_dim = 2 ** qb_per_node\n",
    "    self.n_layers = n_layers\n",
    "    self.init_u3 = init_u3\n",
    "    self.node_halfH = torch.nn.ParameterList([\n",
    "      torch.nn.Parameter(\n",
    "          torch.randn((self.node_state_dim, self.node_state_dim), dtype=torch.cfloat)) \n",
    "      for i in range(n_layers)\n",
    "    ])\n",
    "    self.edge_D = torch.nn.ParameterList([\n",
    "      torch.nn.Parameter(\n",
    "          torch.randn(self.node_state_dim ** 2))\n",
    "      for i in range(n_layers)\n",
    "    ])\n",
    "\n",
    "  def init_state(self, xs):\n",
    "    if self.init_u3:\n",
    "      node_states = [\n",
    "        torch.tensor([torch.cos(feat[0]), torch.exp(1j*feat[1])*torch.sin(feat[0])])\n",
    "        for feat in xs\n",
    "      ]\n",
    "      return tensormul_vecs(node_states)\n",
    "    else:\n",
    "      n_nodes = len(xs)      \n",
    "      full_dim = self.node_state_dim ** n_nodes\n",
    "      return torch.ones(full_dim, dtype = torch.cfloat) / np.sqrt(full_dim)\n",
    "\n",
    "  def prep_node_layer(self, node_halfH, n_nodes):\n",
    "    node_H = hermitian(node_halfH)\n",
    "    node_U = torch.matrix_exp(1j * node_H)\n",
    "    return tensorpow_squaremat(node_U, n_nodes)\n",
    "\n",
    "  def prep_edge_layer(self, edge_D, n_nodes, edge_index):\n",
    "    full_dim = self.node_state_dim ** n_nodes\n",
    "    v = torch.ones(full_dim, dtype=torch.cfloat)      \n",
    "    for n1,n2 in edge_index.T:\n",
    "      d = torch.exp(1j*edge_D)\n",
    "      d = d.reshape(self.node_state_dim, self.node_state_dim)\n",
    "      d = d.repeat([self.node_state_dim]*(n_nodes-2)+[1,1])\n",
    "      if n2 > n1:\n",
    "        perm = list(range(n1)) + [n_nodes-2] + list(range(n1, n2-1)) + [n_nodes-1] + list(range(n2-1, n_nodes-2))\n",
    "      else:\n",
    "        perm = list(range(n2)) + [n_nodes-1] + list(range(n2, n1-1)) + [n_nodes-2] + list(range(n1-1, n_nodes-2))\n",
    "      d = d.permute(perm).flatten()\n",
    "      v *= d\n",
    "    return v\n",
    "\n",
    "  def forward(self, g):\n",
    "    state = self.init_state(g.x)\n",
    "    n = len(g.x)\n",
    "    for i in range(self.n_layers):\n",
    "      \n",
    "      edge_d = self.prep_edge_layer(self.edge_D[i], n, g.edge_index)\n",
    "      state *= edge_d\n",
    "      node_u = self.prep_node_layer(self.node_halfH[i], n)\n",
    "      state = node_u @ state\n",
    "    probs = torch.square(torch.abs(state))\n",
    "    probs = probs / probs.sum() # normalize for floating point inaccuracies\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBVxV9YVbI9o"
   },
   "source": [
    "### Aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "M7KzqgKxa4Gu"
   },
   "outputs": [],
   "source": [
    "class OneCountAggregator(torch.nn.Module):\n",
    "  def __init__(self, max_graph_size, verbose = False):\n",
    "    super(OneCountAggregator, self).__init__()\n",
    "    self.max_n = max_graph_size\n",
    "    self.w = torch.nn.Parameter(torch.zeros(max_graph_size+1))\n",
    "    self.verbose = verbose\n",
    "  \n",
    "  def forward(self, probs):\n",
    "    count_probs = torch.zeros(self.max_n+1)\n",
    "    for s in range(len(probs)):\n",
    "      ones = 0\n",
    "      for i in range(self.max_n):\n",
    "        if (s & (1 << i)):\n",
    "          ones += 1\n",
    "      count_probs[ones] += probs[s]\n",
    "    if self.verbose:\n",
    "      print(\"count probs\", count_probs)\n",
    "    cond_probs = torch.sigmoid(self.w)\n",
    "    total_prob = cond_probs @ count_probs # sum P(count = i) x P(pos | count = i)\n",
    "    return total_prob\n",
    "\n",
    "\n",
    "class OneRatioAggregator(torch.nn.Module):\n",
    "  def __init__(self, mlp_hidden_dim = 15, verbose = False):\n",
    "    super(OneRatioAggregator, self).__init__()\n",
    "    self.mlp = torch.nn.Sequential(\n",
    "        torch.nn.Linear(1, mlp_hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(mlp_hidden_dim, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "    )\n",
    "    self.verbose = verbose\n",
    "  \n",
    "  def forward(self, probs):\n",
    "    max_ones = np.ceil(np.log2(len(probs))).astype('int')\n",
    "    count_probs = torch.zeros(max_ones+1)\n",
    "    for s in range(len(probs)):\n",
    "      ones = 0\n",
    "      for i in range(max_ones):\n",
    "        if (s & (1 << i)):\n",
    "          ones += 1\n",
    "      count_probs[ones] += probs[s]\n",
    "    if self.verbose:\n",
    "      print(\"count probs\", count_probs)\n",
    "    ratios = torch.linspace(0.0, 1.0, max_ones+1)\n",
    "    cond_probs = self.mlp(ratios.reshape(-1,1)).reshape(-1)\n",
    "    total_prob = cond_probs @ count_probs # sum P(count = i) x P(pos | count = i)\n",
    "    return total_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzZ6EL2FbLjD"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1GRW99jkQel"
   },
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AIVE0jl0Mex6"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, lr_scheduler=None, epochs=200, loader=cycles_train_loader, print_metrics=True):\n",
    "  model.train()\n",
    "  for i in range(epochs):\n",
    "    total = 0\n",
    "    correct = 0.0\n",
    "    rso50 = 0\n",
    "    rso55 = 0\n",
    "    loss_sum = 0\n",
    "    min_margin = 0.5\n",
    "    for g in loader: \n",
    "      optimizer.zero_grad()   \n",
    "      out = model(g).unsqueeze(0)\n",
    "      loss = F.binary_cross_entropy(out, g.y.float())  \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total += 1\n",
    "      loss_sum += loss.detach().numpy()      \n",
    "      p = out.detach().numpy()[0]\n",
    "      if g.y == 1:\n",
    "        correct += p\n",
    "        if p >= 0.5:\n",
    "          rso50 += 1\n",
    "          min_margin = min(p-0.5, min_margin)\n",
    "      else:\n",
    "        correct += (1 - p)\n",
    "        if p < 0.5:\n",
    "          rso50 += 1\n",
    "          min_margin = min(0.5-p, min_margin)\n",
    "    if print_metrics:\n",
    "      print(\"Epoch \", i)\n",
    "      print(\"Loss: \", loss_sum/total)\n",
    "      print(\"Acc: \", correct/total)\n",
    "      print(\"RSo50: \", rso50/total)\n",
    "      print(\"MinMargin: \", min_margin)\n",
    "    else:\n",
    "      print(\"Epoch \", i, \", loss \", loss_sum/total)\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    if i == epochs-1:\n",
    "      return {\n",
    "          'Loss': loss_sum/total,\n",
    "          'Acc:': correct/total,\n",
    "          'rso50': rso50/total,\n",
    "          'margin': min_margin\n",
    "      }\n",
    "\n",
    "\n",
    "def evaluate(model, loader=cycles_test_loader):\n",
    "  total = 0\n",
    "  correct = 0.0\n",
    "  rso50 = 0\n",
    "  min_margin = 0.5\n",
    "  loss_sum = 0\n",
    "  with torch.no_grad():\n",
    "    for g in loader: \n",
    "      out = model(g).unsqueeze(0)\n",
    "      loss = F.binary_cross_entropy(out, g.y.float())  \n",
    "      loss_sum += loss.detach().numpy()\n",
    "      total += 1\n",
    "      p = out.detach().numpy()[0]\n",
    "      if g.y == 1:\n",
    "        correct += p\n",
    "        if p >= 0.5:\n",
    "          rso50 += 1\n",
    "          min_margin = min(min_margin, p-0.5)\n",
    "      else:\n",
    "        correct += (1 - p)\n",
    "        if p < 0.5:\n",
    "          rso50 += 1\n",
    "          min_margin = min(min_margin, 0.5-p)\n",
    "  return {\n",
    "    'Loss': loss_sum/total,\n",
    "    'Acc:': correct/total,\n",
    "    'rso50': rso50/total,\n",
    "    'margin': min_margin\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy_Us2jSka9R"
   },
   "source": [
    "### Cycles experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7n2lrpJIEd68",
    "outputId": "59008dee-5eff-4a9e-fad3-58b7f8adc97b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , loss  0.71109724\n",
      "Epoch  1 , loss  0.7049422\n",
      "Epoch  1 , loss  0.7049422\n",
      "Epoch  2 , loss  0.7043533\n",
      "Epoch  2 , loss  0.7043533\n",
      "Epoch  3 , loss  0.69838315\n",
      "Epoch  3 , loss  0.69838315\n",
      "Epoch  4 , loss  0.6966242\n",
      "Epoch  4 , loss  0.6966242\n",
      "Epoch  5 , loss  0.7053985\n",
      "Epoch  5 , loss  0.7053985\n",
      "Epoch  6 , loss  0.69562155\n",
      "Epoch  6 , loss  0.69562155\n",
      "Epoch  7 , loss  0.70764333\n",
      "Epoch  7 , loss  0.70764333\n",
      "Epoch  8 , loss  0.70093346\n",
      "Epoch  8 , loss  0.70093346\n",
      "Epoch  9 , loss  0.7016242\n",
      "Epoch  9 , loss  0.7016242\n",
      "Epoch  10 , loss  0.6966669\n",
      "Epoch  10 , loss  0.6966669\n",
      "Epoch  11 , loss  0.69438225\n",
      "Epoch  11 , loss  0.69438225\n",
      "Epoch  12 , loss  0.7017784\n",
      "Epoch  12 , loss  0.7017784\n",
      "Epoch  13 , loss  0.7022205\n",
      "Epoch  13 , loss  0.7022205\n",
      "Epoch  14 , loss  0.6987811\n",
      "Epoch  14 , loss  0.6987811\n",
      "Epoch  15 , loss  0.702687\n",
      "Epoch  15 , loss  0.702687\n",
      "Epoch  16 , loss  0.6962421\n",
      "Epoch  16 , loss  0.6962421\n",
      "Epoch  17 , loss  0.69818157\n",
      "Epoch  17 , loss  0.69818157\n",
      "Epoch  18 , loss  0.69540644\n",
      "Epoch  18 , loss  0.69540644\n",
      "Epoch  19 , loss  0.6949628\n",
      "Epoch  19 , loss  0.6949628\n",
      "Epoch  20 , loss  0.7011183\n",
      "Epoch  20 , loss  0.7011183\n",
      "Epoch  21 , loss  0.6964018\n",
      "Epoch  21 , loss  0.6964018\n",
      "Epoch  22 , loss  0.6965615\n",
      "Epoch  22 , loss  0.6965615\n",
      "Epoch  23 , loss  0.6976714\n",
      "Epoch  23 , loss  0.6976714\n",
      "Epoch  24 , loss  0.69518834\n",
      "Epoch  24 , loss  0.69518834\n",
      "Epoch  25 , loss  0.6976223\n",
      "Epoch  25 , loss  0.6976223\n",
      "Epoch  26 , loss  0.6958544\n",
      "Epoch  26 , loss  0.6958544\n",
      "Epoch  27 , loss  0.6991501\n",
      "Epoch  27 , loss  0.6991501\n",
      "Epoch  28 , loss  0.6981776\n",
      "Epoch  28 , loss  0.6981776\n",
      "Epoch  29 , loss  0.70329064\n",
      "Epoch  29 , loss  0.70329064\n",
      "Epoch  30 , loss  0.6957007\n",
      "Epoch  30 , loss  0.6957007\n",
      "Epoch  31 , loss  0.69878596\n",
      "Epoch  31 , loss  0.69878596\n",
      "Epoch  32 , loss  0.6955194\n",
      "Epoch  32 , loss  0.6955194\n",
      "Epoch  33 , loss  0.69522214\n",
      "Epoch  33 , loss  0.69522214\n",
      "Epoch  34 , loss  0.7006259\n",
      "Epoch  34 , loss  0.7006259\n",
      "Epoch  35 , loss  0.69875497\n",
      "Epoch  35 , loss  0.69875497\n",
      "Epoch  36 , loss  0.6948895\n",
      "Epoch  36 , loss  0.6948895\n",
      "Epoch  37 , loss  0.69668573\n",
      "Epoch  37 , loss  0.69668573\n",
      "Epoch  38 , loss  0.6952815\n",
      "Epoch  38 , loss  0.6952815\n",
      "Epoch  39 , loss  0.69511515\n",
      "Epoch  39 , loss  0.69511515\n",
      "Epoch  40 , loss  0.69626063\n",
      "Epoch  40 , loss  0.69626063\n",
      "Epoch  41 , loss  0.69683105\n",
      "Epoch  41 , loss  0.69683105\n",
      "Epoch  42 , loss  0.69442075\n",
      "Epoch  42 , loss  0.69442075\n",
      "Epoch  43 , loss  0.69442874\n",
      "Epoch  43 , loss  0.69442874\n",
      "Epoch  44 , loss  0.6958399\n",
      "Epoch  44 , loss  0.6958399\n",
      "Epoch  45 , loss  0.69572514\n",
      "Epoch  45 , loss  0.69572514\n",
      "Epoch  46 , loss  0.69829273\n",
      "Epoch  46 , loss  0.69829273\n",
      "Epoch  47 , loss  0.69601583\n",
      "Epoch  47 , loss  0.69601583\n",
      "Epoch  48 , loss  0.6938923\n",
      "Epoch  48 , loss  0.6938923\n",
      "Epoch  49 , loss  0.6982927\n",
      "Epoch  49 , loss  0.6982927\n",
      "Epoch  50 , loss  0.69463176\n",
      "Epoch  50 , loss  0.69463176\n",
      "Epoch  51 , loss  0.69425815\n",
      "Epoch  51 , loss  0.69425815\n",
      "Epoch  52 , loss  0.6974931\n",
      "Epoch  52 , loss  0.6974931\n",
      "Epoch  53 , loss  0.69420534\n",
      "Epoch  53 , loss  0.69420534\n",
      "Epoch  54 , loss  0.69480014\n",
      "Epoch  54 , loss  0.69480014\n",
      "Epoch  55 , loss  0.6949863\n",
      "Epoch  55 , loss  0.6949863\n",
      "Epoch  56 , loss  0.69582385\n",
      "Epoch  56 , loss  0.69582385\n",
      "Epoch  57 , loss  0.6962126\n",
      "Epoch  57 , loss  0.6962126\n",
      "Epoch  58 , loss  0.69736713\n",
      "Epoch  58 , loss  0.69736713\n",
      "Epoch  59 , loss  0.69784397\n",
      "Epoch  59 , loss  0.69784397\n",
      "Epoch  60 , loss  0.69487\n",
      "Epoch  60 , loss  0.69487\n",
      "Epoch  61 , loss  0.69478077\n",
      "Epoch  61 , loss  0.69478077\n",
      "Epoch  62 , loss  0.6977506\n",
      "Epoch  62 , loss  0.6977506\n",
      "Epoch  63 , loss  0.69612837\n",
      "Epoch  63 , loss  0.69612837\n",
      "Epoch  64 , loss  0.6941076\n",
      "Epoch  64 , loss  0.6941076\n",
      "Epoch  65 , loss  0.6938333\n",
      "Epoch  65 , loss  0.6938333\n",
      "Epoch  66 , loss  0.6991047\n",
      "Epoch  66 , loss  0.6991047\n",
      "Epoch  67 , loss  0.6946536\n",
      "Epoch  67 , loss  0.6946536\n",
      "Epoch  68 , loss  0.69716054\n",
      "Epoch  68 , loss  0.69716054\n",
      "Epoch  69 , loss  0.69447416\n",
      "Epoch  69 , loss  0.69447416\n",
      "Epoch  70 , loss  0.69426805\n",
      "Epoch  70 , loss  0.69426805\n",
      "Epoch  71 , loss  0.6946836\n",
      "Epoch  71 , loss  0.6946836\n",
      "Epoch  72 , loss  0.69606876\n",
      "Epoch  72 , loss  0.69606876\n",
      "Epoch  73 , loss  0.69462204\n",
      "Epoch  73 , loss  0.69462204\n",
      "Epoch  74 , loss  0.6944437\n",
      "Epoch  74 , loss  0.6944437\n",
      "Epoch  75 , loss  0.6945872\n",
      "Epoch  75 , loss  0.6945872\n",
      "Epoch  76 , loss  0.69406414\n",
      "Epoch  76 , loss  0.69406414\n",
      "Epoch  77 , loss  0.69527465\n",
      "Epoch  77 , loss  0.69527465\n",
      "Epoch  78 , loss  0.69467473\n",
      "Epoch  78 , loss  0.69467473\n",
      "Epoch  79 , loss  0.69440055\n",
      "Epoch  79 , loss  0.69440055\n",
      "Epoch  80 , loss  0.69440603\n",
      "Epoch  80 , loss  0.69440603\n",
      "Epoch  81 , loss  0.69553566\n",
      "Epoch  81 , loss  0.69553566\n",
      "Epoch  82 , loss  0.6941014\n",
      "Epoch  82 , loss  0.6941014\n",
      "Epoch  83 , loss  0.69585913\n",
      "Epoch  83 , loss  0.69585913\n",
      "Epoch  84 , loss  0.69491386\n",
      "Epoch  84 , loss  0.69491386\n",
      "Epoch  85 , loss  0.6941693\n",
      "Epoch  85 , loss  0.6941693\n",
      "Epoch  86 , loss  0.69526863\n",
      "Epoch  86 , loss  0.69526863\n",
      "Epoch  87 , loss  0.6945271\n",
      "Epoch  87 , loss  0.6945271\n",
      "Epoch  88 , loss  0.69453937\n",
      "Epoch  88 , loss  0.69453937\n",
      "Epoch  89 , loss  0.6941616\n",
      "Epoch  89 , loss  0.6941616\n",
      "Epoch  90 , loss  0.69646597\n",
      "Epoch  90 , loss  0.69646597\n",
      "Epoch  91 , loss  0.6956007\n",
      "Epoch  91 , loss  0.6956007\n",
      "Epoch  92 , loss  0.69638276\n",
      "Epoch  92 , loss  0.69638276\n",
      "Epoch  93 , loss  0.6950472\n",
      "Epoch  93 , loss  0.6950472\n",
      "Epoch  94 , loss  0.6969381\n",
      "Epoch  94 , loss  0.6969381\n",
      "Epoch  95 , loss  0.6961513\n",
      "Epoch  95 , loss  0.6961513\n",
      "Epoch  96 , loss  0.694469\n",
      "Epoch  96 , loss  0.694469\n",
      "Epoch  97 , loss  0.6946457\n",
      "Epoch  97 , loss  0.6946457\n",
      "Epoch  98 , loss  0.6948349\n",
      "Epoch  98 , loss  0.6948349\n",
      "Epoch  99 , loss  0.6947829\n",
      "1 layers\n",
      "Train:  {'Loss': 0.6947829127311707, 'Acc:': 0.49918970465660095, 'rso50': 0.4583333333333333, 'margin': 4.357099533081055e-05}\n",
      "Eval:  {'Loss': 0.6931551098823547, 'Acc:': 0.5, 'rso50': 0.5, 'margin': 0.0019953250885009766}\n",
      "Epoch  99 , loss  0.6947829\n",
      "1 layers\n",
      "Train:  {'Loss': 0.6947829127311707, 'Acc:': 0.49918970465660095, 'rso50': 0.4583333333333333, 'margin': 4.357099533081055e-05}\n",
      "Eval:  {'Loss': 0.6931551098823547, 'Acc:': 0.5, 'rso50': 0.5, 'margin': 0.0019953250885009766}\n",
      "Epoch  0 , loss  0.73093534\n",
      "Epoch  0 , loss  0.73093534\n",
      "Epoch  1 , loss  0.69045466\n",
      "Epoch  1 , loss  0.69045466\n",
      "Epoch  2 , loss  0.69517916\n",
      "Epoch  2 , loss  0.69517916\n",
      "Epoch  3 , loss  0.6793147\n",
      "Epoch  3 , loss  0.6793147\n",
      "Epoch  4 , loss  0.675515\n",
      "Epoch  4 , loss  0.675515\n",
      "Epoch  5 , loss  0.67868596\n",
      "Epoch  5 , loss  0.67868596\n",
      "Epoch  6 , loss  0.66031677\n",
      "Epoch  6 , loss  0.66031677\n",
      "Epoch  7 , loss  0.6806488\n",
      "Epoch  7 , loss  0.6806488\n",
      "Epoch  8 , loss  0.64482754\n",
      "Epoch  8 , loss  0.64482754\n",
      "Epoch  9 , loss  0.65964884\n",
      "Epoch  9 , loss  0.65964884\n",
      "Epoch  10 , loss  0.6642557\n",
      "Epoch  10 , loss  0.6642557\n",
      "Epoch  11 , loss  0.6324603\n",
      "Epoch  11 , loss  0.6324603\n",
      "Epoch  12 , loss  0.6460796\n",
      "Epoch  12 , loss  0.6460796\n",
      "Epoch  13 , loss  0.63115233\n",
      "Epoch  13 , loss  0.63115233\n",
      "Epoch  14 , loss  0.6319646\n",
      "Epoch  14 , loss  0.6319646\n",
      "Epoch  15 , loss  0.6331242\n",
      "Epoch  15 , loss  0.6331242\n",
      "Epoch  16 , loss  0.62292975\n",
      "Epoch  16 , loss  0.62292975\n",
      "Epoch  17 , loss  0.6023329\n",
      "Epoch  17 , loss  0.6023329\n",
      "Epoch  18 , loss  0.63721234\n",
      "Epoch  18 , loss  0.63721234\n",
      "Epoch  19 , loss  0.6100089\n",
      "Epoch  19 , loss  0.6100089\n",
      "Epoch  20 , loss  0.60087043\n",
      "Epoch  20 , loss  0.60087043\n",
      "Epoch  21 , loss  0.60078615\n",
      "Epoch  21 , loss  0.60078615\n",
      "Epoch  22 , loss  0.60978967\n",
      "Epoch  22 , loss  0.60978967\n",
      "Epoch  23 , loss  0.5991984\n",
      "Epoch  23 , loss  0.5991984\n",
      "Epoch  24 , loss  0.58631384\n",
      "Epoch  24 , loss  0.58631384\n",
      "Epoch  25 , loss  0.60256076\n",
      "Epoch  25 , loss  0.60256076\n",
      "Epoch  26 , loss  0.5866279\n",
      "Epoch  26 , loss  0.5866279\n",
      "Epoch  27 , loss  0.58166766\n",
      "Epoch  27 , loss  0.58166766\n",
      "Epoch  28 , loss  0.5852265\n",
      "Epoch  28 , loss  0.5852265\n",
      "Epoch  29 , loss  0.57458705\n",
      "Epoch  29 , loss  0.57458705\n",
      "Epoch  30 , loss  0.5761574\n",
      "Epoch  30 , loss  0.5761574\n",
      "Epoch  31 , loss  0.56381565\n",
      "Epoch  31 , loss  0.56381565\n",
      "Epoch  32 , loss  0.56877166\n",
      "Epoch  32 , loss  0.56877166\n",
      "Epoch  33 , loss  0.58083224\n",
      "Epoch  33 , loss  0.58083224\n",
      "Epoch  34 , loss  0.56318694\n",
      "Epoch  34 , loss  0.56318694\n",
      "Epoch  35 , loss  0.55999976\n",
      "Epoch  35 , loss  0.55999976\n",
      "Epoch  36 , loss  0.55749077\n",
      "Epoch  36 , loss  0.55749077\n",
      "Epoch  37 , loss  0.56051546\n",
      "Epoch  37 , loss  0.56051546\n",
      "Epoch  38 , loss  0.5516011\n",
      "Epoch  38 , loss  0.5516011\n",
      "Epoch  39 , loss  0.5573515\n",
      "Epoch  39 , loss  0.5573515\n",
      "Epoch  40 , loss  0.5465786\n",
      "Epoch  40 , loss  0.5465786\n",
      "Epoch  41 , loss  0.54102415\n",
      "Epoch  41 , loss  0.54102415\n",
      "Epoch  42 , loss  0.569141\n",
      "Epoch  42 , loss  0.569141\n",
      "Epoch  43 , loss  0.5567648\n",
      "Epoch  43 , loss  0.5567648\n",
      "Epoch  44 , loss  0.55237335\n",
      "Epoch  44 , loss  0.55237335\n",
      "Epoch  45 , loss  0.55572474\n",
      "Epoch  45 , loss  0.55572474\n",
      "Epoch  46 , loss  0.55668163\n",
      "Epoch  46 , loss  0.55668163\n",
      "Epoch  47 , loss  0.5451727\n",
      "Epoch  47 , loss  0.5451727\n",
      "Epoch  48 , loss  0.53556913\n",
      "Epoch  48 , loss  0.53556913\n",
      "Epoch  49 , loss  0.54566526\n",
      "Epoch  49 , loss  0.54566526\n",
      "Epoch  50 , loss  0.5390336\n",
      "Epoch  50 , loss  0.5390336\n",
      "Epoch  51 , loss  0.54804945\n",
      "Epoch  51 , loss  0.54804945\n",
      "Epoch  52 , loss  0.52758163\n",
      "Epoch  52 , loss  0.52758163\n",
      "Epoch  53 , loss  0.55023366\n",
      "Epoch  53 , loss  0.55023366\n",
      "Epoch  54 , loss  0.53609544\n",
      "Epoch  54 , loss  0.53609544\n",
      "Epoch  55 , loss  0.53162175\n",
      "Epoch  55 , loss  0.53162175\n",
      "Epoch  56 , loss  0.5382795\n",
      "Epoch  56 , loss  0.5382795\n",
      "Epoch  57 , loss  0.5307342\n",
      "Epoch  57 , loss  0.5307342\n",
      "Epoch  58 , loss  0.5311929\n",
      "Epoch  58 , loss  0.5311929\n",
      "Epoch  59 , loss  0.53165513\n",
      "Epoch  59 , loss  0.53165513\n",
      "Epoch  60 , loss  0.531568\n",
      "Epoch  60 , loss  0.531568\n",
      "Epoch  61 , loss  0.5393122\n",
      "Epoch  61 , loss  0.5393122\n",
      "Epoch  62 , loss  0.5323046\n",
      "Epoch  62 , loss  0.5323046\n",
      "Epoch  63 , loss  0.5262284\n",
      "Epoch  63 , loss  0.5262284\n",
      "Epoch  64 , loss  0.54084486\n",
      "Epoch  64 , loss  0.54084486\n",
      "Epoch  65 , loss  0.5302394\n",
      "Epoch  65 , loss  0.5302394\n",
      "Epoch  66 , loss  0.5255449\n",
      "Epoch  66 , loss  0.5255449\n",
      "Epoch  67 , loss  0.5225697\n",
      "Epoch  67 , loss  0.5225697\n",
      "Epoch  68 , loss  0.5176768\n",
      "Epoch  68 , loss  0.5176768\n",
      "Epoch  69 , loss  0.5140187\n",
      "Epoch  69 , loss  0.5140187\n",
      "Epoch  70 , loss  0.51709074\n",
      "Epoch  70 , loss  0.51709074\n",
      "Epoch  71 , loss  0.5158885\n",
      "Epoch  71 , loss  0.5158885\n",
      "Epoch  72 , loss  0.533757\n",
      "Epoch  72 , loss  0.533757\n",
      "Epoch  73 , loss  0.506717\n",
      "Epoch  73 , loss  0.506717\n",
      "Epoch  74 , loss  0.51354575\n",
      "Epoch  74 , loss  0.51354575\n",
      "Epoch  75 , loss  0.5079231\n",
      "Epoch  75 , loss  0.5079231\n",
      "Epoch  76 , loss  0.5070299\n",
      "Epoch  76 , loss  0.5070299\n",
      "Epoch  77 , loss  0.51440114\n",
      "Epoch  77 , loss  0.51440114\n",
      "Epoch  78 , loss  0.52759206\n",
      "Epoch  78 , loss  0.52759206\n",
      "Epoch  79 , loss  0.51035625\n",
      "Epoch  79 , loss  0.51035625\n",
      "Epoch  80 , loss  0.5051532\n",
      "Epoch  80 , loss  0.5051532\n",
      "Epoch  81 , loss  0.50765496\n",
      "Epoch  81 , loss  0.50765496\n",
      "Epoch  82 , loss  0.50388193\n",
      "Epoch  82 , loss  0.50388193\n",
      "Epoch  83 , loss  0.5333397\n",
      "Epoch  83 , loss  0.5333397\n",
      "Epoch  84 , loss  0.5176503\n",
      "Epoch  84 , loss  0.5176503\n",
      "Epoch  85 , loss  0.50631636\n",
      "Epoch  85 , loss  0.50631636\n",
      "Epoch  86 , loss  0.51698977\n",
      "Epoch  86 , loss  0.51698977\n",
      "Epoch  87 , loss  0.49564168\n",
      "Epoch  87 , loss  0.49564168\n",
      "Epoch  88 , loss  0.49856043\n",
      "Epoch  88 , loss  0.49856043\n",
      "Epoch  89 , loss  0.50085425\n",
      "Epoch  89 , loss  0.50085425\n",
      "Epoch  90 , loss  0.49728563\n",
      "Epoch  90 , loss  0.49728563\n",
      "Epoch  91 , loss  0.4916116\n",
      "Epoch  91 , loss  0.4916116\n",
      "Epoch  92 , loss  0.49403822\n",
      "Epoch  92 , loss  0.49403822\n",
      "Epoch  93 , loss  0.4942054\n",
      "Epoch  93 , loss  0.4942054\n",
      "Epoch  94 , loss  0.49053583\n",
      "Epoch  94 , loss  0.49053583\n",
      "Epoch  95 , loss  0.49092236\n",
      "Epoch  95 , loss  0.49092236\n",
      "Epoch  96 , loss  0.4923034\n",
      "Epoch  96 , loss  0.4923034\n",
      "Epoch  97 , loss  0.48866156\n",
      "Epoch  97 , loss  0.48866156\n",
      "Epoch  98 , loss  0.4930508\n",
      "Epoch  98 , loss  0.4930508\n",
      "Epoch  99 , loss  0.48389181\n",
      "2 layers\n",
      "Train:  {'Loss': 0.4838918149471283, 'Acc:': 0.6248551607131958, 'rso50': 0.9583333333333334, 'margin': 0.04367268085479736}\n",
      "Eval:  {'Loss': 0.4269615113735199, 'Acc:': 0.6560896039009094, 'rso50': 1.0, 'margin': 0.10620135068893433}\n",
      "Epoch  99 , loss  0.48389181\n",
      "2 layers\n",
      "Train:  {'Loss': 0.4838918149471283, 'Acc:': 0.6248551607131958, 'rso50': 0.9583333333333334, 'margin': 0.04367268085479736}\n",
      "Eval:  {'Loss': 0.4269615113735199, 'Acc:': 0.6560896039009094, 'rso50': 1.0, 'margin': 0.10620135068893433}\n",
      "Epoch  0 , loss  0.7120816\n",
      "Epoch  0 , loss  0.7120816\n",
      "Epoch  1 , loss  0.69625753\n",
      "Epoch  1 , loss  0.69625753\n",
      "Epoch  2 , loss  0.6972797\n",
      "Epoch  2 , loss  0.6972797\n",
      "Epoch  3 , loss  0.68904495\n",
      "Epoch  3 , loss  0.68904495\n",
      "Epoch  4 , loss  0.68393356\n",
      "Epoch  4 , loss  0.68393356\n",
      "Epoch  5 , loss  0.6856\n",
      "Epoch  5 , loss  0.6856\n",
      "Epoch  6 , loss  0.68454856\n",
      "Epoch  6 , loss  0.68454856\n",
      "Epoch  7 , loss  0.68419725\n",
      "Epoch  7 , loss  0.68419725\n",
      "Epoch  8 , loss  0.6611859\n",
      "Epoch  8 , loss  0.6611859\n",
      "Epoch  9 , loss  0.6587802\n",
      "Epoch  9 , loss  0.6587802\n",
      "Epoch  10 , loss  0.65606767\n",
      "Epoch  10 , loss  0.65606767\n",
      "Epoch  11 , loss  0.6477526\n",
      "Epoch  11 , loss  0.6477526\n",
      "Epoch  12 , loss  0.6210064\n",
      "Epoch  12 , loss  0.6210064\n",
      "Epoch  13 , loss  0.6179026\n",
      "Epoch  13 , loss  0.6179026\n",
      "Epoch  14 , loss  0.5997066\n",
      "Epoch  14 , loss  0.5997066\n",
      "Epoch  15 , loss  0.61558676\n",
      "Epoch  15 , loss  0.61558676\n",
      "Epoch  16 , loss  0.5953799\n",
      "Epoch  16 , loss  0.5953799\n",
      "Epoch  17 , loss  0.5875047\n",
      "Epoch  17 , loss  0.5875047\n",
      "Epoch  18 , loss  0.58376074\n",
      "Epoch  18 , loss  0.58376074\n",
      "Epoch  19 , loss  0.6028678\n",
      "Epoch  19 , loss  0.6028678\n",
      "Epoch  20 , loss  0.5579648\n",
      "Epoch  20 , loss  0.5579648\n",
      "Epoch  21 , loss  0.5689754\n",
      "Epoch  21 , loss  0.5689754\n",
      "Epoch  22 , loss  0.5471129\n",
      "Epoch  22 , loss  0.5471129\n",
      "Epoch  23 , loss  0.5556535\n",
      "Epoch  23 , loss  0.5556535\n",
      "Epoch  24 , loss  0.5653841\n",
      "Epoch  24 , loss  0.5653841\n",
      "Epoch  25 , loss  0.55400157\n",
      "Epoch  25 , loss  0.55400157\n",
      "Epoch  26 , loss  0.5375134\n",
      "Epoch  26 , loss  0.5375134\n",
      "Epoch  27 , loss  0.5135991\n",
      "Epoch  27 , loss  0.5135991\n",
      "Epoch  28 , loss  0.5119308\n",
      "Epoch  28 , loss  0.5119308\n",
      "Epoch  29 , loss  0.50710523\n",
      "Epoch  29 , loss  0.50710523\n",
      "Epoch  30 , loss  0.505472\n",
      "Epoch  30 , loss  0.505472\n",
      "Epoch  31 , loss  0.5029258\n",
      "Epoch  31 , loss  0.5029258\n",
      "Epoch  32 , loss  0.5185231\n",
      "Epoch  32 , loss  0.5185231\n",
      "Epoch  33 , loss  0.49243876\n",
      "Epoch  33 , loss  0.49243876\n",
      "Epoch  34 , loss  0.4918528\n",
      "Epoch  34 , loss  0.4918528\n",
      "Epoch  35 , loss  0.47803235\n",
      "Epoch  35 , loss  0.47803235\n",
      "Epoch  36 , loss  0.4760661\n",
      "Epoch  36 , loss  0.4760661\n",
      "Epoch  37 , loss  0.47051057\n",
      "Epoch  37 , loss  0.47051057\n",
      "Epoch  38 , loss  0.49437562\n",
      "Epoch  38 , loss  0.49437562\n",
      "Epoch  39 , loss  0.4786223\n",
      "Epoch  39 , loss  0.4786223\n",
      "Epoch  40 , loss  0.46713796\n",
      "Epoch  40 , loss  0.46713796\n",
      "Epoch  41 , loss  0.45706892\n",
      "Epoch  41 , loss  0.45706892\n",
      "Epoch  42 , loss  0.4560028\n",
      "Epoch  42 , loss  0.4560028\n",
      "Epoch  43 , loss  0.47385225\n",
      "Epoch  43 , loss  0.47385225\n",
      "Epoch  44 , loss  0.45468926\n",
      "Epoch  44 , loss  0.45468926\n",
      "Epoch  45 , loss  0.44852194\n",
      "Epoch  45 , loss  0.44852194\n",
      "Epoch  46 , loss  0.45881304\n",
      "Epoch  46 , loss  0.45881304\n",
      "Epoch  47 , loss  0.44347045\n",
      "Epoch  47 , loss  0.44347045\n",
      "Epoch  48 , loss  0.47104692\n",
      "Epoch  48 , loss  0.47104692\n",
      "Epoch  49 , loss  0.43244362\n",
      "Epoch  49 , loss  0.43244362\n",
      "Epoch  50 , loss  0.4562696\n",
      "Epoch  50 , loss  0.4562696\n",
      "Epoch  51 , loss  0.44163227\n",
      "Epoch  51 , loss  0.44163227\n",
      "Epoch  52 , loss  0.44803283\n",
      "Epoch  52 , loss  0.44803283\n",
      "Epoch  53 , loss  0.44524357\n",
      "Epoch  53 , loss  0.44524357\n",
      "Epoch  54 , loss  0.43825844\n",
      "Epoch  54 , loss  0.43825844\n",
      "Epoch  55 , loss  0.43515813\n",
      "Epoch  55 , loss  0.43515813\n",
      "Epoch  56 , loss  0.43629548\n",
      "Epoch  56 , loss  0.43629548\n",
      "Epoch  57 , loss  0.43103722\n",
      "Epoch  57 , loss  0.43103722\n",
      "Epoch  58 , loss  0.43668148\n",
      "Epoch  58 , loss  0.43668148\n",
      "Epoch  59 , loss  0.46934173\n",
      "Epoch  59 , loss  0.46934173\n",
      "Epoch  60 , loss  0.43309423\n",
      "Epoch  60 , loss  0.43309423\n",
      "Epoch  61 , loss  0.43887863\n",
      "Epoch  61 , loss  0.43887863\n",
      "Epoch  62 , loss  0.42873248\n",
      "Epoch  62 , loss  0.42873248\n",
      "Epoch  63 , loss  0.44283986\n",
      "Epoch  63 , loss  0.44283986\n",
      "Epoch  64 , loss  0.43638253\n",
      "Epoch  64 , loss  0.43638253\n",
      "Epoch  65 , loss  0.43143252\n",
      "Epoch  65 , loss  0.43143252\n",
      "Epoch  66 , loss  0.4316956\n",
      "Epoch  66 , loss  0.4316956\n",
      "Epoch  67 , loss  0.42149842\n",
      "Epoch  67 , loss  0.42149842\n",
      "Epoch  68 , loss  0.42910692\n",
      "Epoch  68 , loss  0.42910692\n",
      "Epoch  69 , loss  0.42176697\n",
      "Epoch  69 , loss  0.42176697\n",
      "Epoch  70 , loss  0.4147335\n",
      "Epoch  70 , loss  0.4147335\n",
      "Epoch  71 , loss  0.41579965\n",
      "Epoch  71 , loss  0.41579965\n",
      "Epoch  72 , loss  0.42424402\n",
      "Epoch  72 , loss  0.42424402\n",
      "Epoch  73 , loss  0.42099914\n",
      "Epoch  73 , loss  0.42099914\n",
      "Epoch  74 , loss  0.41611567\n",
      "Epoch  74 , loss  0.41611567\n",
      "Epoch  75 , loss  0.42173532\n",
      "Epoch  75 , loss  0.42173532\n",
      "Epoch  76 , loss  0.41075182\n",
      "Epoch  76 , loss  0.41075182\n",
      "Epoch  77 , loss  0.4349016\n",
      "Epoch  77 , loss  0.4349016\n",
      "Epoch  78 , loss  0.42551768\n",
      "Epoch  78 , loss  0.42551768\n",
      "Epoch  79 , loss  0.41411582\n",
      "Epoch  79 , loss  0.41411582\n",
      "Epoch  80 , loss  0.4118931\n",
      "Epoch  80 , loss  0.4118931\n",
      "Epoch  81 , loss  0.40841767\n",
      "Epoch  81 , loss  0.40841767\n",
      "Epoch  82 , loss  0.41203973\n",
      "Epoch  82 , loss  0.41203973\n",
      "Epoch  83 , loss  0.40722832\n",
      "Epoch  83 , loss  0.40722832\n",
      "Epoch  84 , loss  0.40131497\n",
      "Epoch  84 , loss  0.40131497\n",
      "Epoch  85 , loss  0.4046282\n",
      "Epoch  85 , loss  0.4046282\n",
      "Epoch  86 , loss  0.4090682\n",
      "Epoch  86 , loss  0.4090682\n",
      "Epoch  87 , loss  0.40177342\n",
      "Epoch  87 , loss  0.40177342\n",
      "Epoch  88 , loss  0.40517887\n",
      "Epoch  88 , loss  0.40517887\n",
      "Epoch  89 , loss  0.40454996\n",
      "Epoch  89 , loss  0.40454996\n",
      "Epoch  90 , loss  0.4027255\n",
      "Epoch  90 , loss  0.4027255\n",
      "Epoch  91 , loss  0.39932165\n",
      "Epoch  91 , loss  0.39932165\n",
      "Epoch  92 , loss  0.40054926\n",
      "Epoch  92 , loss  0.40054926\n",
      "Epoch  93 , loss  0.407667\n",
      "Epoch  93 , loss  0.407667\n",
      "Epoch  94 , loss  0.39981678\n",
      "Epoch  94 , loss  0.39981678\n",
      "Epoch  95 , loss  0.40596357\n",
      "Epoch  95 , loss  0.40596357\n",
      "Epoch  96 , loss  0.3985196\n",
      "Epoch  96 , loss  0.3985196\n",
      "Epoch  97 , loss  0.39344963\n",
      "Epoch  97 , loss  0.39344963\n",
      "Epoch  98 , loss  0.39884964\n",
      "Epoch  98 , loss  0.39884964\n",
      "Epoch  99 , loss  0.39330497\n",
      "3 layers\n",
      "Train:  {'Loss': 0.3933049738407135, 'Acc:': 0.6803696751594543, 'rso50': 1.0, 'margin': 0.08375653624534607}\n",
      "Eval:  {'Loss': 0.3071838319301605, 'Acc:': 0.7419953346252441, 'rso50': 1.0, 'margin': 0.14642131328582764}\n",
      "Epoch  99 , loss  0.39330497\n",
      "3 layers\n",
      "Train:  {'Loss': 0.3933049738407135, 'Acc:': 0.6803696751594543, 'rso50': 1.0, 'margin': 0.08375653624534607}\n",
      "Eval:  {'Loss': 0.3071838319301605, 'Acc:': 0.7419953346252441, 'rso50': 1.0, 'margin': 0.14642131328582764}\n",
      "Epoch  0 , loss  0.7117112\n",
      "Epoch  0 , loss  0.7117112\n",
      "Epoch  1 , loss  0.7003997\n",
      "Epoch  1 , loss  0.7003997\n",
      "Epoch  2 , loss  0.6927336\n",
      "Epoch  2 , loss  0.6927336\n",
      "Epoch  3 , loss  0.6830488\n",
      "Epoch  3 , loss  0.6830488\n",
      "Epoch  4 , loss  0.6756003\n",
      "Epoch  4 , loss  0.6756003\n",
      "Epoch  5 , loss  0.69351506\n",
      "Epoch  5 , loss  0.69351506\n",
      "Epoch  6 , loss  0.6694925\n",
      "Epoch  6 , loss  0.6694925\n",
      "Epoch  7 , loss  0.6628877\n",
      "Epoch  7 , loss  0.6628877\n",
      "Epoch  8 , loss  0.6548144\n",
      "Epoch  8 , loss  0.6548144\n",
      "Epoch  9 , loss  0.65266734\n",
      "Epoch  9 , loss  0.65266734\n",
      "Epoch  10 , loss  0.62726706\n",
      "Epoch  10 , loss  0.62726706\n",
      "Epoch  11 , loss  0.6217749\n",
      "Epoch  11 , loss  0.6217749\n",
      "Epoch  12 , loss  0.6149681\n",
      "Epoch  12 , loss  0.6149681\n",
      "Epoch  13 , loss  0.6199795\n",
      "Epoch  13 , loss  0.6199795\n",
      "Epoch  14 , loss  0.60703033\n",
      "Epoch  14 , loss  0.60703033\n",
      "Epoch  15 , loss  0.5907158\n",
      "Epoch  15 , loss  0.5907158\n",
      "Epoch  16 , loss  0.59023976\n",
      "Epoch  16 , loss  0.59023976\n",
      "Epoch  17 , loss  0.5716098\n",
      "Epoch  17 , loss  0.5716098\n",
      "Epoch  18 , loss  0.5824372\n",
      "Epoch  18 , loss  0.5824372\n",
      "Epoch  19 , loss  0.59083337\n",
      "Epoch  19 , loss  0.59083337\n",
      "Epoch  20 , loss  0.5709503\n",
      "Epoch  20 , loss  0.5709503\n",
      "Epoch  21 , loss  0.61212\n",
      "Epoch  21 , loss  0.61212\n",
      "Epoch  22 , loss  0.55079085\n",
      "Epoch  22 , loss  0.55079085\n",
      "Epoch  23 , loss  0.5845265\n",
      "Epoch  23 , loss  0.5845265\n",
      "Epoch  24 , loss  0.5450741\n",
      "Epoch  24 , loss  0.5450741\n",
      "Epoch  25 , loss  0.5458652\n",
      "Epoch  25 , loss  0.5458652\n",
      "Epoch  26 , loss  0.5487195\n",
      "Epoch  26 , loss  0.5487195\n",
      "Epoch  27 , loss  0.52627414\n",
      "Epoch  27 , loss  0.52627414\n",
      "Epoch  28 , loss  0.52436715\n",
      "Epoch  28 , loss  0.52436715\n",
      "Epoch  29 , loss  0.59192175\n",
      "Epoch  29 , loss  0.59192175\n",
      "Epoch  30 , loss  0.55211335\n",
      "Epoch  30 , loss  0.55211335\n",
      "Epoch  31 , loss  0.5156023\n",
      "Epoch  31 , loss  0.5156023\n",
      "Epoch  32 , loss  0.4989145\n",
      "Epoch  32 , loss  0.4989145\n",
      "Epoch  33 , loss  0.57562333\n",
      "Epoch  33 , loss  0.57562333\n",
      "Epoch  34 , loss  0.48650372\n",
      "Epoch  34 , loss  0.48650372\n",
      "Epoch  35 , loss  0.492336\n",
      "Epoch  35 , loss  0.492336\n",
      "Epoch  36 , loss  0.48237267\n",
      "Epoch  36 , loss  0.48237267\n",
      "Epoch  37 , loss  0.47081462\n",
      "Epoch  37 , loss  0.47081462\n",
      "Epoch  38 , loss  0.46459854\n",
      "Epoch  38 , loss  0.46459854\n",
      "Epoch  39 , loss  0.4633477\n",
      "Epoch  39 , loss  0.4633477\n",
      "Epoch  40 , loss  0.4623345\n",
      "Epoch  40 , loss  0.4623345\n",
      "Epoch  41 , loss  0.45334765\n",
      "Epoch  41 , loss  0.45334765\n",
      "Epoch  42 , loss  0.4634317\n",
      "Epoch  42 , loss  0.4634317\n",
      "Epoch  43 , loss  0.46635595\n",
      "Epoch  43 , loss  0.46635595\n",
      "Epoch  44 , loss  0.46228766\n",
      "Epoch  44 , loss  0.46228766\n",
      "Epoch  45 , loss  0.43867266\n",
      "Epoch  45 , loss  0.43867266\n",
      "Epoch  46 , loss  0.44325218\n",
      "Epoch  46 , loss  0.44325218\n",
      "Epoch  47 , loss  0.43200645\n",
      "Epoch  47 , loss  0.43200645\n",
      "Epoch  48 , loss  0.43305418\n",
      "Epoch  48 , loss  0.43305418\n",
      "Epoch  49 , loss  0.44449902\n",
      "Epoch  49 , loss  0.44449902\n",
      "Epoch  50 , loss  0.45117226\n",
      "Epoch  50 , loss  0.45117226\n",
      "Epoch  51 , loss  0.43953338\n",
      "Epoch  51 , loss  0.43953338\n",
      "Epoch  52 , loss  0.45013627\n",
      "Epoch  52 , loss  0.45013627\n",
      "Epoch  53 , loss  0.45084608\n",
      "Epoch  53 , loss  0.45084608\n",
      "Epoch  54 , loss  0.4288676\n",
      "Epoch  54 , loss  0.4288676\n",
      "Epoch  55 , loss  0.42017362\n",
      "Epoch  55 , loss  0.42017362\n",
      "Epoch  56 , loss  0.42198753\n",
      "Epoch  56 , loss  0.42198753\n",
      "Epoch  57 , loss  0.421131\n",
      "Epoch  57 , loss  0.421131\n",
      "Epoch  58 , loss  0.43186572\n",
      "Epoch  58 , loss  0.43186572\n",
      "Epoch  59 , loss  0.44251215\n",
      "Epoch  59 , loss  0.44251215\n",
      "Epoch  60 , loss  0.430984\n",
      "Epoch  60 , loss  0.430984\n",
      "Epoch  61 , loss  0.44820833\n",
      "Epoch  61 , loss  0.44820833\n",
      "Epoch  62 , loss  0.4234089\n",
      "Epoch  62 , loss  0.4234089\n",
      "Epoch  63 , loss  0.45325294\n",
      "Epoch  63 , loss  0.45325294\n",
      "Epoch  64 , loss  0.42331257\n",
      "Epoch  64 , loss  0.42331257\n",
      "Epoch  65 , loss  0.41591623\n",
      "Epoch  65 , loss  0.41591623\n",
      "Epoch  66 , loss  0.41604006\n",
      "Epoch  66 , loss  0.41604006\n",
      "Epoch  67 , loss  0.40717554\n",
      "Epoch  67 , loss  0.40717554\n",
      "Epoch  68 , loss  0.40701172\n",
      "Epoch  68 , loss  0.40701172\n",
      "Epoch  69 , loss  0.40140483\n",
      "Epoch  69 , loss  0.40140483\n",
      "Epoch  70 , loss  0.4026636\n",
      "Epoch  70 , loss  0.4026636\n",
      "Epoch  71 , loss  0.40136206\n",
      "Epoch  71 , loss  0.40136206\n",
      "Epoch  72 , loss  0.39616045\n",
      "Epoch  72 , loss  0.39616045\n",
      "Epoch  73 , loss  0.3966936\n",
      "Epoch  73 , loss  0.3966936\n",
      "Epoch  74 , loss  0.40192962\n",
      "Epoch  74 , loss  0.40192962\n",
      "Epoch  75 , loss  0.41480398\n",
      "Epoch  75 , loss  0.41480398\n",
      "Epoch  76 , loss  0.410109\n",
      "Epoch  76 , loss  0.410109\n",
      "Epoch  77 , loss  0.39753774\n",
      "Epoch  77 , loss  0.39753774\n",
      "Epoch  78 , loss  0.3956549\n",
      "Epoch  78 , loss  0.3956549\n",
      "Epoch  79 , loss  0.39927137\n",
      "Epoch  79 , loss  0.39927137\n",
      "Epoch  80 , loss  0.4093081\n",
      "Epoch  80 , loss  0.4093081\n",
      "Epoch  81 , loss  0.4118221\n",
      "Epoch  81 , loss  0.4118221\n",
      "Epoch  82 , loss  0.40976644\n",
      "Epoch  82 , loss  0.40976644\n",
      "Epoch  83 , loss  0.40423974\n",
      "Epoch  83 , loss  0.40423974\n",
      "Epoch  84 , loss  0.41134742\n",
      "Epoch  84 , loss  0.41134742\n",
      "Epoch  85 , loss  0.3966944\n",
      "Epoch  85 , loss  0.3966944\n",
      "Epoch  86 , loss  0.38704708\n",
      "Epoch  86 , loss  0.38704708\n",
      "Epoch  87 , loss  0.3884118\n",
      "Epoch  87 , loss  0.3884118\n",
      "Epoch  88 , loss  0.3920622\n",
      "Epoch  88 , loss  0.3920622\n",
      "Epoch  89 , loss  0.3959771\n",
      "Epoch  89 , loss  0.3959771\n",
      "Epoch  90 , loss  0.39299953\n",
      "Epoch  90 , loss  0.39299953\n",
      "Epoch  91 , loss  0.38594663\n",
      "Epoch  91 , loss  0.38594663\n",
      "Epoch  92 , loss  0.3903766\n",
      "Epoch  92 , loss  0.3903766\n",
      "Epoch  93 , loss  0.38254914\n",
      "Epoch  93 , loss  0.38254914\n",
      "Epoch  94 , loss  0.38424826\n",
      "Epoch  94 , loss  0.38424826\n",
      "Epoch  95 , loss  0.38540348\n",
      "Epoch  95 , loss  0.38540348\n",
      "Epoch  96 , loss  0.38579413\n",
      "Epoch  96 , loss  0.38579413\n",
      "Epoch  97 , loss  0.38646695\n",
      "Epoch  97 , loss  0.38646695\n",
      "Epoch  98 , loss  0.38197374\n",
      "Epoch  98 , loss  0.38197374\n",
      "Epoch  99 , loss  0.3830947\n",
      "4 layers\n",
      "Train:  {'Loss': 0.3830946981906891, 'Acc:': 0.6861218810081482, 'rso50': 1.0, 'margin': 0.0665188729763031}\n",
      "Eval:  {'Loss': 0.29221853613853455, 'Acc:': 0.7517178654670715, 'rso50': 1.0, 'margin': 0.1815066933631897}\n",
      "Epoch  99 , loss  0.3830947\n",
      "4 layers\n",
      "Train:  {'Loss': 0.3830946981906891, 'Acc:': 0.6861218810081482, 'rso50': 1.0, 'margin': 0.0665188729763031}\n",
      "Eval:  {'Loss': 0.29221853613853455, 'Acc:': 0.7517178654670715, 'rso50': 1.0, 'margin': 0.1815066933631897}\n",
      "Epoch  0 , loss  0.7326966\n",
      "Epoch  0 , loss  0.7326966\n",
      "Epoch  1 , loss  0.703327\n",
      "Epoch  1 , loss  0.703327\n",
      "Epoch  2 , loss  0.7149907\n",
      "Epoch  2 , loss  0.7149907\n",
      "Epoch  3 , loss  0.70886666\n",
      "Epoch  3 , loss  0.70886666\n",
      "Epoch  4 , loss  0.6853173\n",
      "Epoch  4 , loss  0.6853173\n",
      "Epoch  5 , loss  0.68042517\n",
      "Epoch  5 , loss  0.68042517\n",
      "Epoch  6 , loss  0.66805434\n",
      "Epoch  6 , loss  0.66805434\n",
      "Epoch  7 , loss  0.65673095\n",
      "Epoch  7 , loss  0.65673095\n",
      "Epoch  8 , loss  0.6484571\n",
      "Epoch  8 , loss  0.6484571\n",
      "Epoch  9 , loss  0.6473652\n",
      "Epoch  9 , loss  0.6473652\n",
      "Epoch  10 , loss  0.6057219\n",
      "Epoch  10 , loss  0.6057219\n",
      "Epoch  11 , loss  0.62901455\n",
      "Epoch  11 , loss  0.62901455\n",
      "Epoch  12 , loss  0.6698062\n",
      "Epoch  12 , loss  0.6698062\n",
      "Epoch  13 , loss  0.5894164\n",
      "Epoch  13 , loss  0.5894164\n",
      "Epoch  14 , loss  0.5603434\n",
      "Epoch  14 , loss  0.5603434\n",
      "Epoch  15 , loss  0.52649957\n",
      "Epoch  15 , loss  0.52649957\n",
      "Epoch  16 , loss  0.5199808\n",
      "Epoch  16 , loss  0.5199808\n",
      "Epoch  17 , loss  0.5177329\n",
      "Epoch  17 , loss  0.5177329\n",
      "Epoch  18 , loss  0.5154267\n",
      "Epoch  18 , loss  0.5154267\n",
      "Epoch  19 , loss  0.5021684\n",
      "Epoch  19 , loss  0.5021684\n",
      "Epoch  20 , loss  0.47388685\n",
      "Epoch  20 , loss  0.47388685\n",
      "Epoch  21 , loss  0.4602745\n",
      "Epoch  21 , loss  0.4602745\n",
      "Epoch  22 , loss  0.44692144\n",
      "Epoch  22 , loss  0.44692144\n",
      "Epoch  23 , loss  0.45622513\n",
      "Epoch  23 , loss  0.45622513\n",
      "Epoch  24 , loss  0.4599447\n",
      "Epoch  24 , loss  0.4599447\n",
      "Epoch  25 , loss  0.4431517\n",
      "Epoch  25 , loss  0.4431517\n",
      "Epoch  26 , loss  0.44000018\n",
      "Epoch  26 , loss  0.44000018\n",
      "Epoch  27 , loss  0.43652785\n",
      "Epoch  27 , loss  0.43652785\n",
      "Epoch  28 , loss  0.4229995\n",
      "Epoch  28 , loss  0.4229995\n",
      "Epoch  29 , loss  0.41359004\n",
      "Epoch  29 , loss  0.41359004\n",
      "Epoch  30 , loss  0.41048017\n",
      "Epoch  30 , loss  0.41048017\n",
      "Epoch  31 , loss  0.4339143\n",
      "Epoch  31 , loss  0.4339143\n",
      "Epoch  32 , loss  0.41131935\n",
      "Epoch  32 , loss  0.41131935\n",
      "Epoch  33 , loss  0.4087989\n",
      "Epoch  33 , loss  0.4087989\n",
      "Epoch  34 , loss  0.38920704\n",
      "Epoch  34 , loss  0.38920704\n",
      "Epoch  35 , loss  0.42997774\n",
      "Epoch  35 , loss  0.42997774\n",
      "Epoch  36 , loss  0.42617393\n",
      "Epoch  36 , loss  0.42617393\n",
      "Epoch  37 , loss  0.41196725\n",
      "Epoch  37 , loss  0.41196725\n",
      "Epoch  38 , loss  0.3872665\n",
      "Epoch  38 , loss  0.3872665\n",
      "Epoch  39 , loss  0.40145954\n",
      "Epoch  39 , loss  0.40145954\n",
      "Epoch  40 , loss  0.39691675\n",
      "Epoch  40 , loss  0.39691675\n",
      "Epoch  41 , loss  0.38666323\n",
      "Epoch  41 , loss  0.38666323\n",
      "Epoch  42 , loss  0.37178242\n",
      "Epoch  42 , loss  0.37178242\n",
      "Epoch  43 , loss  0.37462613\n",
      "Epoch  43 , loss  0.37462613\n",
      "Epoch  44 , loss  0.38692412\n",
      "Epoch  44 , loss  0.38692412\n",
      "Epoch  45 , loss  0.37801075\n",
      "Epoch  45 , loss  0.37801075\n",
      "Epoch  46 , loss  0.3789774\n",
      "Epoch  46 , loss  0.3789774\n",
      "Epoch  47 , loss  0.41802725\n",
      "Epoch  47 , loss  0.41802725\n",
      "Epoch  48 , loss  0.39261904\n",
      "Epoch  48 , loss  0.39261904\n",
      "Epoch  49 , loss  0.40214133\n",
      "Epoch  49 , loss  0.40214133\n",
      "Epoch  50 , loss  0.37790963\n",
      "Epoch  50 , loss  0.37790963\n",
      "Epoch  51 , loss  0.35315335\n",
      "Epoch  51 , loss  0.35315335\n",
      "Epoch  52 , loss  0.3622448\n",
      "Epoch  52 , loss  0.3622448\n",
      "Epoch  53 , loss  0.35988215\n",
      "Epoch  53 , loss  0.35988215\n",
      "Epoch  54 , loss  0.38007402\n",
      "Epoch  54 , loss  0.38007402\n",
      "Epoch  55 , loss  0.3792446\n",
      "Epoch  55 , loss  0.3792446\n",
      "Epoch  56 , loss  0.36816144\n",
      "Epoch  56 , loss  0.36816144\n",
      "Epoch  57 , loss  0.35744855\n",
      "Epoch  57 , loss  0.35744855\n",
      "Epoch  58 , loss  0.35361966\n",
      "Epoch  58 , loss  0.35361966\n",
      "Epoch  59 , loss  0.35615352\n",
      "Epoch  59 , loss  0.35615352\n",
      "Epoch  60 , loss  0.35192558\n",
      "Epoch  60 , loss  0.35192558\n",
      "Epoch  61 , loss  0.34832278\n",
      "Epoch  61 , loss  0.34832278\n",
      "Epoch  62 , loss  0.35566822\n",
      "Epoch  62 , loss  0.35566822\n",
      "Epoch  63 , loss  0.35366467\n",
      "Epoch  63 , loss  0.35366467\n",
      "Epoch  64 , loss  0.3482332\n",
      "Epoch  64 , loss  0.3482332\n",
      "Epoch  65 , loss  0.3390251\n",
      "Epoch  65 , loss  0.3390251\n",
      "Epoch  66 , loss  0.3515543\n",
      "Epoch  66 , loss  0.3515543\n",
      "Epoch  67 , loss  0.34985915\n",
      "Epoch  67 , loss  0.34985915\n",
      "Epoch  68 , loss  0.35285363\n",
      "Epoch  68 , loss  0.35285363\n",
      "Epoch  69 , loss  0.34610286\n",
      "Epoch  69 , loss  0.34610286\n",
      "Epoch  70 , loss  0.3372127\n",
      "Epoch  70 , loss  0.3372127\n",
      "Epoch  71 , loss  0.34550008\n",
      "Epoch  71 , loss  0.34550008\n",
      "Epoch  72 , loss  0.3297267\n",
      "Epoch  72 , loss  0.3297267\n",
      "Epoch  73 , loss  0.33079207\n",
      "Epoch  73 , loss  0.33079207\n",
      "Epoch  74 , loss  0.33615446\n",
      "Epoch  74 , loss  0.33615446\n",
      "Epoch  75 , loss  0.32881358\n",
      "Epoch  75 , loss  0.32881358\n",
      "Epoch  76 , loss  0.346756\n",
      "Epoch  76 , loss  0.346756\n",
      "Epoch  77 , loss  0.33632204\n",
      "Epoch  77 , loss  0.33632204\n",
      "Epoch  78 , loss  0.33976737\n",
      "Epoch  78 , loss  0.33976737\n",
      "Epoch  79 , loss  0.35756695\n",
      "Epoch  79 , loss  0.35756695\n",
      "Epoch  80 , loss  0.34306514\n",
      "Epoch  80 , loss  0.34306514\n",
      "Epoch  81 , loss  0.3269515\n",
      "Epoch  81 , loss  0.3269515\n",
      "Epoch  82 , loss  0.33477163\n",
      "Epoch  82 , loss  0.33477163\n",
      "Epoch  83 , loss  0.3282996\n",
      "Epoch  83 , loss  0.3282996\n",
      "Epoch  84 , loss  0.32799482\n",
      "Epoch  84 , loss  0.32799482\n",
      "Epoch  85 , loss  0.32818532\n",
      "Epoch  85 , loss  0.32818532\n",
      "Epoch  86 , loss  0.32471\n",
      "Epoch  86 , loss  0.32471\n",
      "Epoch  87 , loss  0.32522854\n",
      "Epoch  87 , loss  0.32522854\n",
      "Epoch  88 , loss  0.32873264\n",
      "Epoch  88 , loss  0.32873264\n",
      "Epoch  89 , loss  0.32815754\n",
      "Epoch  89 , loss  0.32815754\n",
      "Epoch  90 , loss  0.31971303\n",
      "Epoch  90 , loss  0.31971303\n",
      "Epoch  91 , loss  0.32307425\n",
      "Epoch  91 , loss  0.32307425\n",
      "Epoch  92 , loss  0.3176025\n",
      "Epoch  92 , loss  0.3176025\n",
      "Epoch  93 , loss  0.31658363\n",
      "Epoch  93 , loss  0.31658363\n",
      "Epoch  94 , loss  0.3305684\n",
      "Epoch  94 , loss  0.3305684\n",
      "Epoch  95 , loss  0.33056182\n",
      "Epoch  95 , loss  0.33056182\n",
      "Epoch  96 , loss  0.3187775\n",
      "Epoch  96 , loss  0.3187775\n",
      "Epoch  97 , loss  0.31778085\n",
      "Epoch  97 , loss  0.31778085\n",
      "Epoch  98 , loss  0.31960586\n",
      "Epoch  98 , loss  0.31960586\n",
      "Epoch  99 , loss  0.31876314\n",
      "5 layers\n",
      "Train:  {'Loss': 0.3187631368637085, 'Acc:': 0.734704315662384, 'rso50': 1.0, 'margin': 0.11449933052062988}\n",
      "Eval:  {'Loss': 0.26751241087913513, 'Acc:': 0.7747572064399719, 'rso50': 1.0, 'margin': 0.16128677129745483}\n",
      "Epoch  99 , loss  0.31876314\n",
      "5 layers\n",
      "Train:  {'Loss': 0.3187631368637085, 'Acc:': 0.734704315662384, 'rso50': 1.0, 'margin': 0.11449933052062988}\n",
      "Eval:  {'Loss': 0.26751241087913513, 'Acc:': 0.7747572064399719, 'rso50': 1.0, 'margin': 0.16128677129745483}\n",
      "Epoch  0 , loss  0.7057145\n",
      "Epoch  0 , loss  0.7057145\n",
      "Epoch  1 , loss  0.6976333\n",
      "Epoch  1 , loss  0.6976333\n",
      "Epoch  2 , loss  0.6851751\n",
      "Epoch  2 , loss  0.6851751\n",
      "Epoch  3 , loss  0.6921162\n",
      "Epoch  3 , loss  0.6921162\n",
      "Epoch  4 , loss  0.6805323\n",
      "Epoch  4 , loss  0.6805323\n",
      "Epoch  5 , loss  0.68701774\n",
      "Epoch  5 , loss  0.68701774\n",
      "Epoch  6 , loss  0.6720586\n",
      "Epoch  6 , loss  0.6720586\n",
      "Epoch  7 , loss  0.6636234\n",
      "Epoch  7 , loss  0.6636234\n",
      "Epoch  8 , loss  0.64196277\n",
      "Epoch  8 , loss  0.64196277\n",
      "Epoch  9 , loss  0.6149546\n",
      "Epoch  9 , loss  0.6149546\n",
      "Epoch  10 , loss  0.6123012\n",
      "Epoch  10 , loss  0.6123012\n",
      "Epoch  11 , loss  0.5625055\n",
      "Epoch  11 , loss  0.5625055\n",
      "Epoch  12 , loss  0.55619127\n",
      "Epoch  12 , loss  0.55619127\n",
      "Epoch  13 , loss  0.55467576\n",
      "Epoch  13 , loss  0.55467576\n",
      "Epoch  14 , loss  0.54267913\n",
      "Epoch  14 , loss  0.54267913\n",
      "Epoch  15 , loss  0.52214\n",
      "Epoch  15 , loss  0.52214\n",
      "Epoch  16 , loss  0.51993096\n",
      "Epoch  16 , loss  0.51993096\n",
      "Epoch  17 , loss  0.4805063\n",
      "Epoch  17 , loss  0.4805063\n",
      "Epoch  18 , loss  0.480546\n",
      "Epoch  18 , loss  0.480546\n",
      "Epoch  19 , loss  0.48314705\n",
      "Epoch  19 , loss  0.48314705\n",
      "Epoch  20 , loss  0.4586556\n",
      "Epoch  20 , loss  0.4586556\n",
      "Epoch  21 , loss  0.45869067\n",
      "Epoch  21 , loss  0.45869067\n",
      "Epoch  22 , loss  0.4396344\n",
      "Epoch  22 , loss  0.4396344\n",
      "Epoch  23 , loss  0.44660166\n",
      "Epoch  23 , loss  0.44660166\n",
      "Epoch  24 , loss  0.43174788\n",
      "Epoch  24 , loss  0.43174788\n",
      "Epoch  25 , loss  0.40709794\n",
      "Epoch  25 , loss  0.40709794\n",
      "Epoch  26 , loss  0.39859486\n",
      "Epoch  26 , loss  0.39859486\n",
      "Epoch  27 , loss  0.398777\n",
      "Epoch  27 , loss  0.398777\n",
      "Epoch  28 , loss  0.40195963\n",
      "Epoch  28 , loss  0.40195963\n",
      "Epoch  29 , loss  0.44599906\n",
      "Epoch  29 , loss  0.44599906\n",
      "Epoch  30 , loss  0.3956935\n",
      "Epoch  30 , loss  0.3956935\n",
      "Epoch  31 , loss  0.38070905\n",
      "Epoch  31 , loss  0.38070905\n",
      "Epoch  32 , loss  0.37727952\n",
      "Epoch  32 , loss  0.37727952\n",
      "Epoch  33 , loss  0.45546845\n",
      "Epoch  33 , loss  0.45546845\n",
      "Epoch  34 , loss  0.3874401\n",
      "Epoch  34 , loss  0.3874401\n",
      "Epoch  35 , loss  0.3593236\n",
      "Epoch  35 , loss  0.3593236\n",
      "Epoch  36 , loss  0.36901036\n",
      "Epoch  36 , loss  0.36901036\n",
      "Epoch  37 , loss  0.32040587\n",
      "Epoch  37 , loss  0.32040587\n",
      "Epoch  38 , loss  0.3400071\n",
      "Epoch  38 , loss  0.3400071\n",
      "Epoch  39 , loss  0.34337786\n",
      "Epoch  39 , loss  0.34337786\n",
      "Epoch  40 , loss  0.33403525\n",
      "Epoch  40 , loss  0.33403525\n",
      "Epoch  41 , loss  0.36000648\n",
      "Epoch  41 , loss  0.36000648\n",
      "Epoch  42 , loss  0.32859698\n",
      "Epoch  42 , loss  0.32859698\n",
      "Epoch  43 , loss  0.33413437\n",
      "Epoch  43 , loss  0.33413437\n",
      "Epoch  44 , loss  0.31068686\n",
      "Epoch  44 , loss  0.31068686\n",
      "Epoch  45 , loss  0.31124404\n",
      "Epoch  45 , loss  0.31124404\n",
      "Epoch  46 , loss  0.30308464\n",
      "Epoch  46 , loss  0.30308464\n",
      "Epoch  47 , loss  0.3145528\n",
      "Epoch  47 , loss  0.3145528\n",
      "Epoch  48 , loss  0.3100692\n",
      "Epoch  48 , loss  0.3100692\n",
      "Epoch  49 , loss  0.30800524\n",
      "Epoch  49 , loss  0.30800524\n",
      "Epoch  50 , loss  0.29600325\n",
      "Epoch  50 , loss  0.29600325\n",
      "Epoch  51 , loss  0.31072602\n",
      "Epoch  51 , loss  0.31072602\n",
      "Epoch  52 , loss  0.29715583\n",
      "Epoch  52 , loss  0.29715583\n",
      "Epoch  53 , loss  0.29065242\n",
      "Epoch  53 , loss  0.29065242\n",
      "Epoch  54 , loss  0.29799047\n",
      "Epoch  54 , loss  0.29799047\n",
      "Epoch  55 , loss  0.29413262\n",
      "Epoch  55 , loss  0.29413262\n",
      "Epoch  56 , loss  0.2916609\n",
      "Epoch  56 , loss  0.2916609\n",
      "Epoch  57 , loss  0.29632393\n",
      "Epoch  57 , loss  0.29632393\n",
      "Epoch  58 , loss  0.29875886\n",
      "Epoch  58 , loss  0.29875886\n",
      "Epoch  59 , loss  0.31534413\n",
      "Epoch  59 , loss  0.31534413\n",
      "Epoch  60 , loss  0.29840928\n",
      "Epoch  60 , loss  0.29840928\n",
      "Epoch  61 , loss  0.27436697\n",
      "Epoch  61 , loss  0.27436697\n",
      "Epoch  62 , loss  0.2873803\n",
      "Epoch  62 , loss  0.2873803\n",
      "Epoch  63 , loss  0.29856792\n",
      "Epoch  63 , loss  0.29856792\n",
      "Epoch  64 , loss  0.28565955\n",
      "Epoch  64 , loss  0.28565955\n",
      "Epoch  65 , loss  0.28332064\n",
      "Epoch  65 , loss  0.28332064\n",
      "Epoch  66 , loss  0.29593143\n",
      "Epoch  66 , loss  0.29593143\n",
      "Epoch  67 , loss  0.28184116\n",
      "Epoch  67 , loss  0.28184116\n",
      "Epoch  68 , loss  0.2976207\n",
      "Epoch  68 , loss  0.2976207\n",
      "Epoch  69 , loss  0.2962129\n",
      "Epoch  69 , loss  0.2962129\n",
      "Epoch  70 , loss  0.27283844\n",
      "Epoch  70 , loss  0.27283844\n",
      "Epoch  71 , loss  0.2757955\n",
      "Epoch  71 , loss  0.2757955\n",
      "Epoch  72 , loss  0.27264795\n",
      "Epoch  72 , loss  0.27264795\n",
      "Epoch  73 , loss  0.27098194\n",
      "Epoch  73 , loss  0.27098194\n",
      "Epoch  74 , loss  0.29474667\n",
      "Epoch  74 , loss  0.29474667\n",
      "Epoch  75 , loss  0.2764029\n",
      "Epoch  75 , loss  0.2764029\n",
      "Epoch  76 , loss  0.2791875\n",
      "Epoch  76 , loss  0.2791875\n",
      "Epoch  77 , loss  0.2707171\n",
      "Epoch  77 , loss  0.2707171\n",
      "Epoch  78 , loss  0.2689815\n",
      "Epoch  78 , loss  0.2689815\n",
      "Epoch  79 , loss  0.28075662\n",
      "Epoch  79 , loss  0.28075662\n",
      "Epoch  80 , loss  0.2722766\n",
      "Epoch  80 , loss  0.2722766\n",
      "Epoch  81 , loss  0.27312195\n",
      "Epoch  81 , loss  0.27312195\n",
      "Epoch  82 , loss  0.26931694\n",
      "Epoch  82 , loss  0.26931694\n",
      "Epoch  83 , loss  0.27277264\n",
      "Epoch  83 , loss  0.27277264\n",
      "Epoch  84 , loss  0.26240006\n",
      "Epoch  84 , loss  0.26240006\n",
      "Epoch  85 , loss  0.28818008\n",
      "Epoch  85 , loss  0.28818008\n",
      "Epoch  86 , loss  0.26197007\n",
      "Epoch  86 , loss  0.26197007\n",
      "Epoch  87 , loss  0.26821622\n",
      "Epoch  87 , loss  0.26821622\n",
      "Epoch  88 , loss  0.2607023\n",
      "Epoch  88 , loss  0.2607023\n",
      "Epoch  89 , loss  0.26528558\n",
      "Epoch  89 , loss  0.26528558\n",
      "Epoch  90 , loss  0.27577195\n",
      "Epoch  90 , loss  0.27577195\n",
      "Epoch  91 , loss  0.27000555\n",
      "Epoch  91 , loss  0.27000555\n",
      "Epoch  92 , loss  0.26486304\n",
      "Epoch  92 , loss  0.26486304\n",
      "Epoch  93 , loss  0.26154044\n",
      "Epoch  93 , loss  0.26154044\n",
      "Epoch  94 , loss  0.27692866\n",
      "Epoch  94 , loss  0.27692866\n",
      "Epoch  95 , loss  0.25849393\n",
      "Epoch  95 , loss  0.25849393\n",
      "Epoch  96 , loss  0.25556615\n",
      "Epoch  96 , loss  0.25556615\n",
      "Epoch  97 , loss  0.25646558\n",
      "Epoch  97 , loss  0.25646558\n",
      "Epoch  98 , loss  0.25325924\n",
      "Epoch  98 , loss  0.25325924\n",
      "Epoch  99 , loss  0.2572137\n",
      "6 layers\n",
      "Train:  {'Loss': 0.2572137117385864, 'Acc:': 0.7799903750419617, 'rso50': 1.0, 'margin': 0.06625813245773315}\n",
      "Eval:  {'Loss': 0.23410171270370483, 'Acc:': 0.7993912100791931, 'rso50': 1.0, 'margin': 0.18871217966079712}\n",
      "Epoch  99 , loss  0.2572137\n",
      "6 layers\n",
      "Train:  {'Loss': 0.2572137117385864, 'Acc:': 0.7799903750419617, 'rso50': 1.0, 'margin': 0.06625813245773315}\n",
      "Eval:  {'Loss': 0.23410171270370483, 'Acc:': 0.7993912100791931, 'rso50': 1.0, 'margin': 0.18871217966079712}\n",
      "Epoch  0 , loss  0.6995115\n",
      "Epoch  0 , loss  0.6995115\n",
      "Epoch  1 , loss  0.6751294\n",
      "Epoch  1 , loss  0.6751294\n",
      "Epoch  2 , loss  0.64559835\n",
      "Epoch  2 , loss  0.64559835\n",
      "Epoch  3 , loss  0.6385587\n",
      "Epoch  3 , loss  0.6385587\n",
      "Epoch  4 , loss  0.60005563\n",
      "Epoch  4 , loss  0.60005563\n",
      "Epoch  5 , loss  0.5937337\n",
      "Epoch  5 , loss  0.5937337\n",
      "Epoch  6 , loss  0.5855717\n",
      "Epoch  6 , loss  0.5855717\n",
      "Epoch  7 , loss  0.57570076\n",
      "Epoch  7 , loss  0.57570076\n",
      "Epoch  8 , loss  0.52742213\n",
      "Epoch  8 , loss  0.52742213\n",
      "Epoch  9 , loss  0.5103169\n",
      "Epoch  9 , loss  0.5103169\n",
      "Epoch  10 , loss  0.49296406\n",
      "Epoch  10 , loss  0.49296406\n",
      "Epoch  11 , loss  0.49825647\n",
      "Epoch  11 , loss  0.49825647\n",
      "Epoch  12 , loss  0.47630036\n",
      "Epoch  12 , loss  0.47630036\n",
      "Epoch  13 , loss  0.4493358\n",
      "Epoch  13 , loss  0.4493358\n",
      "Epoch  14 , loss  0.48117208\n",
      "Epoch  14 , loss  0.48117208\n",
      "Epoch  15 , loss  0.47822002\n",
      "Epoch  15 , loss  0.47822002\n",
      "Epoch  16 , loss  0.45518097\n",
      "Epoch  16 , loss  0.45518097\n",
      "Epoch  17 , loss  0.4279842\n",
      "Epoch  17 , loss  0.4279842\n",
      "Epoch  18 , loss  0.39817372\n",
      "Epoch  18 , loss  0.39817372\n",
      "Epoch  19 , loss  0.41892812\n",
      "Epoch  19 , loss  0.41892812\n",
      "Epoch  20 , loss  0.39913145\n",
      "Epoch  20 , loss  0.39913145\n",
      "Epoch  21 , loss  0.7120123\n",
      "Epoch  21 , loss  0.7120123\n",
      "Epoch  22 , loss  0.738667\n",
      "Epoch  22 , loss  0.738667\n",
      "Epoch  23 , loss  0.5568996\n",
      "Epoch  23 , loss  0.5568996\n",
      "Epoch  24 , loss  0.52579206\n",
      "Epoch  24 , loss  0.52579206\n",
      "Epoch  25 , loss  0.4776746\n",
      "Epoch  25 , loss  0.4776746\n",
      "Epoch  26 , loss  0.43052268\n",
      "Epoch  26 , loss  0.43052268\n",
      "Epoch  27 , loss  0.46937022\n",
      "Epoch  27 , loss  0.46937022\n",
      "Epoch  28 , loss  0.41970825\n",
      "Epoch  28 , loss  0.41970825\n",
      "Epoch  29 , loss  0.4066222\n",
      "Epoch  29 , loss  0.4066222\n",
      "Epoch  30 , loss  0.3840821\n",
      "Epoch  30 , loss  0.3840821\n",
      "Epoch  31 , loss  0.39165795\n",
      "Epoch  31 , loss  0.39165795\n",
      "Epoch  32 , loss  0.38943863\n",
      "Epoch  32 , loss  0.38943863\n",
      "Epoch  33 , loss  0.37706348\n",
      "Epoch  33 , loss  0.37706348\n",
      "Epoch  34 , loss  0.3912293\n",
      "Epoch  34 , loss  0.3912293\n",
      "Epoch  35 , loss  0.3678113\n",
      "Epoch  35 , loss  0.3678113\n",
      "Epoch  36 , loss  0.37870145\n",
      "Epoch  36 , loss  0.37870145\n",
      "Epoch  37 , loss  0.36625472\n",
      "Epoch  37 , loss  0.36625472\n",
      "Epoch  38 , loss  0.3612789\n",
      "Epoch  38 , loss  0.3612789\n",
      "Epoch  39 , loss  0.37314567\n",
      "Epoch  39 , loss  0.37314567\n",
      "Epoch  40 , loss  0.34850588\n",
      "Epoch  40 , loss  0.34850588\n",
      "Epoch  41 , loss  0.3496716\n",
      "Epoch  41 , loss  0.3496716\n",
      "Epoch  42 , loss  0.34826407\n",
      "Epoch  42 , loss  0.34826407\n",
      "Epoch  43 , loss  0.3430445\n",
      "Epoch  43 , loss  0.3430445\n",
      "Epoch  44 , loss  0.32543752\n",
      "Epoch  44 , loss  0.32543752\n",
      "Epoch  45 , loss  0.34817418\n",
      "Epoch  45 , loss  0.34817418\n",
      "Epoch  46 , loss  0.347988\n",
      "Epoch  46 , loss  0.347988\n",
      "Epoch  47 , loss  0.31573224\n",
      "Epoch  47 , loss  0.31573224\n",
      "Epoch  48 , loss  0.325004\n",
      "Epoch  48 , loss  0.325004\n",
      "Epoch  49 , loss  0.34030497\n",
      "Epoch  49 , loss  0.34030497\n",
      "Epoch  50 , loss  0.3240098\n",
      "Epoch  50 , loss  0.3240098\n",
      "Epoch  51 , loss  0.33949897\n",
      "Epoch  51 , loss  0.33949897\n",
      "Epoch  52 , loss  0.32214668\n",
      "Epoch  52 , loss  0.32214668\n",
      "Epoch  53 , loss  0.31604278\n",
      "Epoch  53 , loss  0.31604278\n",
      "Epoch  54 , loss  0.3294158\n",
      "Epoch  54 , loss  0.3294158\n",
      "Epoch  55 , loss  0.30044168\n",
      "Epoch  55 , loss  0.30044168\n",
      "Epoch  56 , loss  0.31124863\n",
      "Epoch  56 , loss  0.31124863\n",
      "Epoch  57 , loss  0.33405733\n",
      "Epoch  57 , loss  0.33405733\n",
      "Epoch  58 , loss  0.3528583\n",
      "Epoch  58 , loss  0.3528583\n",
      "Epoch  59 , loss  0.31750953\n",
      "Epoch  59 , loss  0.31750953\n",
      "Epoch  60 , loss  0.31466022\n",
      "Epoch  60 , loss  0.31466022\n",
      "Epoch  61 , loss  0.3328118\n",
      "Epoch  61 , loss  0.3328118\n",
      "Epoch  62 , loss  0.30300727\n",
      "Epoch  62 , loss  0.30300727\n",
      "Epoch  63 , loss  0.29760796\n",
      "Epoch  63 , loss  0.29760796\n",
      "Epoch  64 , loss  0.30841178\n",
      "Epoch  64 , loss  0.30841178\n",
      "Epoch  65 , loss  0.3243854\n",
      "Epoch  65 , loss  0.3243854\n",
      "Epoch  66 , loss  0.321435\n",
      "Epoch  66 , loss  0.321435\n",
      "Epoch  67 , loss  0.3133702\n",
      "Epoch  67 , loss  0.3133702\n",
      "Epoch  68 , loss  0.31144494\n",
      "Epoch  68 , loss  0.31144494\n",
      "Epoch  69 , loss  0.3274748\n",
      "Epoch  69 , loss  0.3274748\n",
      "Epoch  70 , loss  0.3006986\n",
      "Epoch  70 , loss  0.3006986\n",
      "Epoch  71 , loss  0.30412364\n",
      "Epoch  71 , loss  0.30412364\n",
      "Epoch  72 , loss  0.29409748\n",
      "Epoch  72 , loss  0.29409748\n",
      "Epoch  73 , loss  0.29388347\n",
      "Epoch  73 , loss  0.29388347\n",
      "Epoch  74 , loss  0.30618417\n",
      "Epoch  74 , loss  0.30618417\n",
      "Epoch  75 , loss  0.31496862\n",
      "Epoch  75 , loss  0.31496862\n",
      "Epoch  76 , loss  0.30311182\n",
      "Epoch  76 , loss  0.30311182\n",
      "Epoch  77 , loss  0.30682233\n",
      "Epoch  77 , loss  0.30682233\n",
      "Epoch  78 , loss  0.29371855\n",
      "Epoch  78 , loss  0.29371855\n",
      "Epoch  79 , loss  0.286007\n",
      "Epoch  79 , loss  0.286007\n",
      "Epoch  80 , loss  0.28979442\n",
      "Epoch  80 , loss  0.28979442\n",
      "Epoch  81 , loss  0.30386716\n",
      "Epoch  81 , loss  0.30386716\n",
      "Epoch  82 , loss  0.30701318\n",
      "Epoch  82 , loss  0.30701318\n",
      "Epoch  83 , loss  0.28706536\n",
      "Epoch  83 , loss  0.28706536\n",
      "Epoch  84 , loss  0.29935083\n",
      "Epoch  84 , loss  0.29935083\n",
      "Epoch  85 , loss  0.28113267\n",
      "Epoch  85 , loss  0.28113267\n",
      "Epoch  86 , loss  0.279742\n",
      "Epoch  86 , loss  0.279742\n",
      "Epoch  87 , loss  0.28315756\n",
      "Epoch  87 , loss  0.28315756\n",
      "Epoch  88 , loss  0.3253844\n",
      "Epoch  88 , loss  0.3253844\n",
      "Epoch  89 , loss  0.305951\n",
      "Epoch  89 , loss  0.305951\n",
      "Epoch  90 , loss  0.27648443\n",
      "Epoch  90 , loss  0.27648443\n",
      "Epoch  91 , loss  0.2900996\n",
      "Epoch  91 , loss  0.2900996\n",
      "Epoch  92 , loss  0.2893039\n",
      "Epoch  92 , loss  0.2893039\n",
      "Epoch  93 , loss  0.28524432\n",
      "Epoch  93 , loss  0.28524432\n",
      "Epoch  94 , loss  0.27654248\n",
      "Epoch  94 , loss  0.27654248\n",
      "Epoch  95 , loss  0.29745844\n",
      "Epoch  95 , loss  0.29745844\n",
      "Epoch  96 , loss  0.30360612\n",
      "Epoch  96 , loss  0.30360612\n",
      "Epoch  97 , loss  0.2798554\n",
      "Epoch  97 , loss  0.2798554\n",
      "Epoch  98 , loss  0.27582207\n",
      "Epoch  98 , loss  0.27582207\n",
      "Epoch  99 , loss  0.27107856\n",
      "7 layers\n",
      "Train:  {'Loss': 0.27107855677604675, 'Acc:': 0.7710306644439697, 'rso50': 1.0, 'margin': 0.08037829399108887}\n",
      "Eval:  {'Loss': 0.15719549357891083, 'Acc:': 0.8586322665214539, 'rso50': 1.0, 'margin': 0.2748805284500122}\n",
      "Epoch  99 , loss  0.27107856\n",
      "7 layers\n",
      "Train:  {'Loss': 0.27107855677604675, 'Acc:': 0.7710306644439697, 'rso50': 1.0, 'margin': 0.08037829399108887}\n",
      "Eval:  {'Loss': 0.15719549357891083, 'Acc:': 0.8586322665214539, 'rso50': 1.0, 'margin': 0.2748805284500122}\n",
      "Epoch  0 , loss  0.7152157\n",
      "Epoch  0 , loss  0.7152157\n",
      "Epoch  1 , loss  0.6889388\n",
      "Epoch  1 , loss  0.6889388\n",
      "Epoch  2 , loss  0.6770387\n",
      "Epoch  2 , loss  0.6770387\n",
      "Epoch  3 , loss  0.6889767\n",
      "Epoch  3 , loss  0.6889767\n",
      "Epoch  4 , loss  0.66562253\n",
      "Epoch  4 , loss  0.66562253\n",
      "Epoch  5 , loss  0.6705958\n",
      "Epoch  5 , loss  0.6705958\n",
      "Epoch  6 , loss  0.65076137\n",
      "Epoch  6 , loss  0.65076137\n",
      "Epoch  7 , loss  0.62478954\n",
      "Epoch  7 , loss  0.62478954\n",
      "Epoch  8 , loss  0.5904295\n",
      "Epoch  8 , loss  0.5904295\n",
      "Epoch  9 , loss  0.60586715\n",
      "Epoch  9 , loss  0.60586715\n",
      "Epoch  10 , loss  0.6063836\n",
      "Epoch  10 , loss  0.6063836\n",
      "Epoch  11 , loss  0.56900823\n",
      "Epoch  11 , loss  0.56900823\n",
      "Epoch  12 , loss  0.51397735\n",
      "Epoch  12 , loss  0.51397735\n",
      "Epoch  13 , loss  0.50207466\n",
      "Epoch  13 , loss  0.50207466\n",
      "Epoch  14 , loss  0.4909816\n",
      "Epoch  14 , loss  0.4909816\n",
      "Epoch  15 , loss  0.48809662\n",
      "Epoch  15 , loss  0.48809662\n",
      "Epoch  16 , loss  0.4672873\n",
      "Epoch  16 , loss  0.4672873\n",
      "Epoch  17 , loss  0.46132874\n",
      "Epoch  17 , loss  0.46132874\n",
      "Epoch  18 , loss  0.5058262\n",
      "Epoch  18 , loss  0.5058262\n",
      "Epoch  19 , loss  0.43508327\n",
      "Epoch  19 , loss  0.43508327\n",
      "Epoch  20 , loss  0.37519684\n",
      "Epoch  20 , loss  0.37519684\n",
      "Epoch  21 , loss  0.4083301\n",
      "Epoch  21 , loss  0.4083301\n",
      "Epoch  22 , loss  0.38710263\n",
      "Epoch  22 , loss  0.38710263\n",
      "Epoch  23 , loss  0.40129998\n",
      "Epoch  23 , loss  0.40129998\n",
      "Epoch  24 , loss  0.38085636\n",
      "Epoch  24 , loss  0.38085636\n",
      "Epoch  25 , loss  0.36497268\n",
      "Epoch  25 , loss  0.36497268\n",
      "Epoch  26 , loss  0.33262753\n",
      "Epoch  26 , loss  0.33262753\n",
      "Epoch  27 , loss  0.32522628\n",
      "Epoch  27 , loss  0.32522628\n",
      "Epoch  28 , loss  0.32479355\n",
      "Epoch  28 , loss  0.32479355\n",
      "Epoch  29 , loss  0.33375895\n",
      "Epoch  29 , loss  0.33375895\n",
      "Epoch  30 , loss  0.3031489\n",
      "Epoch  30 , loss  0.3031489\n",
      "Epoch  31 , loss  0.35438943\n",
      "Epoch  31 , loss  0.35438943\n",
      "Epoch  32 , loss  0.33287886\n",
      "Epoch  32 , loss  0.33287886\n",
      "Epoch  33 , loss  0.32132182\n",
      "Epoch  33 , loss  0.32132182\n",
      "Epoch  34 , loss  0.34760478\n",
      "Epoch  34 , loss  0.34760478\n",
      "Epoch  35 , loss  0.32645255\n",
      "Epoch  35 , loss  0.32645255\n",
      "Epoch  36 , loss  0.28970134\n",
      "Epoch  36 , loss  0.28970134\n",
      "Epoch  37 , loss  0.3102356\n",
      "Epoch  37 , loss  0.3102356\n",
      "Epoch  38 , loss  0.29011226\n",
      "Epoch  38 , loss  0.29011226\n",
      "Epoch  39 , loss  0.3008754\n",
      "Epoch  39 , loss  0.3008754\n",
      "Epoch  40 , loss  0.31616423\n",
      "Epoch  40 , loss  0.31616423\n",
      "Epoch  41 , loss  0.31603318\n",
      "Epoch  41 , loss  0.31603318\n",
      "Epoch  42 , loss  0.28431842\n",
      "Epoch  42 , loss  0.28431842\n",
      "Epoch  43 , loss  0.30439284\n",
      "Epoch  43 , loss  0.30439284\n",
      "Epoch  44 , loss  0.3027803\n",
      "Epoch  44 , loss  0.3027803\n",
      "Epoch  45 , loss  0.28653595\n",
      "Epoch  45 , loss  0.28653595\n",
      "Epoch  46 , loss  0.26323435\n",
      "Epoch  46 , loss  0.26323435\n",
      "Epoch  47 , loss  0.27594736\n",
      "Epoch  47 , loss  0.27594736\n",
      "Epoch  48 , loss  0.26539293\n",
      "Epoch  48 , loss  0.26539293\n",
      "Epoch  49 , loss  0.2798551\n",
      "Epoch  49 , loss  0.2798551\n",
      "Epoch  50 , loss  0.26985782\n",
      "Epoch  50 , loss  0.26985782\n",
      "Epoch  51 , loss  0.27035066\n",
      "Epoch  51 , loss  0.27035066\n",
      "Epoch  52 , loss  0.33088762\n",
      "Epoch  52 , loss  0.33088762\n",
      "Epoch  53 , loss  0.3176643\n",
      "Epoch  53 , loss  0.3176643\n",
      "Epoch  54 , loss  0.24698955\n",
      "Epoch  54 , loss  0.24698955\n",
      "Epoch  55 , loss  0.25641155\n",
      "Epoch  55 , loss  0.25641155\n",
      "Epoch  56 , loss  0.25587246\n",
      "Epoch  56 , loss  0.25587246\n",
      "Epoch  57 , loss  0.2391447\n",
      "Epoch  57 , loss  0.2391447\n",
      "Epoch  58 , loss  0.24234562\n",
      "Epoch  58 , loss  0.24234562\n",
      "Epoch  59 , loss  0.23627853\n",
      "Epoch  59 , loss  0.23627853\n",
      "Epoch  60 , loss  0.23883486\n",
      "Epoch  60 , loss  0.23883486\n",
      "Epoch  61 , loss  0.24239479\n",
      "Epoch  61 , loss  0.24239479\n",
      "Epoch  62 , loss  0.25614357\n",
      "Epoch  62 , loss  0.25614357\n",
      "Epoch  63 , loss  0.25157535\n",
      "Epoch  63 , loss  0.25157535\n",
      "Epoch  64 , loss  0.22710694\n",
      "Epoch  64 , loss  0.22710694\n",
      "Epoch  65 , loss  0.22625853\n",
      "Epoch  65 , loss  0.22625853\n",
      "Epoch  66 , loss  0.22295876\n",
      "Epoch  66 , loss  0.22295876\n",
      "Epoch  67 , loss  0.24704577\n",
      "Epoch  67 , loss  0.24704577\n",
      "Epoch  68 , loss  0.23556672\n",
      "Epoch  68 , loss  0.23556672\n",
      "Epoch  69 , loss  0.2326407\n",
      "Epoch  69 , loss  0.2326407\n",
      "Epoch  70 , loss  0.25377062\n",
      "Epoch  70 , loss  0.25377062\n",
      "Epoch  71 , loss  0.23073237\n",
      "Epoch  71 , loss  0.23073237\n",
      "Epoch  72 , loss  0.239497\n",
      "Epoch  72 , loss  0.239497\n",
      "Epoch  73 , loss  0.2136587\n",
      "Epoch  73 , loss  0.2136587\n",
      "Epoch  74 , loss  0.21229607\n",
      "Epoch  74 , loss  0.21229607\n",
      "Epoch  75 , loss  0.21183722\n",
      "Epoch  75 , loss  0.21183722\n",
      "Epoch  76 , loss  0.21238743\n",
      "Epoch  76 , loss  0.21238743\n",
      "Epoch  77 , loss  0.21352066\n",
      "Epoch  77 , loss  0.21352066\n",
      "Epoch  78 , loss  0.21657223\n",
      "Epoch  78 , loss  0.21657223\n",
      "Epoch  79 , loss  0.21248852\n",
      "Epoch  79 , loss  0.21248852\n",
      "Epoch  80 , loss  0.220506\n",
      "Epoch  80 , loss  0.220506\n",
      "Epoch  81 , loss  0.20719552\n",
      "Epoch  81 , loss  0.20719552\n",
      "Epoch  82 , loss  0.20188184\n",
      "Epoch  82 , loss  0.20188184\n",
      "Epoch  83 , loss  0.21284\n",
      "Epoch  83 , loss  0.21284\n",
      "Epoch  84 , loss  0.20868279\n",
      "Epoch  84 , loss  0.20868279\n",
      "Epoch  85 , loss  0.20570906\n",
      "Epoch  85 , loss  0.20570906\n",
      "Epoch  86 , loss  0.20399754\n",
      "Epoch  86 , loss  0.20399754\n",
      "Epoch  87 , loss  0.20038436\n",
      "Epoch  87 , loss  0.20038436\n",
      "Epoch  88 , loss  0.20085047\n",
      "Epoch  88 , loss  0.20085047\n",
      "Epoch  89 , loss  0.20489572\n",
      "Epoch  89 , loss  0.20489572\n",
      "Epoch  90 , loss  0.21444078\n",
      "Epoch  90 , loss  0.21444078\n",
      "Epoch  91 , loss  0.21863715\n",
      "Epoch  91 , loss  0.21863715\n",
      "Epoch  92 , loss  0.20927761\n",
      "Epoch  92 , loss  0.20927761\n",
      "Epoch  93 , loss  0.23063074\n",
      "Epoch  93 , loss  0.23063074\n",
      "Epoch  94 , loss  0.23071808\n",
      "Epoch  94 , loss  0.23071808\n",
      "Epoch  95 , loss  0.22530128\n",
      "Epoch  95 , loss  0.22530128\n",
      "Epoch  96 , loss  0.20472789\n",
      "Epoch  96 , loss  0.20472789\n",
      "Epoch  97 , loss  0.2023639\n",
      "Epoch  97 , loss  0.2023639\n",
      "Epoch  98 , loss  0.19520487\n",
      "Epoch  98 , loss  0.19520487\n",
      "Epoch  99 , loss  0.20433374\n",
      "8 layers\n",
      "Train:  {'Loss': 0.20433373749256134, 'Acc:': 0.8263375759124756, 'rso50': 1.0, 'margin': 0.006486237049102783}\n",
      "Eval:  {'Loss': 0.14082762598991394, 'Acc:': 0.8730114102363586, 'rso50': 1.0, 'margin': 0.2864871025085449}\n",
      "Epoch  99 , loss  0.20433374\n",
      "8 layers\n",
      "Train:  {'Loss': 0.20433373749256134, 'Acc:': 0.8263375759124756, 'rso50': 1.0, 'margin': 0.006486237049102783}\n",
      "Eval:  {'Loss': 0.14082762598991394, 'Acc:': 0.8730114102363586, 'rso50': 1.0, 'margin': 0.2864871025085449}\n",
      "Epoch  0 , loss  0.72605467\n",
      "Epoch  0 , loss  0.72605467\n",
      "Epoch  1 , loss  0.69003916\n",
      "Epoch  1 , loss  0.69003916\n",
      "Epoch  2 , loss  0.6878724\n",
      "Epoch  2 , loss  0.6878724\n",
      "Epoch  3 , loss  0.66616625\n",
      "Epoch  3 , loss  0.66616625\n",
      "Epoch  4 , loss  0.6455758\n",
      "Epoch  4 , loss  0.6455758\n",
      "Epoch  5 , loss  0.68255687\n",
      "Epoch  5 , loss  0.68255687\n",
      "Epoch  6 , loss  0.66319084\n",
      "Epoch  6 , loss  0.66319084\n",
      "Epoch  7 , loss  0.6376391\n",
      "Epoch  7 , loss  0.6376391\n",
      "Epoch  8 , loss  0.6459891\n",
      "Epoch  8 , loss  0.6459891\n",
      "Epoch  9 , loss  0.60520005\n",
      "Epoch  9 , loss  0.60520005\n",
      "Epoch  10 , loss  0.5843685\n",
      "Epoch  10 , loss  0.5843685\n",
      "Epoch  11 , loss  0.5624944\n",
      "Epoch  11 , loss  0.5624944\n",
      "Epoch  12 , loss  0.52223045\n",
      "Epoch  12 , loss  0.52223045\n",
      "Epoch  13 , loss  0.54085153\n",
      "Epoch  13 , loss  0.54085153\n",
      "Epoch  14 , loss  0.49315274\n",
      "Epoch  14 , loss  0.49315274\n",
      "Epoch  15 , loss  0.49484298\n",
      "Epoch  15 , loss  0.49484298\n",
      "Epoch  16 , loss  0.553003\n",
      "Epoch  16 , loss  0.553003\n",
      "Epoch  17 , loss  0.47934768\n",
      "Epoch  17 , loss  0.47934768\n",
      "Epoch  18 , loss  0.53147346\n",
      "Epoch  18 , loss  0.53147346\n",
      "Epoch  19 , loss  0.5101999\n",
      "Epoch  19 , loss  0.5101999\n",
      "Epoch  20 , loss  0.39616516\n",
      "Epoch  20 , loss  0.39616516\n",
      "Epoch  21 , loss  0.40420136\n",
      "Epoch  21 , loss  0.40420136\n",
      "Epoch  22 , loss  0.393558\n",
      "Epoch  22 , loss  0.393558\n",
      "Epoch  23 , loss  0.40292883\n",
      "Epoch  23 , loss  0.40292883\n",
      "Epoch  24 , loss  0.41872063\n",
      "Epoch  24 , loss  0.41872063\n",
      "Epoch  25 , loss  0.40565857\n",
      "Epoch  25 , loss  0.40565857\n",
      "Epoch  26 , loss  0.34492993\n",
      "Epoch  26 , loss  0.34492993\n",
      "Epoch  27 , loss  0.3476697\n",
      "Epoch  27 , loss  0.3476697\n",
      "Epoch  28 , loss  0.31436685\n",
      "Epoch  28 , loss  0.31436685\n",
      "Epoch  29 , loss  0.3230162\n",
      "Epoch  29 , loss  0.3230162\n",
      "Epoch  30 , loss  0.32858488\n",
      "Epoch  30 , loss  0.32858488\n",
      "Epoch  31 , loss  0.36874187\n",
      "Epoch  31 , loss  0.36874187\n",
      "Epoch  32 , loss  0.5557302\n",
      "Epoch  32 , loss  0.5557302\n",
      "Epoch  33 , loss  0.7149326\n",
      "Epoch  33 , loss  0.7149326\n",
      "Epoch  34 , loss  0.64163077\n",
      "Epoch  34 , loss  0.64163077\n",
      "Epoch  35 , loss  0.5647908\n",
      "Epoch  35 , loss  0.5647908\n",
      "Epoch  36 , loss  0.5455453\n",
      "Epoch  36 , loss  0.5455453\n",
      "Epoch  37 , loss  0.45873225\n",
      "Epoch  37 , loss  0.45873225\n",
      "Epoch  38 , loss  0.43696785\n",
      "Epoch  38 , loss  0.43696785\n",
      "Epoch  39 , loss  0.398141\n",
      "Epoch  39 , loss  0.398141\n",
      "Epoch  40 , loss  0.42322138\n",
      "Epoch  40 , loss  0.42322138\n",
      "Epoch  41 , loss  0.38692138\n",
      "Epoch  41 , loss  0.38692138\n",
      "Epoch  42 , loss  0.35837427\n",
      "Epoch  42 , loss  0.35837427\n",
      "Epoch  43 , loss  0.3542366\n",
      "Epoch  43 , loss  0.3542366\n",
      "Epoch  44 , loss  0.32477835\n",
      "Epoch  44 , loss  0.32477835\n",
      "Epoch  45 , loss  0.31299314\n",
      "Epoch  45 , loss  0.31299314\n",
      "Epoch  46 , loss  0.3100072\n",
      "Epoch  46 , loss  0.3100072\n",
      "Epoch  47 , loss  0.2840803\n",
      "Epoch  47 , loss  0.2840803\n",
      "Epoch  48 , loss  0.2943216\n",
      "Epoch  48 , loss  0.2943216\n",
      "Epoch  49 , loss  0.26185885\n",
      "Epoch  49 , loss  0.26185885\n",
      "Epoch  50 , loss  0.27892387\n",
      "Epoch  50 , loss  0.27892387\n",
      "Epoch  51 , loss  0.24688071\n",
      "Epoch  51 , loss  0.24688071\n",
      "Epoch  52 , loss  0.2529598\n",
      "Epoch  52 , loss  0.2529598\n",
      "Epoch  53 , loss  0.2706496\n",
      "Epoch  53 , loss  0.2706496\n",
      "Epoch  54 , loss  0.256506\n",
      "Epoch  54 , loss  0.256506\n",
      "Epoch  55 , loss  0.26893452\n",
      "Epoch  55 , loss  0.26893452\n",
      "Epoch  56 , loss  0.25498345\n",
      "Epoch  56 , loss  0.25498345\n",
      "Epoch  57 , loss  0.2477438\n",
      "Epoch  57 , loss  0.2477438\n",
      "Epoch  58 , loss  0.23807037\n",
      "Epoch  58 , loss  0.23807037\n",
      "Epoch  59 , loss  0.24417299\n",
      "Epoch  59 , loss  0.24417299\n",
      "Epoch  60 , loss  0.22004384\n",
      "Epoch  60 , loss  0.22004384\n",
      "Epoch  61 , loss  0.23387754\n",
      "Epoch  61 , loss  0.23387754\n",
      "Epoch  62 , loss  0.23303777\n",
      "Epoch  62 , loss  0.23303777\n",
      "Epoch  63 , loss  0.24960761\n",
      "Epoch  63 , loss  0.24960761\n",
      "Epoch  64 , loss  0.24276249\n",
      "Epoch  64 , loss  0.24276249\n",
      "Epoch  65 , loss  0.22883876\n",
      "Epoch  65 , loss  0.22883876\n",
      "Epoch  66 , loss  0.2250547\n",
      "Epoch  66 , loss  0.2250547\n",
      "Epoch  67 , loss  0.2280171\n",
      "Epoch  67 , loss  0.2280171\n",
      "Epoch  68 , loss  0.23141657\n",
      "Epoch  68 , loss  0.23141657\n",
      "Epoch  69 , loss  0.24120583\n",
      "Epoch  69 , loss  0.24120583\n",
      "Epoch  70 , loss  0.20775431\n",
      "Epoch  70 , loss  0.20775431\n",
      "Epoch  71 , loss  0.20521678\n",
      "Epoch  71 , loss  0.20521678\n",
      "Epoch  72 , loss  0.22006594\n",
      "Epoch  72 , loss  0.22006594\n",
      "Epoch  73 , loss  0.22981937\n",
      "Epoch  73 , loss  0.22981937\n",
      "Epoch  74 , loss  0.22607161\n",
      "Epoch  74 , loss  0.22607161\n",
      "Epoch  75 , loss  0.21791948\n",
      "Epoch  75 , loss  0.21791948\n",
      "Epoch  76 , loss  0.23887403\n",
      "Epoch  76 , loss  0.23887403\n",
      "Epoch  77 , loss  0.22756755\n",
      "Epoch  77 , loss  0.22756755\n",
      "Epoch  78 , loss  0.2647665\n",
      "Epoch  78 , loss  0.2647665\n",
      "Epoch  79 , loss  0.24616806\n",
      "Epoch  79 , loss  0.24616806\n",
      "Epoch  80 , loss  0.23778367\n",
      "Epoch  80 , loss  0.23778367\n",
      "Epoch  81 , loss  0.22186507\n",
      "Epoch  81 , loss  0.22186507\n",
      "Epoch  82 , loss  0.21497856\n",
      "Epoch  82 , loss  0.21497856\n",
      "Epoch  83 , loss  0.19603105\n",
      "Epoch  83 , loss  0.19603105\n",
      "Epoch  84 , loss  0.18643107\n",
      "Epoch  84 , loss  0.18643107\n",
      "Epoch  85 , loss  0.19169492\n",
      "Epoch  85 , loss  0.19169492\n",
      "Epoch  86 , loss  0.18776684\n",
      "Epoch  86 , loss  0.18776684\n",
      "Epoch  87 , loss  0.19232297\n",
      "Epoch  87 , loss  0.19232297\n",
      "Epoch  88 , loss  0.18303926\n",
      "Epoch  88 , loss  0.18303926\n",
      "Epoch  89 , loss  0.19904459\n",
      "Epoch  89 , loss  0.19904459\n",
      "Epoch  90 , loss  0.20318355\n",
      "Epoch  90 , loss  0.20318355\n",
      "Epoch  91 , loss  0.19429593\n",
      "Epoch  91 , loss  0.19429593\n",
      "Epoch  92 , loss  0.18044706\n",
      "Epoch  92 , loss  0.18044706\n",
      "Epoch  93 , loss  0.19827145\n",
      "Epoch  93 , loss  0.19827145\n",
      "Epoch  94 , loss  0.1891694\n",
      "Epoch  94 , loss  0.1891694\n",
      "Epoch  95 , loss  0.18526252\n",
      "Epoch  95 , loss  0.18526252\n",
      "Epoch  96 , loss  0.17970139\n",
      "Epoch  96 , loss  0.17970139\n",
      "Epoch  97 , loss  0.17960995\n",
      "Epoch  97 , loss  0.17960995\n",
      "Epoch  98 , loss  0.18462561\n",
      "Epoch  98 , loss  0.18462561\n",
      "Epoch  99 , loss  0.1929071\n",
      "9 layers\n",
      "Train:  {'Loss': 0.19290709495544434, 'Acc:': 0.8328328728675842, 'rso50': 1.0, 'margin': 0.09715288877487183}\n",
      "Eval:  {'Loss': 0.13172614574432373, 'Acc:': 0.8798802495002747, 'rso50': 1.0, 'margin': 0.3051453232765198}\n",
      "Epoch  99 , loss  0.1929071\n",
      "9 layers\n",
      "Train:  {'Loss': 0.19290709495544434, 'Acc:': 0.8328328728675842, 'rso50': 1.0, 'margin': 0.09715288877487183}\n",
      "Eval:  {'Loss': 0.13172614574432373, 'Acc:': 0.8798802495002747, 'rso50': 1.0, 'margin': 0.3051453232765198}\n",
      "Epoch  0 , loss  0.70984864\n",
      "Epoch  0 , loss  0.70984864\n",
      "Epoch  1 , loss  0.70442367\n",
      "Epoch  1 , loss  0.70442367\n",
      "Epoch  2 , loss  0.69724196\n",
      "Epoch  2 , loss  0.69724196\n",
      "Epoch  3 , loss  0.698901\n",
      "Epoch  3 , loss  0.698901\n",
      "Epoch  4 , loss  0.70390457\n",
      "Epoch  4 , loss  0.70390457\n",
      "Epoch  5 , loss  0.6919785\n",
      "Epoch  5 , loss  0.6919785\n",
      "Epoch  6 , loss  0.69762546\n",
      "Epoch  6 , loss  0.69762546\n",
      "Epoch  7 , loss  0.69966936\n",
      "Epoch  7 , loss  0.69966936\n",
      "Epoch  8 , loss  0.6904804\n",
      "Epoch  8 , loss  0.6904804\n",
      "Epoch  9 , loss  0.6881757\n",
      "Epoch  9 , loss  0.6881757\n",
      "Epoch  10 , loss  0.68454313\n",
      "Epoch  10 , loss  0.68454313\n",
      "Epoch  11 , loss  0.68608737\n",
      "Epoch  11 , loss  0.68608737\n",
      "Epoch  12 , loss  0.67113394\n",
      "Epoch  12 , loss  0.67113394\n",
      "Epoch  13 , loss  0.67075306\n",
      "Epoch  13 , loss  0.67075306\n",
      "Epoch  14 , loss  0.6785006\n",
      "Epoch  14 , loss  0.6785006\n",
      "Epoch  15 , loss  0.6770173\n",
      "Epoch  15 , loss  0.6770173\n",
      "Epoch  16 , loss  0.6934703\n",
      "Epoch  16 , loss  0.6934703\n",
      "Epoch  17 , loss  0.66359174\n",
      "Epoch  17 , loss  0.66359174\n",
      "Epoch  18 , loss  0.6793787\n",
      "Epoch  18 , loss  0.6793787\n",
      "Epoch  19 , loss  0.63968045\n",
      "Epoch  19 , loss  0.63968045\n",
      "Epoch  20 , loss  0.60696274\n",
      "Epoch  20 , loss  0.60696274\n",
      "Epoch  21 , loss  0.5982508\n",
      "Epoch  21 , loss  0.5982508\n",
      "Epoch  22 , loss  0.6650614\n",
      "Epoch  22 , loss  0.6650614\n",
      "Epoch  23 , loss  0.5992258\n",
      "Epoch  23 , loss  0.5992258\n",
      "Epoch  24 , loss  0.5430827\n",
      "Epoch  24 , loss  0.5430827\n",
      "Epoch  25 , loss  0.5941973\n",
      "Epoch  25 , loss  0.5941973\n",
      "Epoch  26 , loss  0.61266524\n",
      "Epoch  26 , loss  0.61266524\n",
      "Epoch  27 , loss  0.5559637\n",
      "Epoch  27 , loss  0.5559637\n",
      "Epoch  28 , loss  0.48886153\n",
      "Epoch  28 , loss  0.48886153\n",
      "Epoch  29 , loss  0.4525174\n",
      "Epoch  29 , loss  0.4525174\n",
      "Epoch  30 , loss  0.45634696\n",
      "Epoch  30 , loss  0.45634696\n",
      "Epoch  31 , loss  0.44451606\n",
      "Epoch  31 , loss  0.44451606\n",
      "Epoch  32 , loss  0.49858454\n",
      "Epoch  32 , loss  0.49858454\n",
      "Epoch  33 , loss  0.43551597\n",
      "Epoch  33 , loss  0.43551597\n",
      "Epoch  34 , loss  0.42370036\n",
      "Epoch  34 , loss  0.42370036\n",
      "Epoch  35 , loss  0.43842915\n",
      "Epoch  35 , loss  0.43842915\n",
      "Epoch  36 , loss  0.44877103\n",
      "Epoch  36 , loss  0.44877103\n",
      "Epoch  37 , loss  0.38349572\n",
      "Epoch  37 , loss  0.38349572\n",
      "Epoch  38 , loss  0.36622858\n",
      "Epoch  38 , loss  0.36622858\n",
      "Epoch  39 , loss  0.35636938\n",
      "Epoch  39 , loss  0.35636938\n",
      "Epoch  40 , loss  0.35246778\n",
      "Epoch  40 , loss  0.35246778\n",
      "Epoch  41 , loss  0.34721538\n",
      "Epoch  41 , loss  0.34721538\n",
      "Epoch  42 , loss  0.3637439\n",
      "Epoch  42 , loss  0.3637439\n",
      "Epoch  43 , loss  0.35365805\n",
      "Epoch  43 , loss  0.35365805\n",
      "Epoch  44 , loss  0.33339384\n",
      "Epoch  44 , loss  0.33339384\n",
      "Epoch  45 , loss  0.3296621\n",
      "Epoch  45 , loss  0.3296621\n",
      "Epoch  46 , loss  0.31653008\n",
      "Epoch  46 , loss  0.31653008\n",
      "Epoch  47 , loss  0.30432138\n",
      "Epoch  47 , loss  0.30432138\n",
      "Epoch  48 , loss  0.3561572\n",
      "Epoch  48 , loss  0.3561572\n",
      "Epoch  49 , loss  0.32257974\n",
      "Epoch  49 , loss  0.32257974\n",
      "Epoch  50 , loss  0.3000014\n",
      "Epoch  50 , loss  0.3000014\n",
      "Epoch  51 , loss  0.31341147\n",
      "Epoch  51 , loss  0.31341147\n",
      "Epoch  52 , loss  0.34659585\n",
      "Epoch  52 , loss  0.34659585\n",
      "Epoch  53 , loss  0.308197\n",
      "Epoch  53 , loss  0.308197\n",
      "Epoch  54 , loss  0.30054602\n",
      "Epoch  54 , loss  0.30054602\n",
      "Epoch  55 , loss  0.28170997\n",
      "Epoch  55 , loss  0.28170997\n",
      "Epoch  56 , loss  0.28639102\n",
      "Epoch  56 , loss  0.28639102\n",
      "Epoch  57 , loss  0.31353858\n",
      "Epoch  57 , loss  0.31353858\n",
      "Epoch  58 , loss  0.33643642\n",
      "Epoch  58 , loss  0.33643642\n",
      "Epoch  59 , loss  0.3229234\n",
      "Epoch  59 , loss  0.3229234\n",
      "Epoch  60 , loss  0.2894316\n",
      "Epoch  60 , loss  0.2894316\n",
      "Epoch  61 , loss  0.3014358\n",
      "Epoch  61 , loss  0.3014358\n",
      "Epoch  62 , loss  0.29446927\n",
      "Epoch  62 , loss  0.29446927\n",
      "Epoch  63 , loss  0.2701199\n",
      "Epoch  63 , loss  0.2701199\n",
      "Epoch  64 , loss  0.26981264\n",
      "Epoch  64 , loss  0.26981264\n",
      "Epoch  65 , loss  0.26785848\n",
      "Epoch  65 , loss  0.26785848\n",
      "Epoch  66 , loss  0.29761064\n",
      "Epoch  66 , loss  0.29761064\n",
      "Epoch  67 , loss  0.2670891\n",
      "Epoch  67 , loss  0.2670891\n",
      "Epoch  68 , loss  0.25753155\n",
      "Epoch  68 , loss  0.25753155\n",
      "Epoch  69 , loss  0.26564893\n",
      "Epoch  69 , loss  0.26564893\n",
      "Epoch  70 , loss  0.3130432\n",
      "Epoch  70 , loss  0.3130432\n",
      "Epoch  71 , loss  0.2696105\n",
      "Epoch  71 , loss  0.2696105\n",
      "Epoch  72 , loss  0.25402823\n",
      "Epoch  72 , loss  0.25402823\n",
      "Epoch  73 , loss  0.23747416\n",
      "Epoch  73 , loss  0.23747416\n",
      "Epoch  74 , loss  0.24924053\n",
      "Epoch  74 , loss  0.24924053\n",
      "Epoch  75 , loss  0.24617441\n",
      "Epoch  75 , loss  0.24617441\n",
      "Epoch  76 , loss  0.2544726\n",
      "Epoch  76 , loss  0.2544726\n",
      "Epoch  77 , loss  0.24659401\n",
      "Epoch  77 , loss  0.24659401\n",
      "Epoch  78 , loss  0.23530345\n",
      "Epoch  78 , loss  0.23530345\n",
      "Epoch  79 , loss  0.2472076\n",
      "Epoch  79 , loss  0.2472076\n",
      "Epoch  80 , loss  0.23642062\n",
      "Epoch  80 , loss  0.23642062\n",
      "Epoch  81 , loss  0.25040707\n",
      "Epoch  81 , loss  0.25040707\n",
      "Epoch  82 , loss  0.24177502\n",
      "Epoch  82 , loss  0.24177502\n",
      "Epoch  83 , loss  0.24065284\n",
      "Epoch  83 , loss  0.24065284\n",
      "Epoch  84 , loss  0.24532616\n",
      "Epoch  84 , loss  0.24532616\n",
      "Epoch  85 , loss  0.24486\n",
      "Epoch  85 , loss  0.24486\n",
      "Epoch  86 , loss  0.22965486\n",
      "Epoch  86 , loss  0.22965486\n",
      "Epoch  87 , loss  0.23807134\n",
      "Epoch  87 , loss  0.23807134\n",
      "Epoch  88 , loss  0.23908657\n",
      "Epoch  88 , loss  0.23908657\n",
      "Epoch  89 , loss  0.2454647\n",
      "Epoch  89 , loss  0.2454647\n",
      "Epoch  90 , loss  0.22729059\n",
      "Epoch  90 , loss  0.22729059\n",
      "Epoch  91 , loss  0.22308332\n",
      "Epoch  91 , loss  0.22308332\n",
      "Epoch  92 , loss  0.22803931\n",
      "Epoch  92 , loss  0.22803931\n",
      "Epoch  93 , loss  0.23558976\n",
      "Epoch  93 , loss  0.23558976\n",
      "Epoch  94 , loss  0.22574975\n",
      "Epoch  94 , loss  0.22574975\n",
      "Epoch  95 , loss  0.21889687\n",
      "Epoch  95 , loss  0.21889687\n",
      "Epoch  96 , loss  0.21713829\n",
      "Epoch  96 , loss  0.21713829\n",
      "Epoch  97 , loss  0.22058928\n",
      "Epoch  97 , loss  0.22058928\n",
      "Epoch  98 , loss  0.21911828\n",
      "Epoch  98 , loss  0.21911828\n",
      "Epoch  99 , loss  0.223683\n",
      "10 layers\n",
      "Train:  {'Loss': 0.22368299961090088, 'Acc:': 0.8047971725463867, 'rso50': 1.0, 'margin': 0.11334961652755737}\n",
      "Eval:  {'Loss': 0.18891529738903046, 'Acc:': 0.8384930491447449, 'rso50': 1.0, 'margin': 0.20536351203918457}\n",
      "Epoch  99 , loss  0.223683\n",
      "10 layers\n",
      "Train:  {'Loss': 0.22368299961090088, 'Acc:': 0.8047971725463867, 'rso50': 1.0, 'margin': 0.11334961652755737}\n",
      "Eval:  {'Loss': 0.18891529738903046, 'Acc:': 0.8384930491447449, 'rso50': 1.0, 'margin': 0.20536351203918457}\n",
      "Epoch  0 , loss  0.7502758\n",
      "Epoch  0 , loss  0.7502758\n",
      "Epoch  1 , loss  0.70503217\n",
      "Epoch  1 , loss  0.70503217\n",
      "Epoch  2 , loss  0.7021999\n",
      "Epoch  2 , loss  0.7021999\n",
      "Epoch  3 , loss  0.69893336\n",
      "Epoch  3 , loss  0.69893336\n",
      "Epoch  4 , loss  0.69644994\n",
      "Epoch  4 , loss  0.69644994\n",
      "Epoch  5 , loss  0.7056586\n",
      "Epoch  5 , loss  0.7056586\n",
      "Epoch  6 , loss  0.6986885\n",
      "Epoch  6 , loss  0.6986885\n",
      "Epoch  7 , loss  0.7051955\n",
      "Epoch  7 , loss  0.7051955\n",
      "Epoch  8 , loss  0.693445\n",
      "Epoch  8 , loss  0.693445\n",
      "Epoch  9 , loss  0.6872437\n",
      "Epoch  9 , loss  0.6872437\n",
      "Epoch  10 , loss  0.68747497\n",
      "Epoch  10 , loss  0.68747497\n",
      "Epoch  11 , loss  0.68923646\n",
      "Epoch  11 , loss  0.68923646\n",
      "Epoch  12 , loss  0.6923427\n",
      "Epoch  12 , loss  0.6923427\n",
      "Epoch  13 , loss  0.6973953\n",
      "Epoch  13 , loss  0.6973953\n",
      "Epoch  14 , loss  0.68876237\n",
      "Epoch  14 , loss  0.68876237\n",
      "Epoch  15 , loss  0.67365485\n",
      "Epoch  15 , loss  0.67365485\n",
      "Epoch  16 , loss  0.672894\n",
      "Epoch  16 , loss  0.672894\n",
      "Epoch  17 , loss  0.6615826\n",
      "Epoch  17 , loss  0.6615826\n",
      "Epoch  18 , loss  0.6713845\n",
      "Epoch  18 , loss  0.6713845\n",
      "Epoch  19 , loss  0.69330794\n",
      "Epoch  19 , loss  0.69330794\n",
      "Epoch  20 , loss  0.673491\n",
      "Epoch  20 , loss  0.673491\n",
      "Epoch  21 , loss  0.6628827\n",
      "Epoch  21 , loss  0.6628827\n",
      "Epoch  22 , loss  0.6663767\n",
      "Epoch  22 , loss  0.6663767\n",
      "Epoch  23 , loss  0.6785119\n",
      "Epoch  23 , loss  0.6785119\n",
      "Epoch  24 , loss  0.6805658\n",
      "Epoch  24 , loss  0.6805658\n",
      "Epoch  25 , loss  0.6793253\n",
      "Epoch  25 , loss  0.6793253\n",
      "Epoch  26 , loss  0.6783345\n",
      "Epoch  26 , loss  0.6783345\n",
      "Epoch  27 , loss  0.66287833\n",
      "Epoch  27 , loss  0.66287833\n",
      "Epoch  28 , loss  0.68144965\n",
      "Epoch  28 , loss  0.68144965\n",
      "Epoch  29 , loss  0.6548051\n",
      "Epoch  29 , loss  0.6548051\n",
      "Epoch  30 , loss  0.69886017\n",
      "Epoch  30 , loss  0.69886017\n",
      "Epoch  31 , loss  0.66853017\n",
      "Epoch  31 , loss  0.66853017\n",
      "Epoch  32 , loss  0.6480128\n",
      "Epoch  32 , loss  0.6480128\n",
      "Epoch  33 , loss  0.6236247\n",
      "Epoch  33 , loss  0.6236247\n",
      "Epoch  34 , loss  0.63219225\n",
      "Epoch  34 , loss  0.63219225\n",
      "Epoch  35 , loss  0.6585095\n",
      "Epoch  35 , loss  0.6585095\n",
      "Epoch  36 , loss  0.7154331\n",
      "Epoch  36 , loss  0.7154331\n",
      "Epoch  37 , loss  0.6109163\n",
      "Epoch  37 , loss  0.6109163\n",
      "Epoch  38 , loss  0.5894057\n",
      "Epoch  38 , loss  0.5894057\n",
      "Epoch  39 , loss  0.57015496\n",
      "Epoch  39 , loss  0.57015496\n",
      "Epoch  40 , loss  0.4953532\n",
      "Epoch  40 , loss  0.4953532\n",
      "Epoch  41 , loss  0.48670042\n",
      "Epoch  41 , loss  0.48670042\n",
      "Epoch  42 , loss  0.46588027\n",
      "Epoch  42 , loss  0.46588027\n",
      "Epoch  43 , loss  0.45825097\n",
      "Epoch  43 , loss  0.45825097\n",
      "Epoch  44 , loss  0.4902331\n",
      "Epoch  44 , loss  0.4902331\n",
      "Epoch  45 , loss  0.52731854\n",
      "Epoch  45 , loss  0.52731854\n",
      "Epoch  46 , loss  0.5087783\n",
      "Epoch  46 , loss  0.5087783\n",
      "Epoch  47 , loss  0.4671655\n",
      "Epoch  47 , loss  0.4671655\n",
      "Epoch  48 , loss  0.42667797\n",
      "Epoch  48 , loss  0.42667797\n",
      "Epoch  49 , loss  0.39609584\n",
      "Epoch  49 , loss  0.39609584\n",
      "Epoch  50 , loss  0.37061584\n",
      "Epoch  50 , loss  0.37061584\n",
      "Epoch  51 , loss  0.41002658\n",
      "Epoch  51 , loss  0.41002658\n",
      "Epoch  52 , loss  0.68025875\n",
      "Epoch  52 , loss  0.68025875\n",
      "Epoch  53 , loss  0.578925\n",
      "Epoch  53 , loss  0.578925\n",
      "Epoch  54 , loss  0.54474306\n",
      "Epoch  54 , loss  0.54474306\n",
      "Epoch  55 , loss  0.52515703\n",
      "Epoch  55 , loss  0.52515703\n",
      "Epoch  56 , loss  0.47897568\n",
      "Epoch  56 , loss  0.47897568\n",
      "Epoch  57 , loss  0.4638469\n",
      "Epoch  57 , loss  0.4638469\n",
      "Epoch  58 , loss  0.42609397\n",
      "Epoch  58 , loss  0.42609397\n",
      "Epoch  59 , loss  0.42250958\n",
      "Epoch  59 , loss  0.42250958\n",
      "Epoch  60 , loss  0.3960475\n",
      "Epoch  60 , loss  0.3960475\n",
      "Epoch  61 , loss  0.38547692\n",
      "Epoch  61 , loss  0.38547692\n",
      "Epoch  62 , loss  0.36294535\n",
      "Epoch  62 , loss  0.36294535\n",
      "Epoch  63 , loss  0.36508617\n",
      "Epoch  63 , loss  0.36508617\n",
      "Epoch  64 , loss  0.37319675\n",
      "Epoch  64 , loss  0.37319675\n",
      "Epoch  65 , loss  0.3686926\n",
      "Epoch  65 , loss  0.3686926\n",
      "Epoch  66 , loss  0.3457913\n",
      "Epoch  66 , loss  0.3457913\n",
      "Epoch  67 , loss  0.33405817\n",
      "Epoch  67 , loss  0.33405817\n",
      "Epoch  68 , loss  0.34035656\n",
      "Epoch  68 , loss  0.34035656\n",
      "Epoch  69 , loss  0.35073984\n",
      "Epoch  69 , loss  0.35073984\n",
      "Epoch  70 , loss  0.34754738\n",
      "Epoch  70 , loss  0.34754738\n",
      "Epoch  71 , loss  0.35506797\n",
      "Epoch  71 , loss  0.35506797\n",
      "Epoch  72 , loss  0.32906166\n",
      "Epoch  72 , loss  0.32906166\n",
      "Epoch  73 , loss  0.32464954\n",
      "Epoch  73 , loss  0.32464954\n",
      "Epoch  74 , loss  0.3131661\n",
      "Epoch  74 , loss  0.3131661\n",
      "Epoch  75 , loss  0.30349323\n",
      "Epoch  75 , loss  0.30349323\n",
      "Epoch  76 , loss  0.30297023\n",
      "Epoch  76 , loss  0.30297023\n",
      "Epoch  77 , loss  0.30570373\n",
      "Epoch  77 , loss  0.30570373\n",
      "Epoch  78 , loss  0.29846847\n",
      "Epoch  78 , loss  0.29846847\n",
      "Epoch  79 , loss  0.29425764\n",
      "Epoch  79 , loss  0.29425764\n",
      "Epoch  80 , loss  0.29979673\n",
      "Epoch  80 , loss  0.29979673\n",
      "Epoch  81 , loss  0.312362\n",
      "Epoch  81 , loss  0.312362\n",
      "Epoch  82 , loss  0.29665962\n",
      "Epoch  82 , loss  0.29665962\n",
      "Epoch  83 , loss  0.29142204\n",
      "Epoch  83 , loss  0.29142204\n",
      "Epoch  84 , loss  0.2878224\n",
      "Epoch  84 , loss  0.2878224\n",
      "Epoch  85 , loss  0.2856898\n",
      "Epoch  85 , loss  0.2856898\n",
      "Epoch  86 , loss  0.27215096\n",
      "Epoch  86 , loss  0.27215096\n",
      "Epoch  87 , loss  0.279268\n",
      "Epoch  87 , loss  0.279268\n",
      "Epoch  88 , loss  0.2797046\n",
      "Epoch  88 , loss  0.2797046\n",
      "Epoch  89 , loss  0.29546663\n",
      "Epoch  89 , loss  0.29546663\n",
      "Epoch  90 , loss  0.28146124\n",
      "Epoch  90 , loss  0.28146124\n",
      "Epoch  91 , loss  0.26743174\n",
      "Epoch  91 , loss  0.26743174\n",
      "Epoch  92 , loss  0.26433727\n",
      "Epoch  92 , loss  0.26433727\n",
      "Epoch  93 , loss  0.27069196\n",
      "Epoch  93 , loss  0.27069196\n",
      "Epoch  94 , loss  0.26609305\n",
      "Epoch  94 , loss  0.26609305\n",
      "Epoch  95 , loss  0.2595042\n",
      "Epoch  95 , loss  0.2595042\n",
      "Epoch  96 , loss  0.28445446\n",
      "Epoch  96 , loss  0.28445446\n",
      "Epoch  97 , loss  0.26522055\n",
      "Epoch  97 , loss  0.26522055\n",
      "Epoch  98 , loss  0.25737277\n",
      "Epoch  98 , loss  0.25737277\n",
      "Epoch  99 , loss  0.2700006\n",
      "11 layers\n",
      "Train:  {'Loss': 0.2700006067752838, 'Acc:': 0.7708733677864075, 'rso50': 1.0, 'margin': 0.04425358772277832}\n",
      "Eval:  {'Loss': 0.28556928038597107, 'Acc:': 0.761853039264679, 'rso50': 1.0, 'margin': 0.1372309923171997}\n",
      "Epoch  99 , loss  0.2700006\n",
      "11 layers\n",
      "Train:  {'Loss': 0.2700006067752838, 'Acc:': 0.7708733677864075, 'rso50': 1.0, 'margin': 0.04425358772277832}\n",
      "Eval:  {'Loss': 0.28556928038597107, 'Acc:': 0.761853039264679, 'rso50': 1.0, 'margin': 0.1372309923171997}\n",
      "Epoch  0 , loss  0.6999466\n",
      "Epoch  0 , loss  0.6999466\n",
      "Epoch  1 , loss  0.69442827\n",
      "Epoch  1 , loss  0.69442827\n",
      "Epoch  2 , loss  0.6877472\n",
      "Epoch  2 , loss  0.6877472\n",
      "Epoch  3 , loss  0.6906113\n",
      "Epoch  3 , loss  0.6906113\n",
      "Epoch  4 , loss  0.6814994\n",
      "Epoch  4 , loss  0.6814994\n",
      "Epoch  5 , loss  0.6801165\n",
      "Epoch  5 , loss  0.6801165\n",
      "Epoch  6 , loss  0.67330146\n",
      "Epoch  6 , loss  0.67330146\n",
      "Epoch  7 , loss  0.6679967\n",
      "Epoch  7 , loss  0.6679967\n",
      "Epoch  8 , loss  0.6697874\n",
      "Epoch  8 , loss  0.6697874\n",
      "Epoch  9 , loss  0.6700673\n",
      "Epoch  9 , loss  0.6700673\n",
      "Epoch  10 , loss  0.6924806\n",
      "Epoch  10 , loss  0.6924806\n",
      "Epoch  11 , loss  0.666583\n",
      "Epoch  11 , loss  0.666583\n",
      "Epoch  12 , loss  0.63199776\n",
      "Epoch  12 , loss  0.63199776\n",
      "Epoch  13 , loss  0.61131006\n",
      "Epoch  13 , loss  0.61131006\n",
      "Epoch  14 , loss  0.61852926\n",
      "Epoch  14 , loss  0.61852926\n",
      "Epoch  15 , loss  0.6381272\n",
      "Epoch  15 , loss  0.6381272\n",
      "Epoch  16 , loss  0.6149511\n",
      "Epoch  16 , loss  0.6149511\n",
      "Epoch  17 , loss  0.61142915\n",
      "Epoch  17 , loss  0.61142915\n",
      "Epoch  18 , loss  0.72284174\n",
      "Epoch  18 , loss  0.72284174\n",
      "Epoch  19 , loss  0.6833484\n",
      "Epoch  19 , loss  0.6833484\n",
      "Epoch  20 , loss  0.60893184\n",
      "Epoch  20 , loss  0.60893184\n",
      "Epoch  21 , loss  0.59211993\n",
      "Epoch  21 , loss  0.59211993\n",
      "Epoch  22 , loss  0.6025687\n",
      "Epoch  22 , loss  0.6025687\n",
      "Epoch  23 , loss  0.57076883\n",
      "Epoch  23 , loss  0.57076883\n",
      "Epoch  24 , loss  0.5251769\n",
      "Epoch  24 , loss  0.5251769\n",
      "Epoch  25 , loss  0.5852527\n",
      "Epoch  25 , loss  0.5852527\n",
      "Epoch  26 , loss  0.48242006\n",
      "Epoch  26 , loss  0.48242006\n",
      "Epoch  27 , loss  0.48213252\n",
      "Epoch  27 , loss  0.48213252\n",
      "Epoch  28 , loss  0.45839205\n",
      "Epoch  28 , loss  0.45839205\n",
      "Epoch  29 , loss  0.4870919\n",
      "Epoch  29 , loss  0.4870919\n",
      "Epoch  30 , loss  0.55405205\n",
      "Epoch  30 , loss  0.55405205\n",
      "Epoch  31 , loss  0.64252466\n",
      "Epoch  31 , loss  0.64252466\n",
      "Epoch  32 , loss  0.72243476\n",
      "Epoch  32 , loss  0.72243476\n",
      "Epoch  33 , loss  0.65688\n",
      "Epoch  33 , loss  0.65688\n",
      "Epoch  34 , loss  0.50560266\n",
      "Epoch  34 , loss  0.50560266\n",
      "Epoch  35 , loss  0.46335235\n",
      "Epoch  35 , loss  0.46335235\n",
      "Epoch  36 , loss  0.4320916\n",
      "Epoch  36 , loss  0.4320916\n",
      "Epoch  37 , loss  0.43567118\n",
      "Epoch  37 , loss  0.43567118\n",
      "Epoch  38 , loss  0.46356466\n",
      "Epoch  38 , loss  0.46356466\n",
      "Epoch  39 , loss  0.548145\n",
      "Epoch  39 , loss  0.548145\n",
      "Epoch  40 , loss  0.4729952\n",
      "Epoch  40 , loss  0.4729952\n",
      "Epoch  41 , loss  0.4129385\n",
      "Epoch  41 , loss  0.4129385\n",
      "Epoch  42 , loss  0.4006175\n",
      "Epoch  42 , loss  0.4006175\n",
      "Epoch  43 , loss  0.45119885\n",
      "Epoch  43 , loss  0.45119885\n",
      "Epoch  44 , loss  0.42704877\n",
      "Epoch  44 , loss  0.42704877\n",
      "Epoch  45 , loss  0.38352117\n",
      "Epoch  45 , loss  0.38352117\n",
      "Epoch  46 , loss  0.37188807\n",
      "Epoch  46 , loss  0.37188807\n",
      "Epoch  47 , loss  0.35373008\n",
      "Epoch  47 , loss  0.35373008\n",
      "Epoch  48 , loss  0.33388984\n",
      "Epoch  48 , loss  0.33388984\n",
      "Epoch  49 , loss  0.36628184\n",
      "Epoch  49 , loss  0.36628184\n",
      "Epoch  50 , loss  0.32044983\n",
      "Epoch  50 , loss  0.32044983\n",
      "Epoch  51 , loss  0.3162238\n",
      "Epoch  51 , loss  0.3162238\n",
      "Epoch  52 , loss  0.33650136\n",
      "Epoch  52 , loss  0.33650136\n",
      "Epoch  53 , loss  0.27800027\n",
      "Epoch  53 , loss  0.27800027\n",
      "Epoch  54 , loss  0.26942238\n",
      "Epoch  54 , loss  0.26942238\n",
      "Epoch  55 , loss  0.27938232\n",
      "Epoch  55 , loss  0.27938232\n",
      "Epoch  56 , loss  0.27419388\n",
      "Epoch  56 , loss  0.27419388\n",
      "Epoch  57 , loss  0.26702964\n",
      "Epoch  57 , loss  0.26702964\n",
      "Epoch  58 , loss  0.2516191\n",
      "Epoch  58 , loss  0.2516191\n",
      "Epoch  59 , loss  0.22626732\n",
      "Epoch  59 , loss  0.22626732\n",
      "Epoch  60 , loss  0.24393958\n",
      "Epoch  60 , loss  0.24393958\n",
      "Epoch  61 , loss  0.2330233\n",
      "Epoch  61 , loss  0.2330233\n",
      "Epoch  62 , loss  0.23265739\n",
      "Epoch  62 , loss  0.23265739\n",
      "Epoch  63 , loss  0.24671043\n",
      "Epoch  63 , loss  0.24671043\n",
      "Epoch  64 , loss  0.21674746\n",
      "Epoch  64 , loss  0.21674746\n",
      "Epoch  65 , loss  0.21302658\n",
      "Epoch  65 , loss  0.21302658\n",
      "Epoch  66 , loss  0.21034189\n",
      "Epoch  66 , loss  0.21034189\n",
      "Epoch  67 , loss  0.19933026\n",
      "Epoch  67 , loss  0.19933026\n",
      "Epoch  68 , loss  0.24766217\n",
      "Epoch  68 , loss  0.24766217\n",
      "Epoch  69 , loss  0.25722417\n",
      "Epoch  69 , loss  0.25722417\n",
      "Epoch  70 , loss  0.202985\n",
      "Epoch  70 , loss  0.202985\n",
      "Epoch  71 , loss  0.2026969\n",
      "Epoch  71 , loss  0.2026969\n",
      "Epoch  72 , loss  0.20169298\n",
      "Epoch  72 , loss  0.20169298\n",
      "Epoch  73 , loss  0.20023994\n",
      "Epoch  73 , loss  0.20023994\n",
      "Epoch  74 , loss  0.18395479\n",
      "Epoch  74 , loss  0.18395479\n",
      "Epoch  75 , loss  0.19247663\n",
      "Epoch  75 , loss  0.19247663\n",
      "Epoch  76 , loss  0.18042068\n",
      "Epoch  76 , loss  0.18042068\n",
      "Epoch  77 , loss  0.18441944\n",
      "Epoch  77 , loss  0.18441944\n",
      "Epoch  78 , loss  0.19207533\n",
      "Epoch  78 , loss  0.19207533\n",
      "Epoch  79 , loss  0.18345898\n",
      "Epoch  79 , loss  0.18345898\n",
      "Epoch  80 , loss  0.18107991\n",
      "Epoch  80 , loss  0.18107991\n",
      "Epoch  81 , loss  0.19087344\n",
      "Epoch  81 , loss  0.19087344\n",
      "Epoch  82 , loss  0.1862367\n",
      "Epoch  82 , loss  0.1862367\n",
      "Epoch  83 , loss  0.20225476\n",
      "Epoch  83 , loss  0.20225476\n",
      "Epoch  84 , loss  0.17670906\n",
      "Epoch  84 , loss  0.17670906\n",
      "Epoch  85 , loss  0.18330026\n",
      "Epoch  85 , loss  0.18330026\n",
      "Epoch  86 , loss  0.18639481\n",
      "Epoch  86 , loss  0.18639481\n",
      "Epoch  87 , loss  0.1620961\n",
      "Epoch  87 , loss  0.1620961\n",
      "Epoch  88 , loss  0.17081662\n",
      "Epoch  88 , loss  0.17081662\n",
      "Epoch  89 , loss  0.17372382\n",
      "Epoch  89 , loss  0.17372382\n",
      "Epoch  90 , loss  0.16582498\n",
      "Epoch  90 , loss  0.16582498\n",
      "Epoch  91 , loss  0.16523744\n",
      "Epoch  91 , loss  0.16523744\n",
      "Epoch  92 , loss  0.1739459\n",
      "Epoch  92 , loss  0.1739459\n",
      "Epoch  93 , loss  0.18057297\n",
      "Epoch  93 , loss  0.18057297\n",
      "Epoch  94 , loss  0.16384569\n",
      "Epoch  94 , loss  0.16384569\n",
      "Epoch  95 , loss  0.15849856\n",
      "Epoch  95 , loss  0.15849856\n",
      "Epoch  96 , loss  0.16617388\n",
      "Epoch  96 , loss  0.16617388\n",
      "Epoch  97 , loss  0.15928723\n",
      "Epoch  97 , loss  0.15928723\n",
      "Epoch  98 , loss  0.16416925\n",
      "Epoch  98 , loss  0.16416925\n",
      "Epoch  99 , loss  0.15603217\n",
      "12 layers\n",
      "Train:  {'Loss': 0.15603217482566833, 'Acc:': 0.863246738910675, 'rso50': 1.0, 'margin': 0.01887422800064087}\n",
      "Eval:  {'Loss': 0.069516122341156, 'Acc:': 0.9333875775337219, 'rso50': 1.0, 'margin': 0.40445923805236816}\n",
      "Epoch  99 , loss  0.15603217\n",
      "12 layers\n",
      "Train:  {'Loss': 0.15603217482566833, 'Acc:': 0.863246738910675, 'rso50': 1.0, 'margin': 0.01887422800064087}\n",
      "Eval:  {'Loss': 0.069516122341156, 'Acc:': 0.9333875775337219, 'rso50': 1.0, 'margin': 0.40445923805236816}\n",
      "Epoch  0 , loss  0.7261486\n",
      "Epoch  0 , loss  0.7261486\n",
      "Epoch  1 , loss  0.680111\n",
      "Epoch  1 , loss  0.680111\n",
      "Epoch  2 , loss  0.67788005\n",
      "Epoch  2 , loss  0.67788005\n",
      "Epoch  3 , loss  0.69135547\n",
      "Epoch  3 , loss  0.69135547\n",
      "Epoch  4 , loss  0.65535337\n",
      "Epoch  4 , loss  0.65535337\n",
      "Epoch  5 , loss  0.6574456\n",
      "Epoch  5 , loss  0.6574456\n",
      "Epoch  6 , loss  0.63621646\n",
      "Epoch  6 , loss  0.63621646\n",
      "Epoch  7 , loss  0.6178434\n",
      "Epoch  7 , loss  0.6178434\n",
      "Epoch  8 , loss  0.67095447\n",
      "Epoch  8 , loss  0.67095447\n",
      "Epoch  9 , loss  0.6592813\n",
      "Epoch  9 , loss  0.6592813\n",
      "Epoch  10 , loss  0.6111242\n",
      "Epoch  10 , loss  0.6111242\n",
      "Epoch  11 , loss  0.5771011\n",
      "Epoch  11 , loss  0.5771011\n",
      "Epoch  12 , loss  0.5595305\n",
      "Epoch  12 , loss  0.5595305\n",
      "Epoch  13 , loss  0.54457355\n",
      "Epoch  13 , loss  0.54457355\n",
      "Epoch  14 , loss  0.5844778\n",
      "Epoch  14 , loss  0.5844778\n",
      "Epoch  15 , loss  0.53326666\n",
      "Epoch  15 , loss  0.53326666\n",
      "Epoch  16 , loss  0.5534516\n",
      "Epoch  16 , loss  0.5534516\n",
      "Epoch  17 , loss  0.4506972\n",
      "Epoch  17 , loss  0.4506972\n",
      "Epoch  18 , loss  0.43045917\n",
      "Epoch  18 , loss  0.43045917\n",
      "Epoch  19 , loss  0.4063228\n",
      "Epoch  19 , loss  0.4063228\n",
      "Epoch  20 , loss  0.49093798\n",
      "Epoch  20 , loss  0.49093798\n",
      "Epoch  21 , loss  0.48231718\n",
      "Epoch  21 , loss  0.48231718\n",
      "Epoch  22 , loss  0.4080758\n",
      "Epoch  22 , loss  0.4080758\n",
      "Epoch  23 , loss  0.3684083\n",
      "Epoch  23 , loss  0.3684083\n",
      "Epoch  24 , loss  0.41051912\n",
      "Epoch  24 , loss  0.41051912\n",
      "Epoch  25 , loss  0.42196372\n",
      "Epoch  25 , loss  0.42196372\n",
      "Epoch  26 , loss  0.5799668\n",
      "Epoch  26 , loss  0.5799668\n",
      "Epoch  27 , loss  0.5894954\n",
      "Epoch  27 , loss  0.5894954\n",
      "Epoch  28 , loss  0.6072623\n",
      "Epoch  28 , loss  0.6072623\n",
      "Epoch  29 , loss  0.5078556\n",
      "Epoch  29 , loss  0.5078556\n",
      "Epoch  30 , loss  0.525366\n",
      "Epoch  30 , loss  0.525366\n",
      "Epoch  31 , loss  0.50793135\n",
      "Epoch  31 , loss  0.50793135\n",
      "Epoch  32 , loss  0.41588846\n",
      "Epoch  32 , loss  0.41588846\n",
      "Epoch  33 , loss  0.5846843\n",
      "Epoch  33 , loss  0.5846843\n",
      "Epoch  34 , loss  0.63525504\n",
      "Epoch  34 , loss  0.63525504\n",
      "Epoch  35 , loss  0.5352686\n",
      "Epoch  35 , loss  0.5352686\n",
      "Epoch  36 , loss  0.50468063\n",
      "Epoch  36 , loss  0.50468063\n",
      "Epoch  37 , loss  0.4531912\n",
      "Epoch  37 , loss  0.4531912\n",
      "Epoch  38 , loss  0.43440923\n",
      "Epoch  38 , loss  0.43440923\n",
      "Epoch  39 , loss  0.38873088\n",
      "Epoch  39 , loss  0.38873088\n",
      "Epoch  40 , loss  0.37881222\n",
      "Epoch  40 , loss  0.37881222\n",
      "Epoch  41 , loss  0.3761861\n",
      "Epoch  41 , loss  0.3761861\n",
      "Epoch  42 , loss  0.35532078\n",
      "Epoch  42 , loss  0.35532078\n",
      "Epoch  43 , loss  0.34347618\n",
      "Epoch  43 , loss  0.34347618\n",
      "Epoch  44 , loss  0.3705887\n",
      "Epoch  44 , loss  0.3705887\n",
      "Epoch  45 , loss  0.33850372\n",
      "Epoch  45 , loss  0.33850372\n",
      "Epoch  46 , loss  0.3277969\n",
      "Epoch  46 , loss  0.3277969\n",
      "Epoch  47 , loss  0.28993014\n",
      "Epoch  47 , loss  0.28993014\n",
      "Epoch  48 , loss  0.3263146\n",
      "Epoch  48 , loss  0.3263146\n",
      "Epoch  49 , loss  0.339983\n",
      "Epoch  49 , loss  0.339983\n",
      "Epoch  50 , loss  0.29693982\n",
      "Epoch  50 , loss  0.29693982\n",
      "Epoch  51 , loss  0.29041985\n",
      "Epoch  51 , loss  0.29041985\n",
      "Epoch  52 , loss  0.28963718\n",
      "Epoch  52 , loss  0.28963718\n",
      "Epoch  53 , loss  0.278746\n",
      "Epoch  53 , loss  0.278746\n",
      "Epoch  54 , loss  0.29765224\n",
      "Epoch  54 , loss  0.29765224\n",
      "Epoch  55 , loss  0.2773883\n",
      "Epoch  55 , loss  0.2773883\n",
      "Epoch  56 , loss  0.28088546\n",
      "Epoch  56 , loss  0.28088546\n",
      "Epoch  57 , loss  0.25216097\n",
      "Epoch  57 , loss  0.25216097\n",
      "Epoch  58 , loss  0.2566745\n",
      "Epoch  58 , loss  0.2566745\n",
      "Epoch  59 , loss  0.26524284\n",
      "Epoch  59 , loss  0.26524284\n",
      "Epoch  60 , loss  0.2617392\n",
      "Epoch  60 , loss  0.2617392\n",
      "Epoch  61 , loss  0.25515488\n",
      "Epoch  61 , loss  0.25515488\n",
      "Epoch  62 , loss  0.25638345\n",
      "Epoch  62 , loss  0.25638345\n",
      "Epoch  63 , loss  0.28173926\n",
      "Epoch  63 , loss  0.28173926\n",
      "Epoch  64 , loss  0.27448142\n",
      "Epoch  64 , loss  0.27448142\n",
      "Epoch  65 , loss  0.25384942\n",
      "Epoch  65 , loss  0.25384942\n",
      "Epoch  66 , loss  0.24487276\n",
      "Epoch  66 , loss  0.24487276\n",
      "Epoch  67 , loss  0.24536653\n",
      "Epoch  67 , loss  0.24536653\n",
      "Epoch  68 , loss  0.24855982\n",
      "Epoch  68 , loss  0.24855982\n",
      "Epoch  69 , loss  0.26516074\n",
      "Epoch  69 , loss  0.26516074\n",
      "Epoch  70 , loss  0.25980434\n",
      "Epoch  70 , loss  0.25980434\n",
      "Epoch  71 , loss  0.25297683\n",
      "Epoch  71 , loss  0.25297683\n",
      "Epoch  72 , loss  0.26300904\n",
      "Epoch  72 , loss  0.26300904\n",
      "Epoch  73 , loss  0.22943039\n",
      "Epoch  73 , loss  0.22943039\n",
      "Epoch  74 , loss  0.22768043\n",
      "Epoch  74 , loss  0.22768043\n",
      "Epoch  75 , loss  0.23070227\n",
      "Epoch  75 , loss  0.23070227\n",
      "Epoch  76 , loss  0.23599392\n",
      "Epoch  76 , loss  0.23599392\n",
      "Epoch  77 , loss  0.25421342\n",
      "Epoch  77 , loss  0.25421342\n",
      "Epoch  78 , loss  0.23498814\n",
      "Epoch  78 , loss  0.23498814\n",
      "Epoch  79 , loss  0.23437719\n",
      "Epoch  79 , loss  0.23437719\n",
      "Epoch  80 , loss  0.2678452\n",
      "Epoch  80 , loss  0.2678452\n",
      "Epoch  81 , loss  0.2748103\n",
      "Epoch  81 , loss  0.2748103\n",
      "Epoch  82 , loss  0.24325024\n",
      "Epoch  82 , loss  0.24325024\n",
      "Epoch  83 , loss  0.23949651\n",
      "Epoch  83 , loss  0.23949651\n",
      "Epoch  84 , loss  0.24098323\n",
      "Epoch  84 , loss  0.24098323\n",
      "Epoch  85 , loss  0.27226114\n",
      "Epoch  85 , loss  0.27226114\n",
      "Epoch  86 , loss  0.25765076\n",
      "Epoch  86 , loss  0.25765076\n",
      "Epoch  87 , loss  0.21950068\n",
      "Epoch  87 , loss  0.21950068\n",
      "Epoch  88 , loss  0.22109471\n",
      "Epoch  88 , loss  0.22109471\n",
      "Epoch  89 , loss  0.21727751\n",
      "Epoch  89 , loss  0.21727751\n",
      "Epoch  90 , loss  0.22099926\n",
      "Epoch  90 , loss  0.22099926\n",
      "Epoch  91 , loss  0.21796532\n",
      "Epoch  91 , loss  0.21796532\n",
      "Epoch  92 , loss  0.2207911\n",
      "Epoch  92 , loss  0.2207911\n",
      "Epoch  93 , loss  0.21587388\n",
      "Epoch  93 , loss  0.21587388\n",
      "Epoch  94 , loss  0.21846952\n",
      "Epoch  94 , loss  0.21846952\n",
      "Epoch  95 , loss  0.20929243\n",
      "Epoch  95 , loss  0.20929243\n",
      "Epoch  96 , loss  0.21676077\n",
      "Epoch  96 , loss  0.21676077\n",
      "Epoch  97 , loss  0.21861805\n",
      "Epoch  97 , loss  0.21861805\n",
      "Epoch  98 , loss  0.2566737\n",
      "Epoch  98 , loss  0.2566737\n",
      "Epoch  99 , loss  0.23102719\n",
      "13 layers\n",
      "Train:  {'Loss': 0.23102718591690063, 'Acc:': 0.8035970330238342, 'rso50': 0.9583333333333334, 'margin': 0.1285422444343567}\n",
      "Eval:  {'Loss': 0.2831231653690338, 'Acc:': 0.765958309173584, 'rso50': 1.0, 'margin': 0.12926006317138672}\n",
      "Epoch  99 , loss  0.23102719\n",
      "13 layers\n",
      "Train:  {'Loss': 0.23102718591690063, 'Acc:': 0.8035970330238342, 'rso50': 0.9583333333333334, 'margin': 0.1285422444343567}\n",
      "Eval:  {'Loss': 0.2831231653690338, 'Acc:': 0.765958309173584, 'rso50': 1.0, 'margin': 0.12926006317138672}\n",
      "Epoch  0 , loss  0.7103146\n",
      "Epoch  0 , loss  0.7103146\n",
      "Epoch  1 , loss  0.6903812\n",
      "Epoch  1 , loss  0.6903812\n",
      "Epoch  2 , loss  0.67735773\n",
      "Epoch  2 , loss  0.67735773\n",
      "Epoch  3 , loss  0.69107634\n",
      "Epoch  3 , loss  0.69107634\n",
      "Epoch  4 , loss  0.6961642\n",
      "Epoch  4 , loss  0.6961642\n",
      "Epoch  5 , loss  0.6870962\n",
      "Epoch  5 , loss  0.6870962\n",
      "Epoch  6 , loss  0.6796968\n",
      "Epoch  6 , loss  0.6796968\n",
      "Epoch  7 , loss  0.66120505\n",
      "Epoch  7 , loss  0.66120505\n",
      "Epoch  8 , loss  0.6647325\n",
      "Epoch  8 , loss  0.6647325\n",
      "Epoch  9 , loss  0.69454306\n",
      "Epoch  9 , loss  0.69454306\n",
      "Epoch  10 , loss  0.664109\n",
      "Epoch  10 , loss  0.664109\n",
      "Epoch  11 , loss  0.64883846\n",
      "Epoch  11 , loss  0.64883846\n",
      "Epoch  12 , loss  0.6780737\n",
      "Epoch  12 , loss  0.6780737\n",
      "Epoch  13 , loss  0.6346783\n",
      "Epoch  13 , loss  0.6346783\n",
      "Epoch  14 , loss  0.60022277\n",
      "Epoch  14 , loss  0.60022277\n",
      "Epoch  15 , loss  0.574973\n",
      "Epoch  15 , loss  0.574973\n",
      "Epoch  16 , loss  0.5866406\n",
      "Epoch  16 , loss  0.5866406\n",
      "Epoch  17 , loss  0.612783\n",
      "Epoch  17 , loss  0.612783\n",
      "Epoch  18 , loss  0.5338555\n",
      "Epoch  18 , loss  0.5338555\n",
      "Epoch  19 , loss  0.54453933\n",
      "Epoch  19 , loss  0.54453933\n",
      "Epoch  20 , loss  0.57157195\n",
      "Epoch  20 , loss  0.57157195\n",
      "Epoch  21 , loss  0.54112667\n",
      "Epoch  21 , loss  0.54112667\n",
      "Epoch  22 , loss  0.58432794\n",
      "Epoch  22 , loss  0.58432794\n",
      "Epoch  23 , loss  0.62989664\n",
      "Epoch  23 , loss  0.62989664\n",
      "Epoch  24 , loss  0.56369764\n",
      "Epoch  24 , loss  0.56369764\n",
      "Epoch  25 , loss  0.52083546\n",
      "Epoch  25 , loss  0.52083546\n",
      "Epoch  26 , loss  0.58154255\n",
      "Epoch  26 , loss  0.58154255\n",
      "Epoch  27 , loss  0.610212\n",
      "Epoch  27 , loss  0.610212\n",
      "Epoch  28 , loss  0.60415554\n",
      "Epoch  28 , loss  0.60415554\n",
      "Epoch  29 , loss  0.785156\n",
      "Epoch  29 , loss  0.785156\n",
      "Epoch  30 , loss  0.6249225\n",
      "Epoch  30 , loss  0.6249225\n",
      "Epoch  31 , loss  0.57228667\n",
      "Epoch  31 , loss  0.57228667\n",
      "Epoch  32 , loss  0.51281065\n",
      "Epoch  32 , loss  0.51281065\n",
      "Epoch  33 , loss  0.43998316\n",
      "Epoch  33 , loss  0.43998316\n",
      "Epoch  34 , loss  0.4519236\n",
      "Epoch  34 , loss  0.4519236\n",
      "Epoch  35 , loss  0.38891378\n",
      "Epoch  35 , loss  0.38891378\n",
      "Epoch  36 , loss  0.37719592\n",
      "Epoch  36 , loss  0.37719592\n",
      "Epoch  37 , loss  0.38151494\n",
      "Epoch  37 , loss  0.38151494\n",
      "Epoch  38 , loss  0.35786995\n",
      "Epoch  38 , loss  0.35786995\n",
      "Epoch  39 , loss  0.3440497\n",
      "Epoch  39 , loss  0.3440497\n",
      "Epoch  40 , loss  0.34481874\n",
      "Epoch  40 , loss  0.34481874\n",
      "Epoch  41 , loss  0.34542158\n",
      "Epoch  41 , loss  0.34542158\n",
      "Epoch  42 , loss  0.31516555\n",
      "Epoch  42 , loss  0.31516555\n",
      "Epoch  43 , loss  0.33377388\n",
      "Epoch  43 , loss  0.33377388\n",
      "Epoch  44 , loss  0.32147658\n",
      "Epoch  44 , loss  0.32147658\n",
      "Epoch  45 , loss  0.2999054\n",
      "Epoch  45 , loss  0.2999054\n",
      "Epoch  46 , loss  0.38594237\n",
      "Epoch  46 , loss  0.38594237\n",
      "Epoch  47 , loss  0.33552957\n",
      "Epoch  47 , loss  0.33552957\n",
      "Epoch  48 , loss  0.3188456\n",
      "Epoch  48 , loss  0.3188456\n",
      "Epoch  49 , loss  0.33194897\n",
      "Epoch  49 , loss  0.33194897\n",
      "Epoch  50 , loss  0.3177136\n",
      "Epoch  50 , loss  0.3177136\n",
      "Epoch  51 , loss  0.26333046\n",
      "Epoch  51 , loss  0.26333046\n",
      "Epoch  52 , loss  0.25654247\n",
      "Epoch  52 , loss  0.25654247\n",
      "Epoch  53 , loss  0.26436993\n",
      "Epoch  53 , loss  0.26436993\n",
      "Epoch  54 , loss  0.27297646\n",
      "Epoch  54 , loss  0.27297646\n",
      "Epoch  55 , loss  0.24492157\n",
      "Epoch  55 , loss  0.24492157\n",
      "Epoch  56 , loss  0.2803604\n",
      "Epoch  56 , loss  0.2803604\n",
      "Epoch  57 , loss  0.27440155\n",
      "Epoch  57 , loss  0.27440155\n",
      "Epoch  58 , loss  0.26081842\n",
      "Epoch  58 , loss  0.26081842\n",
      "Epoch  59 , loss  0.26810727\n",
      "Epoch  59 , loss  0.26810727\n",
      "Epoch  60 , loss  0.3504136\n",
      "Epoch  60 , loss  0.3504136\n",
      "Epoch  61 , loss  0.26228106\n",
      "Epoch  61 , loss  0.26228106\n",
      "Epoch  62 , loss  0.24846393\n",
      "Epoch  62 , loss  0.24846393\n",
      "Epoch  63 , loss  0.23066385\n",
      "Epoch  63 , loss  0.23066385\n",
      "Epoch  64 , loss  0.22272271\n",
      "Epoch  64 , loss  0.22272271\n",
      "Epoch  65 , loss  0.2657617\n",
      "Epoch  65 , loss  0.2657617\n",
      "Epoch  66 , loss  0.24070823\n",
      "Epoch  66 , loss  0.24070823\n",
      "Epoch  67 , loss  0.2955102\n",
      "Epoch  67 , loss  0.2955102\n",
      "Epoch  68 , loss  0.26634392\n",
      "Epoch  68 , loss  0.26634392\n",
      "Epoch  69 , loss  0.25239366\n",
      "Epoch  69 , loss  0.25239366\n",
      "Epoch  70 , loss  0.24390751\n",
      "Epoch  70 , loss  0.24390751\n",
      "Epoch  71 , loss  0.23216252\n",
      "Epoch  71 , loss  0.23216252\n",
      "Epoch  72 , loss  0.23383386\n",
      "Epoch  72 , loss  0.23383386\n",
      "Epoch  73 , loss  0.22905737\n",
      "Epoch  73 , loss  0.22905737\n",
      "Epoch  74 , loss  0.21863592\n",
      "Epoch  74 , loss  0.21863592\n",
      "Epoch  75 , loss  0.22950625\n",
      "Epoch  75 , loss  0.22950625\n",
      "Epoch  76 , loss  0.25009415\n",
      "Epoch  76 , loss  0.25009415\n",
      "Epoch  77 , loss  0.23261674\n",
      "Epoch  77 , loss  0.23261674\n",
      "Epoch  78 , loss  0.2285826\n",
      "Epoch  78 , loss  0.2285826\n",
      "Epoch  79 , loss  0.21579249\n",
      "Epoch  79 , loss  0.21579249\n",
      "Epoch  80 , loss  0.21295647\n",
      "Epoch  80 , loss  0.21295647\n",
      "Epoch  81 , loss  0.20762958\n",
      "Epoch  81 , loss  0.20762958\n",
      "Epoch  82 , loss  0.19726332\n",
      "Epoch  82 , loss  0.19726332\n",
      "Epoch  83 , loss  0.20021285\n",
      "Epoch  83 , loss  0.20021285\n",
      "Epoch  84 , loss  0.21171974\n",
      "Epoch  84 , loss  0.21171974\n",
      "Epoch  85 , loss  0.20138878\n",
      "Epoch  85 , loss  0.20138878\n",
      "Epoch  86 , loss  0.19820888\n",
      "Epoch  86 , loss  0.19820888\n",
      "Epoch  87 , loss  0.19370824\n",
      "Epoch  87 , loss  0.19370824\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "  res = dict()\n",
    "  for n_layers in range(1, 15):\n",
    "    model = torch.nn.Sequential(EDU_QGC(n_layers=n_layers), OneRatioAggregator())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n",
    "    train_metrics = train(model, optimizer, scheduler, epochs=100, print_metrics=False)\n",
    "    eval_metrics = evaluate(model)\n",
    "    res[str(n_layers) + '_train'] = train_metrics\n",
    "    res[str(n_layers) + '_eval'] = eval_metrics\n",
    "    print(n_layers, 'layers')\n",
    "    print('Train: ', train_metrics)\n",
    "    print('Eval: ', eval_metrics)\n",
    "  with open('res_' + str(i) + '.json', 'w') as fp:\n",
    "    json.dump(res, fp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Equivariant Quantum Graph Circuits for submission.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
