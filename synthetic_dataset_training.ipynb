{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er0xRbYNoF6E"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets.molecule_net import MoleculeNet\n",
        "import random \n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import itertools\n",
        "import json\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWYbSE9Fa8YD"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHAKAS8ssJB_"
      },
      "outputs": [],
      "source": [
        "def gen_cycle_pairs(sizes):\n",
        "  pairs = []\n",
        "  for n in sizes:\n",
        "    for k in range(3, n-2):\n",
        "      single = nx.cycle_graph(n)\n",
        "      disjoint = nx.disjoint_union(nx.cycle_graph(k),\n",
        "                                   nx.cycle_graph(n-k))\n",
        "      pairs.append((single, disjoint))\n",
        "  return pairs\n",
        "\n",
        "\n",
        "def to_pyg(g, label):\n",
        "  data = torch_geometric.utils.from_networkx(g)\n",
        "  data.x = torch.zeros((g.number_of_nodes(), 50))\n",
        "  data.y = torch.tensor([label])\n",
        "  return data\n",
        "\n",
        "\n",
        "def cycles_dataset(sizes):\n",
        "  graph_pairs = gen_cycle_pairs(sizes)\n",
        "  data = sum(([to_pyg(g1, 1), to_pyg(g2, 0)] for (g1,g2) in graph_pairs), [])\n",
        "  return data\n",
        "\n",
        "\n",
        "def cycles_dict(n1, n2):\n",
        "  return {i: to_pyg(nx.cycle_graph(i), 1) for i in range(n1, n2+1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UurrtCpMsPVP",
        "outputId": "5157b55c-8299-4a00-d1f2-7b46aa003400"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "cycles_train_loader = torch_geometric.data.DataLoader(cycles_dataset([6, 7, 9, 10]), batch_size=1, shuffle=True)\n",
        "cycles_test_loader = torch_geometric.data.DataLoader(cycles_dataset([8]), batch_size=1, shuffle=True)\n",
        "cycles = cycles_dict(3,13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxs2eBudkls0"
      },
      "outputs": [],
      "source": [
        "def prepare_tinymolhiv():\n",
        "  tiny_molhiv = MoleculeNet(root = './molhiv', name='HIV', \n",
        "                            pre_filter = lambda g: len(g.x) <= 10)  \n",
        "  pos_samples = [g for g in tiny_molhiv if g.y == 1]\n",
        "  neg_samples = [g for g in tiny_molhiv if g.y == 0]\n",
        "  random.seed(0)\n",
        "  random.shuffle(pos_samples)\n",
        "  random.shuffle(neg_samples)\n",
        "  neg_samples = neg_samples[:3*len(pos_samples)]\n",
        "  \n",
        "  for g in pos_samples:\n",
        "    g.y = g.y.squeeze(dim=0)\n",
        "  for g in neg_samples:\n",
        "    g.y = g.y.squeeze(dim=0)\n",
        "  \n",
        "  pos_splits = [pos_samples[i::5] for i in range(5)]\n",
        "  neg_splits = [neg_samples[i::5] for i in range(5)]\n",
        "\n",
        "  splits = [3*pos_splits[i] + neg_splits[i] for i in range(5)]\n",
        "  for i in range(5):\n",
        "    random.shuffle(splits[i])\n",
        "    for g in splits[i]:\n",
        "      g.x = g.x.float()\n",
        "  return splits\n",
        "\n",
        "def prepare_loaders(splits, eval_split=0):\n",
        "  train_data = sum((splits[i] for i in range(len(splits)) if not i == eval_split), [])\n",
        "  eval_data = splits[eval_split]\n",
        "  train_loader = torch_geometric.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
        "  eval_loader = torch_geometric.data.DataLoader(eval_data, batch_size=1, shuffle=True)\n",
        "  return train_loader, eval_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVyhptOEmuzH"
      },
      "outputs": [],
      "source": [
        "hiv_splits = prepare_tinymolhiv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS_aFO8_bAgY"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHtZ4SNybCa7"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSV3fO30FH9B"
      },
      "outputs": [],
      "source": [
        "def tensorpow_squaremat(t, n):\n",
        "  res = t\n",
        "  start_dim = t.shape[0]\n",
        "  dim = start_dim\n",
        "  for i in range(n-1):\n",
        "    res = torch.tensordot(res, t, dims=0)\n",
        "    res = res.permute((0, 2, 1, 3))\n",
        "    dim *= start_dim    \n",
        "    res = res.reshape((dim, dim))\n",
        "  return res\n",
        "\n",
        "def tensormul_vecs(terms):\n",
        "  res = terms[0]\n",
        "  start_dim = res.shape[0]\n",
        "  dim = start_dim\n",
        "  for t in terms[1:]:\n",
        "    res = torch.tensordot(res, t, dims=0)\n",
        "    dim *= start_dim    \n",
        "    res = res.reshape(dim)\n",
        "  return res\n",
        "\n",
        "def hermitian(t):\n",
        "  return t + t.conj().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETkLNFY5bFuK"
      },
      "source": [
        "### EQGC classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejdrBz81Rswc"
      },
      "outputs": [],
      "source": [
        "class EDU_QGC(torch.nn.Module):\n",
        "  def __init__(self, qb_per_node=1, n_layers = 1, init_u3=False):\n",
        "    super(EDU_QGC, self).__init__()\n",
        "    self.qb_per_node = qb_per_node\n",
        "    self.node_state_dim = 2 ** qb_per_node\n",
        "    self.n_layers = n_layers\n",
        "    self.init_u3 = init_u3\n",
        "    self.node_halfH = torch.nn.ParameterList([\n",
        "      torch.nn.Parameter(\n",
        "          torch.randn((self.node_state_dim, self.node_state_dim), dtype=torch.cfloat)) \n",
        "      for i in range(n_layers)\n",
        "    ])\n",
        "    self.edge_D = torch.nn.ParameterList([\n",
        "      torch.nn.Parameter(\n",
        "          torch.randn(self.node_state_dim ** 2))\n",
        "      for i in range(n_layers)\n",
        "    ])\n",
        "\n",
        "  def init_state(self, xs):\n",
        "    if self.init_u3:\n",
        "      node_states = [\n",
        "        torch.tensor([torch.cos(feat[0]), torch.exp(1j*feat[1])*torch.sin(feat[0])])\n",
        "        for feat in xs\n",
        "      ]\n",
        "      return tensormul_vecs(node_states)\n",
        "    else:\n",
        "      n_nodes = len(xs)      \n",
        "      full_dim = self.node_state_dim ** n_nodes\n",
        "      return torch.ones(full_dim, dtype = torch.cfloat) / np.sqrt(full_dim)\n",
        "\n",
        "  def prep_node_layer(self, node_halfH, n_nodes):\n",
        "    node_H = hermitian(node_halfH)\n",
        "    node_U = torch.matrix_exp(1j * node_H)\n",
        "    return tensorpow_squaremat(node_U, n_nodes)\n",
        "\n",
        "  def prep_edge_layer(self, edge_D, n_nodes, edge_index):\n",
        "    full_dim = self.node_state_dim ** n_nodes\n",
        "    v = torch.ones(full_dim, dtype=torch.cfloat)      \n",
        "    for n1,n2 in edge_index.T:\n",
        "      d = torch.exp(1j*edge_D)\n",
        "      d = d.reshape(self.node_state_dim, self.node_state_dim)\n",
        "      d = d.repeat([self.node_state_dim]*(n_nodes-2)+[1,1])\n",
        "      if n2 > n1:\n",
        "        perm = list(range(n1)) + [n_nodes-2] + list(range(n1, n2-1)) + [n_nodes-1] + list(range(n2-1, n_nodes-2))\n",
        "      else:\n",
        "        perm = list(range(n2)) + [n_nodes-1] + list(range(n2, n1-1)) + [n_nodes-2] + list(range(n1-1, n_nodes-2))\n",
        "      d = d.permute(perm).flatten()\n",
        "      v *= d\n",
        "    return v\n",
        "\n",
        "  def forward(self, g):\n",
        "    state = self.init_state(g.x)\n",
        "    n = len(g.x)\n",
        "    for i in range(self.n_layers):\n",
        "      \n",
        "      edge_d = self.prep_edge_layer(self.edge_D[i], n, g.edge_index)\n",
        "      state *= edge_d\n",
        "      node_u = self.prep_node_layer(self.node_halfH[i], n)\n",
        "      state = node_u @ state\n",
        "    probs = torch.square(torch.abs(state))\n",
        "    probs = probs / probs.sum() # normalize for floating point inaccuracies\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBVxV9YVbI9o"
      },
      "source": [
        "### Aggregators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7KzqgKxa4Gu"
      },
      "outputs": [],
      "source": [
        "class OneCountAggregator(torch.nn.Module):\n",
        "  def __init__(self, max_graph_size, verbose = False):\n",
        "    super(OneCountAggregator, self).__init__()\n",
        "    self.max_n = max_graph_size\n",
        "    self.w = torch.nn.Parameter(torch.zeros(max_graph_size+1))\n",
        "    self.verbose = verbose\n",
        "  \n",
        "  def forward(self, probs):\n",
        "    count_probs = torch.zeros(self.max_n+1)\n",
        "    for s in range(len(probs)):\n",
        "      ones = 0\n",
        "      for i in range(self.max_n):\n",
        "        if (s & (1 << i)):\n",
        "          ones += 1\n",
        "      count_probs[ones] += probs[s]\n",
        "    if self.verbose:\n",
        "      print(\"count probs\", count_probs)\n",
        "    cond_probs = torch.sigmoid(self.w)\n",
        "    total_prob = cond_probs @ count_probs # sum P(count = i) x P(pos | count = i)\n",
        "    return total_prob\n",
        "\n",
        "\n",
        "class OneRatioAggregator(torch.nn.Module):\n",
        "  def __init__(self, mlp_hidden_dim = 15, verbose = False):\n",
        "    super(OneRatioAggregator, self).__init__()\n",
        "    self.mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(1, mlp_hidden_dim),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(mlp_hidden_dim, 1),\n",
        "        torch.nn.Sigmoid()\n",
        "    )\n",
        "    self.verbose = verbose\n",
        "  \n",
        "  def forward(self, probs):\n",
        "    max_ones = np.ceil(np.log2(len(probs))).astype('int')\n",
        "    count_probs = torch.zeros(max_ones+1)\n",
        "    for s in range(len(probs)):\n",
        "      ones = 0\n",
        "      for i in range(max_ones):\n",
        "        if (s & (1 << i)):\n",
        "          ones += 1\n",
        "      count_probs[ones] += probs[s]\n",
        "    if self.verbose:\n",
        "      print(\"count probs\", count_probs)\n",
        "    ratios = torch.linspace(0.0, 1.0, max_ones+1)\n",
        "    cond_probs = self.mlp(ratios.reshape(-1,1)).reshape(-1)\n",
        "    total_prob = cond_probs @ count_probs # sum P(count = i) x P(pos | count = i)\n",
        "    return total_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzZ6EL2FbLjD"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1GRW99jkQel"
      },
      "source": [
        "### Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIVE0jl0Mex6"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, lr_scheduler=None, epochs=200, loader=cycles_train_loader, print_metrics=True):\n",
        "  model.train()\n",
        "  for i in range(epochs):\n",
        "    total = 0\n",
        "    correct = 0.0\n",
        "    rso50 = 0\n",
        "    rso55 = 0\n",
        "    loss_sum = 0\n",
        "    min_margin = 0.5\n",
        "    for g in loader: \n",
        "      optimizer.zero_grad()   \n",
        "      out = model(g).unsqueeze(0)\n",
        "      loss = F.binary_cross_entropy(out, g.y.float())  \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      total += 1\n",
        "      loss_sum += loss.detach().numpy()      \n",
        "      p = out.detach().numpy()[0]\n",
        "      if g.y == 1:\n",
        "        correct += p\n",
        "        if p >= 0.5:\n",
        "          rso50 += 1\n",
        "          min_margin = min(p-0.5, min_margin)\n",
        "      else:\n",
        "        correct += (1 - p)\n",
        "        if p < 0.5:\n",
        "          rso50 += 1\n",
        "          min_margin = min(0.5-p, min_margin)\n",
        "    if print_metrics:\n",
        "      print(\"Epoch \", i)\n",
        "      print(\"Loss: \", loss_sum/total)\n",
        "      print(\"Acc: \", correct/total)\n",
        "      print(\"RSo50: \", rso50/total)\n",
        "      print(\"MinMargin: \", min_margin)\n",
        "    else:\n",
        "      print(\"Epoch \", i, \", loss \", loss_sum/total)\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "    if i == epochs-1:\n",
        "      return {\n",
        "          'Loss': loss_sum/total,\n",
        "          'Acc:': correct/total,\n",
        "          'rso50': rso50/total,\n",
        "          'margin': min_margin\n",
        "      }\n",
        "\n",
        "\n",
        "def evaluate(model, loader=cycles_test_loader):\n",
        "  total = 0\n",
        "  correct = 0.0\n",
        "  rso50 = 0\n",
        "  min_margin = 0.5\n",
        "  loss_sum = 0\n",
        "  with torch.no_grad():\n",
        "    for g in loader: \n",
        "      out = model(g).unsqueeze(0)\n",
        "      loss = F.binary_cross_entropy(out, g.y.float())  \n",
        "      loss_sum += loss.detach().numpy()\n",
        "      total += 1\n",
        "      p = out.detach().numpy()[0]\n",
        "      if g.y == 1:\n",
        "        correct += p\n",
        "        if p >= 0.5:\n",
        "          rso50 += 1\n",
        "          min_margin = min(min_margin, p-0.5)\n",
        "      else:\n",
        "        correct += (1 - p)\n",
        "        if p < 0.5:\n",
        "          rso50 += 1\n",
        "          min_margin = min(min_margin, 0.5-p)\n",
        "  return {\n",
        "    'Loss': loss_sum/total,\n",
        "    'Acc:': correct/total,\n",
        "    'rso50': rso50/total,\n",
        "    'margin': min_margin\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_Us2jSka9R"
      },
      "source": [
        "### Cycles experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n2lrpJIEd68",
        "outputId": "59008dee-5eff-4a9e-fad3-58b7f8adc97b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 layers\n",
            "Train:  {'Loss': 0.6780364091197649, 'Acc:': 0.5078519197801749, 'rso50': 0.75, 'margin': 0.001971423625946045}\n",
            "Eval:  {'Loss': 0.6748795012633005, 'Acc:': 0.5093061973651251, 'rso50': 0.8333333333333334, 'margin': 0.006442070007324219}\n",
            "2 layers\n",
            "Train:  {'Loss': 0.6177651981512705, 'Acc:': 0.5418155615528425, 'rso50': 0.9583333333333334, 'margin': 9.495019912719727e-05}\n",
            "Eval:  {'Loss': 0.6040907402833303, 'Acc:': 0.5491860409577688, 'rso50': 0.5, 'margin': 0.10236728191375732}\n",
            "3 layers\n",
            "Train:  {'Loss': 0.3950042249634862, 'Acc:': 0.6816817863533894, 'rso50': 1.0, 'margin': 0.039186716079711914}\n",
            "Eval:  {'Loss': 0.31118466208378476, 'Acc:': 0.7440535922845205, 'rso50': 1.0, 'margin': 0.11598515510559082}\n",
            "4 layers\n",
            "Train:  {'Loss': 0.6938727994759878, 'Acc:': 0.49963802595933277, 'rso50': 0.375, 'margin': 7.134675979614258e-05}\n",
            "Eval:  {'Loss': 0.6931493580341339, 'Acc:': 0.5000005314747492, 'rso50': 0.5, 'margin': 0.0012707710266113281}\n",
            "5 layers\n",
            "Train:  {'Loss': 0.36236930048714083, 'Acc:': 0.7054703546067079, 'rso50': 1.0, 'margin': 0.028486549854278564}\n",
            "Eval:  {'Loss': 0.2878715091695388, 'Acc:': 0.7650510656336943, 'rso50': 1.0, 'margin': 0.11382585763931274}\n",
            "6 layers\n",
            "Train:  {'Loss': 0.2824582978306959, 'Acc:': 0.7654300247862315, 'rso50': 0.875, 'margin': 0.21298950910568237}\n",
            "Eval:  {'Loss': 0.22683589905500412, 'Acc:': 0.8005739028255144, 'rso50': 1.0, 'margin': 0.22556966543197632}\n",
            "7 layers\n",
            "Train:  {'Loss': 0.2536517071227233, 'Acc:': 0.7863054277064899, 'rso50': 0.9583333333333334, 'margin': 0.11008089780807495}\n",
            "Eval:  {'Loss': 0.22645253439744314, 'Acc:': 0.8036039248108864, 'rso50': 1.0, 'margin': 0.20539331436157227}\n",
            "8 layers\n",
            "Train:  {'Loss': 0.2519287444495906, 'Acc:': 0.7883403215867778, 'rso50': 0.9583333333333334, 'margin': 0.06200599670410156}\n",
            "Eval:  {'Loss': 0.19591139443218708, 'Acc:': 0.8290224149823189, 'rso50': 1.0, 'margin': 0.22276604175567627}\n",
            "9 layers\n",
            "Train:  {'Loss': 0.28205102930466336, 'Acc:': 0.7612009509466588, 'rso50': 1.0, 'margin': 0.10567349195480347}\n",
            "Eval:  {'Loss': 0.23683389695361257, 'Acc:': 0.7977231397914389, 'rso50': 1.0, 'margin': 0.1855006217956543}\n",
            "10 layers\n",
            "Train:  {'Loss': 0.1991144533191497, 'Acc:': 0.8254928331783352, 'rso50': 1.0, 'margin': 0.1000707745552063}\n",
            "Eval:  {'Loss': 0.1725525123377641, 'Acc:': 0.8460773508995771, 'rso50': 1.0, 'margin': 0.258980929851532}\n",
            "11 layers\n",
            "Train:  {'Loss': 0.19850503536872566, 'Acc:': 0.8310686284676194, 'rso50': 0.9583333333333334, 'margin': 0.0372852087020874}\n",
            "Eval:  {'Loss': 0.13976543272535005, 'Acc:': 0.8699387274682522, 'rso50': 1.0, 'margin': 0.35187286138534546}\n",
            "12 layers\n",
            "Train:  {'Loss': 0.2134596286729599, 'Acc:': 0.8134736531258872, 'rso50': 1.0, 'margin': 0.15150439739227295}\n",
            "Eval:  {'Loss': 0.18664464199294648, 'Acc:': 0.8414272374163071, 'rso50': 1.0, 'margin': 0.2016671895980835}\n",
            "13 layers\n",
            "Train:  {'Loss': 0.21611430170984627, 'Acc:': 0.8133862794978389, 'rso50': 1.0, 'margin': 0.07769453525543213}\n",
            "Eval:  {'Loss': 0.25290496988842887, 'Acc:': 0.7925663137187561, 'rso50': 1.0, 'margin': 0.13432753086090088}\n",
            "14 layers\n",
            "Train:  {'Loss': 0.1308611455412271, 'Acc:': 0.8831796667557986, 'rso50': 1.0, 'margin': 0.08753639459609985}\n",
            "Eval:  {'Loss': 0.10052713476276647, 'Acc:': 0.9081305599538609, 'rso50': 1.0, 'margin': 0.32553231716156006}\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 11):\n",
        "  res = dict()\n",
        "  for n_layers in range(1, 15):\n",
        "    model = torch.nn.Sequential(EDU_QGC(n_layers=n_layers), OneRatioAggregator())\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\n",
        "    train_metrics = train(model, optimizer, scheduler, epochs=100, print_metrics=False)\n",
        "    eval_metrics = evaluate(model)\n",
        "    res[str(n_layers) + '_train'] = train_metrics\n",
        "    res[str(n_layers) + '_eval'] = eval_metrics\n",
        "    print(n_layers, 'layers')\n",
        "    print('Train: ', train_metrics)\n",
        "    print('Eval: ', eval_metrics)\n",
        "  with open('res_' + str(i) + '.json', 'w') as fp:\n",
        "    json.dump(res, fp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Equivariant Quantum Graph Circuits for submission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}